{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPv6dbZCXYCV",
        "outputId": "52e4a5b8-1306-40ce-a4eb-b312fba61147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.94MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created with 15784468 trainable parameters\n",
            "Starting training...\n",
            "Configuration: Config(batch_size=256, epochs=10, learning_rate=0.001, no_cuda=False, seed=10, log_interval=2, category=10, alpha=0.3, data_dir='./data', output_dir='./image', encoder_channels=64, decoder_channels=64, input_channels=1, latent_dim=1024, hidden_dim=512)\n",
            "Train Epoch: 1 [    0/60000 (  0%)]\tLoss: 629.653076\n",
            "Train Epoch: 1 [  512/60000 (  1%)]\tLoss: 599.224365\n",
            "Train Epoch: 1 [ 1024/60000 (  2%)]\tLoss: 272.866852\n",
            "Train Epoch: 1 [ 1536/60000 (  3%)]\tLoss: 226.046082\n",
            "Train Epoch: 1 [ 2048/60000 (  3%)]\tLoss: 207.254395\n",
            "Train Epoch: 1 [ 2560/60000 (  4%)]\tLoss: 187.622986\n",
            "Train Epoch: 1 [ 3072/60000 (  5%)]\tLoss: 184.652435\n",
            "Train Epoch: 1 [ 3584/60000 (  6%)]\tLoss: 174.254547\n",
            "Train Epoch: 1 [ 4096/60000 (  7%)]\tLoss: 176.130463\n",
            "Train Epoch: 1 [ 4608/60000 (  8%)]\tLoss: 164.968262\n",
            "Train Epoch: 1 [ 5120/60000 (  9%)]\tLoss: 164.438477\n",
            "Train Epoch: 1 [ 5632/60000 (  9%)]\tLoss: 161.881729\n",
            "Train Epoch: 1 [ 6144/60000 ( 10%)]\tLoss: 168.125549\n",
            "Train Epoch: 1 [ 6656/60000 ( 11%)]\tLoss: 164.448898\n",
            "Train Epoch: 1 [ 7168/60000 ( 12%)]\tLoss: 157.169464\n",
            "Train Epoch: 1 [ 7680/60000 ( 13%)]\tLoss: 161.176636\n",
            "Train Epoch: 1 [ 8192/60000 ( 14%)]\tLoss: 165.848145\n",
            "Train Epoch: 1 [ 8704/60000 ( 14%)]\tLoss: 161.468552\n",
            "Train Epoch: 1 [ 9216/60000 ( 15%)]\tLoss: 165.356323\n",
            "Train Epoch: 1 [ 9728/60000 ( 16%)]\tLoss: 160.192825\n",
            "Train Epoch: 1 [10240/60000 ( 17%)]\tLoss: 164.331818\n",
            "Train Epoch: 1 [10752/60000 ( 18%)]\tLoss: 165.234940\n",
            "Train Epoch: 1 [11264/60000 ( 19%)]\tLoss: 160.200287\n",
            "Train Epoch: 1 [11776/60000 ( 20%)]\tLoss: 161.282776\n",
            "Train Epoch: 1 [12288/60000 ( 20%)]\tLoss: 161.710449\n",
            "Train Epoch: 1 [12800/60000 ( 21%)]\tLoss: 160.640106\n",
            "Train Epoch: 1 [13312/60000 ( 22%)]\tLoss: 163.365707\n",
            "Train Epoch: 1 [13824/60000 ( 23%)]\tLoss: 161.481232\n",
            "Train Epoch: 1 [14336/60000 ( 24%)]\tLoss: 160.262085\n",
            "Train Epoch: 1 [14848/60000 ( 25%)]\tLoss: 160.721527\n",
            "Train Epoch: 1 [15360/60000 ( 26%)]\tLoss: 161.770569\n",
            "Train Epoch: 1 [15872/60000 ( 26%)]\tLoss: 161.115723\n",
            "Train Epoch: 1 [16384/60000 ( 27%)]\tLoss: 155.816864\n",
            "Train Epoch: 1 [16896/60000 ( 28%)]\tLoss: 161.622772\n",
            "Train Epoch: 1 [17408/60000 ( 29%)]\tLoss: 158.344574\n",
            "Train Epoch: 1 [17920/60000 ( 30%)]\tLoss: 154.253983\n",
            "Train Epoch: 1 [18432/60000 ( 31%)]\tLoss: 158.661148\n",
            "Train Epoch: 1 [18944/60000 ( 31%)]\tLoss: 160.013428\n",
            "Train Epoch: 1 [19456/60000 ( 32%)]\tLoss: 154.399872\n",
            "Train Epoch: 1 [19968/60000 ( 33%)]\tLoss: 160.489822\n",
            "Train Epoch: 1 [20480/60000 ( 34%)]\tLoss: 153.442993\n",
            "Train Epoch: 1 [20992/60000 ( 35%)]\tLoss: 153.868256\n",
            "Train Epoch: 1 [21504/60000 ( 36%)]\tLoss: 155.594147\n",
            "Train Epoch: 1 [22016/60000 ( 37%)]\tLoss: 157.968140\n",
            "Train Epoch: 1 [22528/60000 ( 37%)]\tLoss: 150.762756\n",
            "Train Epoch: 1 [23040/60000 ( 38%)]\tLoss: 149.701248\n",
            "Train Epoch: 1 [23552/60000 ( 39%)]\tLoss: 150.729218\n",
            "Train Epoch: 1 [24064/60000 ( 40%)]\tLoss: 157.311981\n",
            "Train Epoch: 1 [24576/60000 ( 41%)]\tLoss: 157.265045\n",
            "Train Epoch: 1 [25088/60000 ( 42%)]\tLoss: 151.969528\n",
            "Train Epoch: 1 [25600/60000 ( 43%)]\tLoss: 155.191589\n",
            "Train Epoch: 1 [26112/60000 ( 43%)]\tLoss: 148.239685\n",
            "Train Epoch: 1 [26624/60000 ( 44%)]\tLoss: 151.793625\n",
            "Train Epoch: 1 [27136/60000 ( 45%)]\tLoss: 146.807724\n",
            "Train Epoch: 1 [27648/60000 ( 46%)]\tLoss: 151.556274\n",
            "Train Epoch: 1 [28160/60000 ( 47%)]\tLoss: 147.971619\n",
            "Train Epoch: 1 [28672/60000 ( 48%)]\tLoss: 152.883942\n",
            "Train Epoch: 1 [29184/60000 ( 49%)]\tLoss: 149.143921\n",
            "Train Epoch: 1 [29696/60000 ( 49%)]\tLoss: 140.993057\n",
            "Train Epoch: 1 [30208/60000 ( 50%)]\tLoss: 145.876801\n",
            "Train Epoch: 1 [30720/60000 ( 51%)]\tLoss: 148.609909\n",
            "Train Epoch: 1 [31232/60000 ( 52%)]\tLoss: 149.453766\n",
            "Train Epoch: 1 [31744/60000 ( 53%)]\tLoss: 144.629959\n",
            "Train Epoch: 1 [32256/60000 ( 54%)]\tLoss: 147.113312\n",
            "Train Epoch: 1 [32768/60000 ( 54%)]\tLoss: 148.598083\n",
            "Train Epoch: 1 [33280/60000 ( 55%)]\tLoss: 145.130951\n",
            "Train Epoch: 1 [33792/60000 ( 56%)]\tLoss: 148.185852\n",
            "Train Epoch: 1 [34304/60000 ( 57%)]\tLoss: 146.973846\n",
            "Train Epoch: 1 [34816/60000 ( 58%)]\tLoss: 147.772995\n",
            "Train Epoch: 1 [35328/60000 ( 59%)]\tLoss: 144.953140\n",
            "Train Epoch: 1 [35840/60000 ( 60%)]\tLoss: 147.626938\n",
            "Train Epoch: 1 [36352/60000 ( 60%)]\tLoss: 148.199036\n",
            "Train Epoch: 1 [36864/60000 ( 61%)]\tLoss: 143.819870\n",
            "Train Epoch: 1 [37376/60000 ( 62%)]\tLoss: 148.337372\n",
            "Train Epoch: 1 [37888/60000 ( 63%)]\tLoss: 145.657532\n",
            "Train Epoch: 1 [38400/60000 ( 64%)]\tLoss: 145.070755\n",
            "Train Epoch: 1 [38912/60000 ( 65%)]\tLoss: 144.637100\n",
            "Train Epoch: 1 [39424/60000 ( 66%)]\tLoss: 144.397171\n",
            "Train Epoch: 1 [39936/60000 ( 66%)]\tLoss: 142.258194\n",
            "Train Epoch: 1 [40448/60000 ( 67%)]\tLoss: 143.934738\n",
            "Train Epoch: 1 [40960/60000 ( 68%)]\tLoss: 143.803940\n",
            "Train Epoch: 1 [41472/60000 ( 69%)]\tLoss: 142.004013\n",
            "Train Epoch: 1 [41984/60000 ( 70%)]\tLoss: 143.711456\n",
            "Train Epoch: 1 [42496/60000 ( 71%)]\tLoss: 141.183868\n",
            "Train Epoch: 1 [43008/60000 ( 71%)]\tLoss: 146.914398\n",
            "Train Epoch: 1 [43520/60000 ( 72%)]\tLoss: 145.564331\n",
            "Train Epoch: 1 [44032/60000 ( 73%)]\tLoss: 143.971512\n",
            "Train Epoch: 1 [44544/60000 ( 74%)]\tLoss: 140.587891\n",
            "Train Epoch: 1 [45056/60000 ( 75%)]\tLoss: 137.518768\n",
            "Train Epoch: 1 [45568/60000 ( 76%)]\tLoss: 136.015640\n",
            "Train Epoch: 1 [46080/60000 ( 77%)]\tLoss: 140.606827\n",
            "Train Epoch: 1 [46592/60000 ( 77%)]\tLoss: 139.511429\n",
            "Train Epoch: 1 [47104/60000 ( 78%)]\tLoss: 140.692078\n",
            "Train Epoch: 1 [47616/60000 ( 79%)]\tLoss: 142.229050\n",
            "Train Epoch: 1 [48128/60000 ( 80%)]\tLoss: 137.907379\n",
            "Train Epoch: 1 [48640/60000 ( 81%)]\tLoss: 134.430099\n",
            "Train Epoch: 1 [49152/60000 ( 82%)]\tLoss: 135.328674\n",
            "Train Epoch: 1 [49664/60000 ( 83%)]\tLoss: 143.175186\n",
            "Train Epoch: 1 [50176/60000 ( 83%)]\tLoss: 142.217117\n",
            "Train Epoch: 1 [50688/60000 ( 84%)]\tLoss: 136.891693\n",
            "Train Epoch: 1 [51200/60000 ( 85%)]\tLoss: 139.212982\n",
            "Train Epoch: 1 [51712/60000 ( 86%)]\tLoss: 136.888489\n",
            "Train Epoch: 1 [52224/60000 ( 87%)]\tLoss: 134.351944\n",
            "Train Epoch: 1 [52736/60000 ( 88%)]\tLoss: 138.529724\n",
            "Train Epoch: 1 [53248/60000 ( 89%)]\tLoss: 133.993408\n",
            "Train Epoch: 1 [53760/60000 ( 89%)]\tLoss: 131.414978\n",
            "Train Epoch: 1 [54272/60000 ( 90%)]\tLoss: 134.199173\n",
            "Train Epoch: 1 [54784/60000 ( 91%)]\tLoss: 130.219696\n",
            "Train Epoch: 1 [55296/60000 ( 92%)]\tLoss: 129.805481\n",
            "Train Epoch: 1 [55808/60000 ( 93%)]\tLoss: 128.531464\n",
            "Train Epoch: 1 [56320/60000 ( 94%)]\tLoss: 122.490387\n",
            "Train Epoch: 1 [56832/60000 ( 94%)]\tLoss: 132.326630\n",
            "Train Epoch: 1 [57344/60000 ( 95%)]\tLoss: 127.734024\n",
            "Train Epoch: 1 [57856/60000 ( 96%)]\tLoss: 129.913406\n",
            "Train Epoch: 1 [58368/60000 ( 97%)]\tLoss: 126.273170\n",
            "Train Epoch: 1 [58880/60000 ( 98%)]\tLoss: 126.209442\n",
            "Train Epoch: 1 [59392/60000 ( 99%)]\tLoss: 121.423538\n",
            "Train Epoch: 1 [22464/60000 (100%)]\tLoss: 127.277822\n",
            "====> Epoch: 1 Average loss: 160.9842\n",
            "====> Test set loss: 128.6250\n",
            "Train Epoch: 2 [    0/60000 (  0%)]\tLoss: 127.452492\n",
            "Train Epoch: 2 [  512/60000 (  1%)]\tLoss: 120.910538\n",
            "Train Epoch: 2 [ 1024/60000 (  2%)]\tLoss: 125.690826\n",
            "Train Epoch: 2 [ 1536/60000 (  3%)]\tLoss: 123.838799\n",
            "Train Epoch: 2 [ 2048/60000 (  3%)]\tLoss: 118.829025\n",
            "Train Epoch: 2 [ 2560/60000 (  4%)]\tLoss: 120.088936\n",
            "Train Epoch: 2 [ 3072/60000 (  5%)]\tLoss: 123.783470\n",
            "Train Epoch: 2 [ 3584/60000 (  6%)]\tLoss: 116.819603\n",
            "Train Epoch: 2 [ 4096/60000 (  7%)]\tLoss: 117.997528\n",
            "Train Epoch: 2 [ 4608/60000 (  8%)]\tLoss: 121.158066\n",
            "Train Epoch: 2 [ 5120/60000 (  9%)]\tLoss: 117.369125\n",
            "Train Epoch: 2 [ 5632/60000 (  9%)]\tLoss: 120.166763\n",
            "Train Epoch: 2 [ 6144/60000 ( 10%)]\tLoss: 114.364586\n",
            "Train Epoch: 2 [ 6656/60000 ( 11%)]\tLoss: 115.636551\n",
            "Train Epoch: 2 [ 7168/60000 ( 12%)]\tLoss: 117.954582\n",
            "Train Epoch: 2 [ 7680/60000 ( 13%)]\tLoss: 112.160820\n",
            "Train Epoch: 2 [ 8192/60000 ( 14%)]\tLoss: 114.965385\n",
            "Train Epoch: 2 [ 8704/60000 ( 14%)]\tLoss: 112.233856\n",
            "Train Epoch: 2 [ 9216/60000 ( 15%)]\tLoss: 114.228897\n",
            "Train Epoch: 2 [ 9728/60000 ( 16%)]\tLoss: 113.184837\n",
            "Train Epoch: 2 [10240/60000 ( 17%)]\tLoss: 113.073975\n",
            "Train Epoch: 2 [10752/60000 ( 18%)]\tLoss: 110.021797\n",
            "Train Epoch: 2 [11264/60000 ( 19%)]\tLoss: 108.348022\n",
            "Train Epoch: 2 [11776/60000 ( 20%)]\tLoss: 109.211639\n",
            "Train Epoch: 2 [12288/60000 ( 20%)]\tLoss: 109.523636\n",
            "Train Epoch: 2 [12800/60000 ( 21%)]\tLoss: 109.973915\n",
            "Train Epoch: 2 [13312/60000 ( 22%)]\tLoss: 112.010704\n",
            "Train Epoch: 2 [13824/60000 ( 23%)]\tLoss: 106.053185\n",
            "Train Epoch: 2 [14336/60000 ( 24%)]\tLoss: 99.984276\n",
            "Train Epoch: 2 [14848/60000 ( 25%)]\tLoss: 107.551865\n",
            "Train Epoch: 2 [15360/60000 ( 26%)]\tLoss: 114.707161\n",
            "Train Epoch: 2 [15872/60000 ( 26%)]\tLoss: 107.377449\n",
            "Train Epoch: 2 [16384/60000 ( 27%)]\tLoss: 107.782280\n",
            "Train Epoch: 2 [16896/60000 ( 28%)]\tLoss: 110.758049\n",
            "Train Epoch: 2 [17408/60000 ( 29%)]\tLoss: 107.166878\n",
            "Train Epoch: 2 [17920/60000 ( 30%)]\tLoss: 106.271469\n",
            "Train Epoch: 2 [18432/60000 ( 31%)]\tLoss: 109.395218\n",
            "Train Epoch: 2 [18944/60000 ( 31%)]\tLoss: 107.387405\n",
            "Train Epoch: 2 [19456/60000 ( 32%)]\tLoss: 106.154999\n",
            "Train Epoch: 2 [19968/60000 ( 33%)]\tLoss: 106.573181\n",
            "Train Epoch: 2 [20480/60000 ( 34%)]\tLoss: 99.401588\n",
            "Train Epoch: 2 [20992/60000 ( 35%)]\tLoss: 101.451744\n",
            "Train Epoch: 2 [21504/60000 ( 36%)]\tLoss: 105.568489\n",
            "Train Epoch: 2 [22016/60000 ( 37%)]\tLoss: 106.296997\n",
            "Train Epoch: 2 [22528/60000 ( 37%)]\tLoss: 97.732224\n",
            "Train Epoch: 2 [23040/60000 ( 38%)]\tLoss: 97.012367\n",
            "Train Epoch: 2 [23552/60000 ( 39%)]\tLoss: 102.382896\n",
            "Train Epoch: 2 [24064/60000 ( 40%)]\tLoss: 101.322136\n",
            "Train Epoch: 2 [24576/60000 ( 41%)]\tLoss: 102.625755\n",
            "Train Epoch: 2 [25088/60000 ( 42%)]\tLoss: 98.039429\n",
            "Train Epoch: 2 [25600/60000 ( 43%)]\tLoss: 101.334518\n",
            "Train Epoch: 2 [26112/60000 ( 43%)]\tLoss: 98.021011\n",
            "Train Epoch: 2 [26624/60000 ( 44%)]\tLoss: 104.175537\n",
            "Train Epoch: 2 [27136/60000 ( 45%)]\tLoss: 101.286728\n",
            "Train Epoch: 2 [27648/60000 ( 46%)]\tLoss: 104.069733\n",
            "Train Epoch: 2 [28160/60000 ( 47%)]\tLoss: 98.165001\n",
            "Train Epoch: 2 [28672/60000 ( 48%)]\tLoss: 104.195320\n",
            "Train Epoch: 2 [29184/60000 ( 49%)]\tLoss: 98.333450\n",
            "Train Epoch: 2 [29696/60000 ( 49%)]\tLoss: 98.980499\n",
            "Train Epoch: 2 [30208/60000 ( 50%)]\tLoss: 101.840492\n",
            "Train Epoch: 2 [30720/60000 ( 51%)]\tLoss: 96.267479\n",
            "Train Epoch: 2 [31232/60000 ( 52%)]\tLoss: 94.322418\n",
            "Train Epoch: 2 [31744/60000 ( 53%)]\tLoss: 99.569527\n",
            "Train Epoch: 2 [32256/60000 ( 54%)]\tLoss: 97.828995\n",
            "Train Epoch: 2 [32768/60000 ( 54%)]\tLoss: 99.697105\n",
            "Train Epoch: 2 [33280/60000 ( 55%)]\tLoss: 96.227989\n",
            "Train Epoch: 2 [33792/60000 ( 56%)]\tLoss: 95.199837\n",
            "Train Epoch: 2 [34304/60000 ( 57%)]\tLoss: 95.094292\n",
            "Train Epoch: 2 [34816/60000 ( 58%)]\tLoss: 97.737457\n",
            "Train Epoch: 2 [35328/60000 ( 59%)]\tLoss: 95.133041\n",
            "Train Epoch: 2 [35840/60000 ( 60%)]\tLoss: 92.905212\n",
            "Train Epoch: 2 [36352/60000 ( 60%)]\tLoss: 91.288597\n",
            "Train Epoch: 2 [36864/60000 ( 61%)]\tLoss: 91.081345\n",
            "Train Epoch: 2 [37376/60000 ( 62%)]\tLoss: 94.111557\n",
            "Train Epoch: 2 [37888/60000 ( 63%)]\tLoss: 95.774544\n",
            "Train Epoch: 2 [38400/60000 ( 64%)]\tLoss: 101.390945\n",
            "Train Epoch: 2 [38912/60000 ( 65%)]\tLoss: 94.176353\n",
            "Train Epoch: 2 [39424/60000 ( 66%)]\tLoss: 94.534485\n",
            "Train Epoch: 2 [39936/60000 ( 66%)]\tLoss: 96.009590\n",
            "Train Epoch: 2 [40448/60000 ( 67%)]\tLoss: 93.178825\n",
            "Train Epoch: 2 [40960/60000 ( 68%)]\tLoss: 99.917747\n",
            "Train Epoch: 2 [41472/60000 ( 69%)]\tLoss: 96.852119\n",
            "Train Epoch: 2 [41984/60000 ( 70%)]\tLoss: 93.864449\n",
            "Train Epoch: 2 [42496/60000 ( 71%)]\tLoss: 96.725616\n",
            "Train Epoch: 2 [43008/60000 ( 71%)]\tLoss: 93.766716\n",
            "Train Epoch: 2 [43520/60000 ( 72%)]\tLoss: 93.013603\n",
            "Train Epoch: 2 [44032/60000 ( 73%)]\tLoss: 92.558220\n",
            "Train Epoch: 2 [44544/60000 ( 74%)]\tLoss: 91.336052\n",
            "Train Epoch: 2 [45056/60000 ( 75%)]\tLoss: 90.535301\n",
            "Train Epoch: 2 [45568/60000 ( 76%)]\tLoss: 87.627991\n",
            "Train Epoch: 2 [46080/60000 ( 77%)]\tLoss: 90.793396\n",
            "Train Epoch: 2 [46592/60000 ( 77%)]\tLoss: 91.403740\n",
            "Train Epoch: 2 [47104/60000 ( 78%)]\tLoss: 87.143097\n",
            "Train Epoch: 2 [47616/60000 ( 79%)]\tLoss: 92.846245\n",
            "Train Epoch: 2 [48128/60000 ( 80%)]\tLoss: 94.272278\n",
            "Train Epoch: 2 [48640/60000 ( 81%)]\tLoss: 91.673561\n",
            "Train Epoch: 2 [49152/60000 ( 82%)]\tLoss: 93.887215\n",
            "Train Epoch: 2 [49664/60000 ( 83%)]\tLoss: 91.660866\n",
            "Train Epoch: 2 [50176/60000 ( 83%)]\tLoss: 92.585037\n",
            "Train Epoch: 2 [50688/60000 ( 84%)]\tLoss: 91.601395\n",
            "Train Epoch: 2 [51200/60000 ( 85%)]\tLoss: 86.354065\n",
            "Train Epoch: 2 [51712/60000 ( 86%)]\tLoss: 90.293274\n",
            "Train Epoch: 2 [52224/60000 ( 87%)]\tLoss: 88.362915\n",
            "Train Epoch: 2 [52736/60000 ( 88%)]\tLoss: 91.440750\n",
            "Train Epoch: 2 [53248/60000 ( 89%)]\tLoss: 88.749123\n",
            "Train Epoch: 2 [53760/60000 ( 89%)]\tLoss: 90.300232\n",
            "Train Epoch: 2 [54272/60000 ( 90%)]\tLoss: 87.036903\n",
            "Train Epoch: 2 [54784/60000 ( 91%)]\tLoss: 87.271942\n",
            "Train Epoch: 2 [55296/60000 ( 92%)]\tLoss: 90.960602\n",
            "Train Epoch: 2 [55808/60000 ( 93%)]\tLoss: 93.140457\n",
            "Train Epoch: 2 [56320/60000 ( 94%)]\tLoss: 89.070671\n",
            "Train Epoch: 2 [56832/60000 ( 94%)]\tLoss: 89.254318\n",
            "Train Epoch: 2 [57344/60000 ( 95%)]\tLoss: 87.124992\n",
            "Train Epoch: 2 [57856/60000 ( 96%)]\tLoss: 87.092041\n",
            "Train Epoch: 2 [58368/60000 ( 97%)]\tLoss: 89.309708\n",
            "Train Epoch: 2 [58880/60000 ( 98%)]\tLoss: 86.919800\n",
            "Train Epoch: 2 [59392/60000 ( 99%)]\tLoss: 87.354897\n",
            "Train Epoch: 2 [22464/60000 (100%)]\tLoss: 93.336507\n",
            "====> Epoch: 2 Average loss: 101.2539\n",
            "====> Test set loss: 95.3988\n",
            "Train Epoch: 3 [    0/60000 (  0%)]\tLoss: 93.405487\n",
            "Train Epoch: 3 [  512/60000 (  1%)]\tLoss: 90.335190\n",
            "Train Epoch: 3 [ 1024/60000 (  2%)]\tLoss: 88.629379\n",
            "Train Epoch: 3 [ 1536/60000 (  3%)]\tLoss: 91.887695\n",
            "Train Epoch: 3 [ 2048/60000 (  3%)]\tLoss: 95.004845\n",
            "Train Epoch: 3 [ 2560/60000 (  4%)]\tLoss: 90.932907\n",
            "Train Epoch: 3 [ 3072/60000 (  5%)]\tLoss: 89.322960\n",
            "Train Epoch: 3 [ 3584/60000 (  6%)]\tLoss: 87.843369\n",
            "Train Epoch: 3 [ 4096/60000 (  7%)]\tLoss: 88.845627\n",
            "Train Epoch: 3 [ 4608/60000 (  8%)]\tLoss: 86.089912\n",
            "Train Epoch: 3 [ 5120/60000 (  9%)]\tLoss: 84.634941\n",
            "Train Epoch: 3 [ 5632/60000 (  9%)]\tLoss: 86.901451\n",
            "Train Epoch: 3 [ 6144/60000 ( 10%)]\tLoss: 87.917068\n",
            "Train Epoch: 3 [ 6656/60000 ( 11%)]\tLoss: 86.319191\n",
            "Train Epoch: 3 [ 7168/60000 ( 12%)]\tLoss: 84.959045\n",
            "Train Epoch: 3 [ 7680/60000 ( 13%)]\tLoss: 82.399757\n",
            "Train Epoch: 3 [ 8192/60000 ( 14%)]\tLoss: 82.072289\n",
            "Train Epoch: 3 [ 8704/60000 ( 14%)]\tLoss: 85.471985\n",
            "Train Epoch: 3 [ 9216/60000 ( 15%)]\tLoss: 86.491135\n",
            "Train Epoch: 3 [ 9728/60000 ( 16%)]\tLoss: 83.304337\n",
            "Train Epoch: 3 [10240/60000 ( 17%)]\tLoss: 86.242966\n",
            "Train Epoch: 3 [10752/60000 ( 18%)]\tLoss: 90.480431\n",
            "Train Epoch: 3 [11264/60000 ( 19%)]\tLoss: 84.739586\n",
            "Train Epoch: 3 [11776/60000 ( 20%)]\tLoss: 82.008919\n",
            "Train Epoch: 3 [12288/60000 ( 20%)]\tLoss: 81.590347\n",
            "Train Epoch: 3 [12800/60000 ( 21%)]\tLoss: 86.508102\n",
            "Train Epoch: 3 [13312/60000 ( 22%)]\tLoss: 84.425903\n",
            "Train Epoch: 3 [13824/60000 ( 23%)]\tLoss: 81.412674\n",
            "Train Epoch: 3 [14336/60000 ( 24%)]\tLoss: 90.961143\n",
            "Train Epoch: 3 [14848/60000 ( 25%)]\tLoss: 81.236031\n",
            "Train Epoch: 3 [15360/60000 ( 26%)]\tLoss: 84.207306\n",
            "Train Epoch: 3 [15872/60000 ( 26%)]\tLoss: 81.159386\n",
            "Train Epoch: 3 [16384/60000 ( 27%)]\tLoss: 81.266403\n",
            "Train Epoch: 3 [16896/60000 ( 28%)]\tLoss: 82.706284\n",
            "Train Epoch: 3 [17408/60000 ( 29%)]\tLoss: 86.663422\n",
            "Train Epoch: 3 [17920/60000 ( 30%)]\tLoss: 86.587570\n",
            "Train Epoch: 3 [18432/60000 ( 31%)]\tLoss: 81.853622\n",
            "Train Epoch: 3 [18944/60000 ( 31%)]\tLoss: 87.005051\n",
            "Train Epoch: 3 [19456/60000 ( 32%)]\tLoss: 84.241211\n",
            "Train Epoch: 3 [19968/60000 ( 33%)]\tLoss: 81.719284\n",
            "Train Epoch: 3 [20480/60000 ( 34%)]\tLoss: 83.082687\n",
            "Train Epoch: 3 [20992/60000 ( 35%)]\tLoss: 83.845177\n",
            "Train Epoch: 3 [21504/60000 ( 36%)]\tLoss: 84.691467\n",
            "Train Epoch: 3 [22016/60000 ( 37%)]\tLoss: 79.103409\n",
            "Train Epoch: 3 [22528/60000 ( 37%)]\tLoss: 79.644623\n",
            "Train Epoch: 3 [23040/60000 ( 38%)]\tLoss: 80.744965\n",
            "Train Epoch: 3 [23552/60000 ( 39%)]\tLoss: 84.143860\n",
            "Train Epoch: 3 [24064/60000 ( 40%)]\tLoss: 83.129135\n",
            "Train Epoch: 3 [24576/60000 ( 41%)]\tLoss: 78.485687\n",
            "Train Epoch: 3 [25088/60000 ( 42%)]\tLoss: 78.544518\n",
            "Train Epoch: 3 [25600/60000 ( 43%)]\tLoss: 81.576096\n",
            "Train Epoch: 3 [26112/60000 ( 43%)]\tLoss: 83.764885\n",
            "Train Epoch: 3 [26624/60000 ( 44%)]\tLoss: 82.623627\n",
            "Train Epoch: 3 [27136/60000 ( 45%)]\tLoss: 78.749725\n",
            "Train Epoch: 3 [27648/60000 ( 46%)]\tLoss: 82.446777\n",
            "Train Epoch: 3 [28160/60000 ( 47%)]\tLoss: 81.052979\n",
            "Train Epoch: 3 [28672/60000 ( 48%)]\tLoss: 80.964478\n",
            "Train Epoch: 3 [29184/60000 ( 49%)]\tLoss: 78.458260\n",
            "Train Epoch: 3 [29696/60000 ( 49%)]\tLoss: 78.952530\n",
            "Train Epoch: 3 [30208/60000 ( 50%)]\tLoss: 78.209023\n",
            "Train Epoch: 3 [30720/60000 ( 51%)]\tLoss: 80.795883\n",
            "Train Epoch: 3 [31232/60000 ( 52%)]\tLoss: 80.261917\n",
            "Train Epoch: 3 [31744/60000 ( 53%)]\tLoss: 79.528847\n",
            "Train Epoch: 3 [32256/60000 ( 54%)]\tLoss: 82.865143\n",
            "Train Epoch: 3 [32768/60000 ( 54%)]\tLoss: 82.226074\n",
            "Train Epoch: 3 [33280/60000 ( 55%)]\tLoss: 78.488564\n",
            "Train Epoch: 3 [33792/60000 ( 56%)]\tLoss: 79.031357\n",
            "Train Epoch: 3 [34304/60000 ( 57%)]\tLoss: 77.908463\n",
            "Train Epoch: 3 [34816/60000 ( 58%)]\tLoss: 78.715767\n",
            "Train Epoch: 3 [35328/60000 ( 59%)]\tLoss: 80.603836\n",
            "Train Epoch: 3 [35840/60000 ( 60%)]\tLoss: 80.624527\n",
            "Train Epoch: 3 [36352/60000 ( 60%)]\tLoss: 80.129776\n",
            "Train Epoch: 3 [36864/60000 ( 61%)]\tLoss: 78.862015\n",
            "Train Epoch: 3 [37376/60000 ( 62%)]\tLoss: 77.157265\n",
            "Train Epoch: 3 [37888/60000 ( 63%)]\tLoss: 78.648087\n",
            "Train Epoch: 3 [38400/60000 ( 64%)]\tLoss: 78.522888\n",
            "Train Epoch: 3 [38912/60000 ( 65%)]\tLoss: 81.155144\n",
            "Train Epoch: 3 [39424/60000 ( 66%)]\tLoss: 78.136772\n",
            "Train Epoch: 3 [39936/60000 ( 66%)]\tLoss: 82.706375\n",
            "Train Epoch: 3 [40448/60000 ( 67%)]\tLoss: 80.195221\n",
            "Train Epoch: 3 [40960/60000 ( 68%)]\tLoss: 77.257690\n",
            "Train Epoch: 3 [41472/60000 ( 69%)]\tLoss: 83.972626\n",
            "Train Epoch: 3 [41984/60000 ( 70%)]\tLoss: 76.789772\n",
            "Train Epoch: 3 [42496/60000 ( 71%)]\tLoss: 75.620827\n",
            "Train Epoch: 3 [43008/60000 ( 71%)]\tLoss: 82.539688\n",
            "Train Epoch: 3 [43520/60000 ( 72%)]\tLoss: 80.892319\n",
            "Train Epoch: 3 [44032/60000 ( 73%)]\tLoss: 78.834106\n",
            "Train Epoch: 3 [44544/60000 ( 74%)]\tLoss: 82.513092\n",
            "Train Epoch: 3 [45056/60000 ( 75%)]\tLoss: 81.804840\n",
            "Train Epoch: 3 [45568/60000 ( 76%)]\tLoss: 77.109100\n",
            "Train Epoch: 3 [46080/60000 ( 77%)]\tLoss: 79.129807\n",
            "Train Epoch: 3 [46592/60000 ( 77%)]\tLoss: 79.541496\n",
            "Train Epoch: 3 [47104/60000 ( 78%)]\tLoss: 79.242050\n",
            "Train Epoch: 3 [47616/60000 ( 79%)]\tLoss: 76.369843\n",
            "Train Epoch: 3 [48128/60000 ( 80%)]\tLoss: 73.934929\n",
            "Train Epoch: 3 [48640/60000 ( 81%)]\tLoss: 76.499634\n",
            "Train Epoch: 3 [49152/60000 ( 82%)]\tLoss: 75.813889\n",
            "Train Epoch: 3 [49664/60000 ( 83%)]\tLoss: 78.092941\n",
            "Train Epoch: 3 [50176/60000 ( 83%)]\tLoss: 76.940079\n",
            "Train Epoch: 3 [50688/60000 ( 84%)]\tLoss: 77.961472\n",
            "Train Epoch: 3 [51200/60000 ( 85%)]\tLoss: 78.066772\n",
            "Train Epoch: 3 [51712/60000 ( 86%)]\tLoss: 78.612259\n",
            "Train Epoch: 3 [52224/60000 ( 87%)]\tLoss: 75.052223\n",
            "Train Epoch: 3 [52736/60000 ( 88%)]\tLoss: 76.834999\n",
            "Train Epoch: 3 [53248/60000 ( 89%)]\tLoss: 78.112366\n",
            "Train Epoch: 3 [53760/60000 ( 89%)]\tLoss: 75.606560\n",
            "Train Epoch: 3 [54272/60000 ( 90%)]\tLoss: 76.039253\n",
            "Train Epoch: 3 [54784/60000 ( 91%)]\tLoss: 77.045334\n",
            "Train Epoch: 3 [55296/60000 ( 92%)]\tLoss: 80.923790\n",
            "Train Epoch: 3 [55808/60000 ( 93%)]\tLoss: 71.932739\n",
            "Train Epoch: 3 [56320/60000 ( 94%)]\tLoss: 77.238869\n",
            "Train Epoch: 3 [56832/60000 ( 94%)]\tLoss: 79.439598\n",
            "Train Epoch: 3 [57344/60000 ( 95%)]\tLoss: 76.700676\n",
            "Train Epoch: 3 [57856/60000 ( 96%)]\tLoss: 76.134796\n",
            "Train Epoch: 3 [58368/60000 ( 97%)]\tLoss: 77.494659\n",
            "Train Epoch: 3 [58880/60000 ( 98%)]\tLoss: 72.353683\n",
            "Train Epoch: 3 [59392/60000 ( 99%)]\tLoss: 77.290001\n",
            "Train Epoch: 3 [22464/60000 (100%)]\tLoss: 77.735291\n",
            "====> Epoch: 3 Average loss: 81.5718\n",
            "====> Test set loss: 80.0590\n",
            "Train Epoch: 4 [    0/60000 (  0%)]\tLoss: 81.283585\n",
            "Train Epoch: 4 [  512/60000 (  1%)]\tLoss: 73.255844\n",
            "Train Epoch: 4 [ 1024/60000 (  2%)]\tLoss: 75.902435\n",
            "Train Epoch: 4 [ 1536/60000 (  3%)]\tLoss: 74.267784\n",
            "Train Epoch: 4 [ 2048/60000 (  3%)]\tLoss: 79.841217\n",
            "Train Epoch: 4 [ 2560/60000 (  4%)]\tLoss: 70.529701\n",
            "Train Epoch: 4 [ 3072/60000 (  5%)]\tLoss: 75.287491\n",
            "Train Epoch: 4 [ 3584/60000 (  6%)]\tLoss: 77.570839\n",
            "Train Epoch: 4 [ 4096/60000 (  7%)]\tLoss: 81.449379\n",
            "Train Epoch: 4 [ 4608/60000 (  8%)]\tLoss: 71.706635\n",
            "Train Epoch: 4 [ 5120/60000 (  9%)]\tLoss: 78.505096\n",
            "Train Epoch: 4 [ 5632/60000 (  9%)]\tLoss: 77.036514\n",
            "Train Epoch: 4 [ 6144/60000 ( 10%)]\tLoss: 73.439713\n",
            "Train Epoch: 4 [ 6656/60000 ( 11%)]\tLoss: 75.958809\n",
            "Train Epoch: 4 [ 7168/60000 ( 12%)]\tLoss: 73.140366\n",
            "Train Epoch: 4 [ 7680/60000 ( 13%)]\tLoss: 73.631912\n",
            "Train Epoch: 4 [ 8192/60000 ( 14%)]\tLoss: 75.036682\n",
            "Train Epoch: 4 [ 8704/60000 ( 14%)]\tLoss: 75.304214\n",
            "Train Epoch: 4 [ 9216/60000 ( 15%)]\tLoss: 78.399902\n",
            "Train Epoch: 4 [ 9728/60000 ( 16%)]\tLoss: 75.559692\n",
            "Train Epoch: 4 [10240/60000 ( 17%)]\tLoss: 77.452087\n",
            "Train Epoch: 4 [10752/60000 ( 18%)]\tLoss: 80.648270\n",
            "Train Epoch: 4 [11264/60000 ( 19%)]\tLoss: 76.697273\n",
            "Train Epoch: 4 [11776/60000 ( 20%)]\tLoss: 72.977615\n",
            "Train Epoch: 4 [12288/60000 ( 20%)]\tLoss: 75.200912\n",
            "Train Epoch: 4 [12800/60000 ( 21%)]\tLoss: 71.812790\n",
            "Train Epoch: 4 [13312/60000 ( 22%)]\tLoss: 72.314850\n",
            "Train Epoch: 4 [13824/60000 ( 23%)]\tLoss: 72.506935\n",
            "Train Epoch: 4 [14336/60000 ( 24%)]\tLoss: 74.025040\n",
            "Train Epoch: 4 [14848/60000 ( 25%)]\tLoss: 69.134003\n",
            "Train Epoch: 4 [15360/60000 ( 26%)]\tLoss: 74.381088\n",
            "Train Epoch: 4 [15872/60000 ( 26%)]\tLoss: 74.144814\n",
            "Train Epoch: 4 [16384/60000 ( 27%)]\tLoss: 75.393410\n",
            "Train Epoch: 4 [16896/60000 ( 28%)]\tLoss: 76.830612\n",
            "Train Epoch: 4 [17408/60000 ( 29%)]\tLoss: 73.745544\n",
            "Train Epoch: 4 [17920/60000 ( 30%)]\tLoss: 77.656982\n",
            "Train Epoch: 4 [18432/60000 ( 31%)]\tLoss: 77.352440\n",
            "Train Epoch: 4 [18944/60000 ( 31%)]\tLoss: 73.238525\n",
            "Train Epoch: 4 [19456/60000 ( 32%)]\tLoss: 71.656662\n",
            "Train Epoch: 4 [19968/60000 ( 33%)]\tLoss: 71.918304\n",
            "Train Epoch: 4 [20480/60000 ( 34%)]\tLoss: 75.223366\n",
            "Train Epoch: 4 [20992/60000 ( 35%)]\tLoss: 71.200836\n",
            "Train Epoch: 4 [21504/60000 ( 36%)]\tLoss: 78.135300\n",
            "Train Epoch: 4 [22016/60000 ( 37%)]\tLoss: 73.971092\n",
            "Train Epoch: 4 [22528/60000 ( 37%)]\tLoss: 73.897926\n",
            "Train Epoch: 4 [23040/60000 ( 38%)]\tLoss: 77.525360\n",
            "Train Epoch: 4 [23552/60000 ( 39%)]\tLoss: 70.041595\n",
            "Train Epoch: 4 [24064/60000 ( 40%)]\tLoss: 74.047066\n",
            "Train Epoch: 4 [24576/60000 ( 41%)]\tLoss: 69.104767\n",
            "Train Epoch: 4 [25088/60000 ( 42%)]\tLoss: 80.005875\n",
            "Train Epoch: 4 [25600/60000 ( 43%)]\tLoss: 75.029381\n",
            "Train Epoch: 4 [26112/60000 ( 43%)]\tLoss: 74.936050\n",
            "Train Epoch: 4 [26624/60000 ( 44%)]\tLoss: 72.347183\n",
            "Train Epoch: 4 [27136/60000 ( 45%)]\tLoss: 70.543556\n",
            "Train Epoch: 4 [27648/60000 ( 46%)]\tLoss: 71.219521\n",
            "Train Epoch: 4 [28160/60000 ( 47%)]\tLoss: 73.113785\n",
            "Train Epoch: 4 [28672/60000 ( 48%)]\tLoss: 74.083214\n",
            "Train Epoch: 4 [29184/60000 ( 49%)]\tLoss: 72.604248\n",
            "Train Epoch: 4 [29696/60000 ( 49%)]\tLoss: 72.173370\n",
            "Train Epoch: 4 [30208/60000 ( 50%)]\tLoss: 74.632385\n",
            "Train Epoch: 4 [30720/60000 ( 51%)]\tLoss: 69.994850\n",
            "Train Epoch: 4 [31232/60000 ( 52%)]\tLoss: 71.975571\n",
            "Train Epoch: 4 [31744/60000 ( 53%)]\tLoss: 72.772110\n",
            "Train Epoch: 4 [32256/60000 ( 54%)]\tLoss: 71.062355\n",
            "Train Epoch: 4 [32768/60000 ( 54%)]\tLoss: 81.419678\n",
            "Train Epoch: 4 [33280/60000 ( 55%)]\tLoss: 72.499420\n",
            "Train Epoch: 4 [33792/60000 ( 56%)]\tLoss: 75.857903\n",
            "Train Epoch: 4 [34304/60000 ( 57%)]\tLoss: 70.866577\n",
            "Train Epoch: 4 [34816/60000 ( 58%)]\tLoss: 76.278511\n",
            "Train Epoch: 4 [35328/60000 ( 59%)]\tLoss: 73.656479\n",
            "Train Epoch: 4 [35840/60000 ( 60%)]\tLoss: 70.455421\n",
            "Train Epoch: 4 [36352/60000 ( 60%)]\tLoss: 74.559914\n",
            "Train Epoch: 4 [36864/60000 ( 61%)]\tLoss: 71.699905\n",
            "Train Epoch: 4 [37376/60000 ( 62%)]\tLoss: 71.508713\n",
            "Train Epoch: 4 [37888/60000 ( 63%)]\tLoss: 74.278099\n",
            "Train Epoch: 4 [38400/60000 ( 64%)]\tLoss: 79.206833\n",
            "Train Epoch: 4 [38912/60000 ( 65%)]\tLoss: 73.323029\n",
            "Train Epoch: 4 [39424/60000 ( 66%)]\tLoss: 75.921768\n",
            "Train Epoch: 4 [39936/60000 ( 66%)]\tLoss: 76.480217\n",
            "Train Epoch: 4 [40448/60000 ( 67%)]\tLoss: 72.483543\n",
            "Train Epoch: 4 [40960/60000 ( 68%)]\tLoss: 75.361877\n",
            "Train Epoch: 4 [41472/60000 ( 69%)]\tLoss: 72.833832\n",
            "Train Epoch: 4 [41984/60000 ( 70%)]\tLoss: 73.589111\n",
            "Train Epoch: 4 [42496/60000 ( 71%)]\tLoss: 75.771667\n",
            "Train Epoch: 4 [43008/60000 ( 71%)]\tLoss: 72.691269\n",
            "Train Epoch: 4 [43520/60000 ( 72%)]\tLoss: 74.487473\n",
            "Train Epoch: 4 [44032/60000 ( 73%)]\tLoss: 73.872055\n",
            "Train Epoch: 4 [44544/60000 ( 74%)]\tLoss: 73.237869\n",
            "Train Epoch: 4 [45056/60000 ( 75%)]\tLoss: 73.311981\n",
            "Train Epoch: 4 [45568/60000 ( 76%)]\tLoss: 71.465668\n",
            "Train Epoch: 4 [46080/60000 ( 77%)]\tLoss: 71.691795\n",
            "Train Epoch: 4 [46592/60000 ( 77%)]\tLoss: 74.007591\n",
            "Train Epoch: 4 [47104/60000 ( 78%)]\tLoss: 71.585426\n",
            "Train Epoch: 4 [47616/60000 ( 79%)]\tLoss: 72.167976\n",
            "Train Epoch: 4 [48128/60000 ( 80%)]\tLoss: 70.234711\n",
            "Train Epoch: 4 [48640/60000 ( 81%)]\tLoss: 74.135529\n",
            "Train Epoch: 4 [49152/60000 ( 82%)]\tLoss: 71.991318\n",
            "Train Epoch: 4 [49664/60000 ( 83%)]\tLoss: 71.494469\n",
            "Train Epoch: 4 [50176/60000 ( 83%)]\tLoss: 71.004112\n",
            "Train Epoch: 4 [50688/60000 ( 84%)]\tLoss: 70.848289\n",
            "Train Epoch: 4 [51200/60000 ( 85%)]\tLoss: 70.781151\n",
            "Train Epoch: 4 [51712/60000 ( 86%)]\tLoss: 73.672760\n",
            "Train Epoch: 4 [52224/60000 ( 87%)]\tLoss: 69.193199\n",
            "Train Epoch: 4 [52736/60000 ( 88%)]\tLoss: 73.598557\n",
            "Train Epoch: 4 [53248/60000 ( 89%)]\tLoss: 72.017914\n",
            "Train Epoch: 4 [53760/60000 ( 89%)]\tLoss: 69.376282\n",
            "Train Epoch: 4 [54272/60000 ( 90%)]\tLoss: 69.237503\n",
            "Train Epoch: 4 [54784/60000 ( 91%)]\tLoss: 70.217735\n",
            "Train Epoch: 4 [55296/60000 ( 92%)]\tLoss: 70.532372\n",
            "Train Epoch: 4 [55808/60000 ( 93%)]\tLoss: 71.286896\n",
            "Train Epoch: 4 [56320/60000 ( 94%)]\tLoss: 68.483437\n",
            "Train Epoch: 4 [56832/60000 ( 94%)]\tLoss: 74.794876\n",
            "Train Epoch: 4 [57344/60000 ( 95%)]\tLoss: 73.674583\n",
            "Train Epoch: 4 [57856/60000 ( 96%)]\tLoss: 71.579735\n",
            "Train Epoch: 4 [58368/60000 ( 97%)]\tLoss: 70.292755\n",
            "Train Epoch: 4 [58880/60000 ( 98%)]\tLoss: 71.281555\n",
            "Train Epoch: 4 [59392/60000 ( 99%)]\tLoss: 70.263329\n",
            "Train Epoch: 4 [22464/60000 (100%)]\tLoss: 68.332296\n",
            "====> Epoch: 4 Average loss: 73.9429\n",
            "====> Test set loss: 72.4618\n",
            "Train Epoch: 5 [    0/60000 (  0%)]\tLoss: 75.792465\n",
            "Train Epoch: 5 [  512/60000 (  1%)]\tLoss: 72.863358\n",
            "Train Epoch: 5 [ 1024/60000 (  2%)]\tLoss: 73.143814\n",
            "Train Epoch: 5 [ 1536/60000 (  3%)]\tLoss: 71.986534\n",
            "Train Epoch: 5 [ 2048/60000 (  3%)]\tLoss: 74.787735\n",
            "Train Epoch: 5 [ 2560/60000 (  4%)]\tLoss: 71.893517\n",
            "Train Epoch: 5 [ 3072/60000 (  5%)]\tLoss: 71.864052\n",
            "Train Epoch: 5 [ 3584/60000 (  6%)]\tLoss: 72.706139\n",
            "Train Epoch: 5 [ 4096/60000 (  7%)]\tLoss: 71.754272\n",
            "Train Epoch: 5 [ 4608/60000 (  8%)]\tLoss: 72.055794\n",
            "Train Epoch: 5 [ 5120/60000 (  9%)]\tLoss: 70.544724\n",
            "Train Epoch: 5 [ 5632/60000 (  9%)]\tLoss: 75.260895\n",
            "Train Epoch: 5 [ 6144/60000 ( 10%)]\tLoss: 71.543549\n",
            "Train Epoch: 5 [ 6656/60000 ( 11%)]\tLoss: 73.311447\n",
            "Train Epoch: 5 [ 7168/60000 ( 12%)]\tLoss: 71.006676\n",
            "Train Epoch: 5 [ 7680/60000 ( 13%)]\tLoss: 68.408592\n",
            "Train Epoch: 5 [ 8192/60000 ( 14%)]\tLoss: 72.141724\n",
            "Train Epoch: 5 [ 8704/60000 ( 14%)]\tLoss: 70.198662\n",
            "Train Epoch: 5 [ 9216/60000 ( 15%)]\tLoss: 70.202271\n",
            "Train Epoch: 5 [ 9728/60000 ( 16%)]\tLoss: 68.915924\n",
            "Train Epoch: 5 [10240/60000 ( 17%)]\tLoss: 71.118309\n",
            "Train Epoch: 5 [10752/60000 ( 18%)]\tLoss: 69.139481\n",
            "Train Epoch: 5 [11264/60000 ( 19%)]\tLoss: 73.234726\n",
            "Train Epoch: 5 [11776/60000 ( 20%)]\tLoss: 68.406311\n",
            "Train Epoch: 5 [12288/60000 ( 20%)]\tLoss: 69.714600\n",
            "Train Epoch: 5 [12800/60000 ( 21%)]\tLoss: 68.818390\n",
            "Train Epoch: 5 [13312/60000 ( 22%)]\tLoss: 73.560928\n",
            "Train Epoch: 5 [13824/60000 ( 23%)]\tLoss: 70.795074\n",
            "Train Epoch: 5 [14336/60000 ( 24%)]\tLoss: 68.806931\n",
            "Train Epoch: 5 [14848/60000 ( 25%)]\tLoss: 68.411171\n",
            "Train Epoch: 5 [15360/60000 ( 26%)]\tLoss: 73.441566\n",
            "Train Epoch: 5 [15872/60000 ( 26%)]\tLoss: 71.549347\n",
            "Train Epoch: 5 [16384/60000 ( 27%)]\tLoss: 73.513290\n",
            "Train Epoch: 5 [16896/60000 ( 28%)]\tLoss: 71.736908\n",
            "Train Epoch: 5 [17408/60000 ( 29%)]\tLoss: 75.160583\n",
            "Train Epoch: 5 [17920/60000 ( 30%)]\tLoss: 69.407089\n",
            "Train Epoch: 5 [18432/60000 ( 31%)]\tLoss: 66.654198\n",
            "Train Epoch: 5 [18944/60000 ( 31%)]\tLoss: 69.730759\n",
            "Train Epoch: 5 [19456/60000 ( 32%)]\tLoss: 68.793365\n",
            "Train Epoch: 5 [19968/60000 ( 33%)]\tLoss: 68.947029\n",
            "Train Epoch: 5 [20480/60000 ( 34%)]\tLoss: 70.399200\n",
            "Train Epoch: 5 [20992/60000 ( 35%)]\tLoss: 71.934723\n",
            "Train Epoch: 5 [21504/60000 ( 36%)]\tLoss: 69.611160\n",
            "Train Epoch: 5 [22016/60000 ( 37%)]\tLoss: 70.218933\n",
            "Train Epoch: 5 [22528/60000 ( 37%)]\tLoss: 69.467743\n",
            "Train Epoch: 5 [23040/60000 ( 38%)]\tLoss: 69.301964\n",
            "Train Epoch: 5 [23552/60000 ( 39%)]\tLoss: 71.429199\n",
            "Train Epoch: 5 [24064/60000 ( 40%)]\tLoss: 71.489914\n",
            "Train Epoch: 5 [24576/60000 ( 41%)]\tLoss: 71.022766\n",
            "Train Epoch: 5 [25088/60000 ( 42%)]\tLoss: 70.715698\n",
            "Train Epoch: 5 [25600/60000 ( 43%)]\tLoss: 66.832001\n",
            "Train Epoch: 5 [26112/60000 ( 43%)]\tLoss: 68.660156\n",
            "Train Epoch: 5 [26624/60000 ( 44%)]\tLoss: 68.944710\n",
            "Train Epoch: 5 [27136/60000 ( 45%)]\tLoss: 70.697906\n",
            "Train Epoch: 5 [27648/60000 ( 46%)]\tLoss: 71.705124\n",
            "Train Epoch: 5 [28160/60000 ( 47%)]\tLoss: 70.119415\n",
            "Train Epoch: 5 [28672/60000 ( 48%)]\tLoss: 65.897430\n",
            "Train Epoch: 5 [29184/60000 ( 49%)]\tLoss: 72.562828\n",
            "Train Epoch: 5 [29696/60000 ( 49%)]\tLoss: 73.864319\n",
            "Train Epoch: 5 [30208/60000 ( 50%)]\tLoss: 71.797600\n",
            "Train Epoch: 5 [30720/60000 ( 51%)]\tLoss: 69.420753\n",
            "Train Epoch: 5 [31232/60000 ( 52%)]\tLoss: 74.428268\n",
            "Train Epoch: 5 [31744/60000 ( 53%)]\tLoss: 70.377213\n",
            "Train Epoch: 5 [32256/60000 ( 54%)]\tLoss: 68.283340\n",
            "Train Epoch: 5 [32768/60000 ( 54%)]\tLoss: 69.727112\n",
            "Train Epoch: 5 [33280/60000 ( 55%)]\tLoss: 67.756622\n",
            "Train Epoch: 5 [33792/60000 ( 56%)]\tLoss: 67.682899\n",
            "Train Epoch: 5 [34304/60000 ( 57%)]\tLoss: 70.891708\n",
            "Train Epoch: 5 [34816/60000 ( 58%)]\tLoss: 72.248528\n",
            "Train Epoch: 5 [35328/60000 ( 59%)]\tLoss: 69.923874\n",
            "Train Epoch: 5 [35840/60000 ( 60%)]\tLoss: 67.784401\n",
            "Train Epoch: 5 [36352/60000 ( 60%)]\tLoss: 67.309174\n",
            "Train Epoch: 5 [36864/60000 ( 61%)]\tLoss: 69.714378\n",
            "Train Epoch: 5 [37376/60000 ( 62%)]\tLoss: 68.779327\n",
            "Train Epoch: 5 [37888/60000 ( 63%)]\tLoss: 68.668854\n",
            "Train Epoch: 5 [38400/60000 ( 64%)]\tLoss: 69.671677\n",
            "Train Epoch: 5 [38912/60000 ( 65%)]\tLoss: 70.531868\n",
            "Train Epoch: 5 [39424/60000 ( 66%)]\tLoss: 68.364868\n",
            "Train Epoch: 5 [39936/60000 ( 66%)]\tLoss: 68.009697\n",
            "Train Epoch: 5 [40448/60000 ( 67%)]\tLoss: 70.413017\n",
            "Train Epoch: 5 [40960/60000 ( 68%)]\tLoss: 68.703972\n",
            "Train Epoch: 5 [41472/60000 ( 69%)]\tLoss: 67.263901\n",
            "Train Epoch: 5 [41984/60000 ( 70%)]\tLoss: 68.281448\n",
            "Train Epoch: 5 [42496/60000 ( 71%)]\tLoss: 65.992249\n",
            "Train Epoch: 5 [43008/60000 ( 71%)]\tLoss: 69.311249\n",
            "Train Epoch: 5 [43520/60000 ( 72%)]\tLoss: 69.367638\n",
            "Train Epoch: 5 [44032/60000 ( 73%)]\tLoss: 71.566910\n",
            "Train Epoch: 5 [44544/60000 ( 74%)]\tLoss: 69.272903\n",
            "Train Epoch: 5 [45056/60000 ( 75%)]\tLoss: 69.533875\n",
            "Train Epoch: 5 [45568/60000 ( 76%)]\tLoss: 66.409271\n",
            "Train Epoch: 5 [46080/60000 ( 77%)]\tLoss: 66.182449\n",
            "Train Epoch: 5 [46592/60000 ( 77%)]\tLoss: 67.639450\n",
            "Train Epoch: 5 [47104/60000 ( 78%)]\tLoss: 74.441528\n",
            "Train Epoch: 5 [47616/60000 ( 79%)]\tLoss: 68.486450\n",
            "Train Epoch: 5 [48128/60000 ( 80%)]\tLoss: 65.246521\n",
            "Train Epoch: 5 [48640/60000 ( 81%)]\tLoss: 68.175377\n",
            "Train Epoch: 5 [49152/60000 ( 82%)]\tLoss: 68.387695\n",
            "Train Epoch: 5 [49664/60000 ( 83%)]\tLoss: 69.294281\n",
            "Train Epoch: 5 [50176/60000 ( 83%)]\tLoss: 67.140640\n",
            "Train Epoch: 5 [50688/60000 ( 84%)]\tLoss: 69.185257\n",
            "Train Epoch: 5 [51200/60000 ( 85%)]\tLoss: 67.460258\n",
            "Train Epoch: 5 [51712/60000 ( 86%)]\tLoss: 69.695938\n",
            "Train Epoch: 5 [52224/60000 ( 87%)]\tLoss: 72.407646\n",
            "Train Epoch: 5 [52736/60000 ( 88%)]\tLoss: 70.277550\n",
            "Train Epoch: 5 [53248/60000 ( 89%)]\tLoss: 67.692215\n",
            "Train Epoch: 5 [53760/60000 ( 89%)]\tLoss: 69.676338\n",
            "Train Epoch: 5 [54272/60000 ( 90%)]\tLoss: 69.080482\n",
            "Train Epoch: 5 [54784/60000 ( 91%)]\tLoss: 68.125275\n",
            "Train Epoch: 5 [55296/60000 ( 92%)]\tLoss: 69.809380\n",
            "Train Epoch: 5 [55808/60000 ( 93%)]\tLoss: 71.018021\n",
            "Train Epoch: 5 [56320/60000 ( 94%)]\tLoss: 65.327118\n",
            "Train Epoch: 5 [56832/60000 ( 94%)]\tLoss: 67.501999\n",
            "Train Epoch: 5 [57344/60000 ( 95%)]\tLoss: 68.598885\n",
            "Train Epoch: 5 [57856/60000 ( 96%)]\tLoss: 69.671417\n",
            "Train Epoch: 5 [58368/60000 ( 97%)]\tLoss: 70.541908\n",
            "Train Epoch: 5 [58880/60000 ( 98%)]\tLoss: 70.881668\n",
            "Train Epoch: 5 [59392/60000 ( 99%)]\tLoss: 71.698982\n",
            "Train Epoch: 5 [22464/60000 (100%)]\tLoss: 64.130402\n",
            "====> Epoch: 5 Average loss: 70.0429\n",
            "====> Test set loss: 70.6013\n",
            "Train Epoch: 6 [    0/60000 (  0%)]\tLoss: 70.328659\n",
            "Train Epoch: 6 [  512/60000 (  1%)]\tLoss: 71.221420\n",
            "Train Epoch: 6 [ 1024/60000 (  2%)]\tLoss: 69.447876\n",
            "Train Epoch: 6 [ 1536/60000 (  3%)]\tLoss: 67.897232\n",
            "Train Epoch: 6 [ 2048/60000 (  3%)]\tLoss: 67.108994\n",
            "Train Epoch: 6 [ 2560/60000 (  4%)]\tLoss: 69.174919\n",
            "Train Epoch: 6 [ 3072/60000 (  5%)]\tLoss: 70.738724\n",
            "Train Epoch: 6 [ 3584/60000 (  6%)]\tLoss: 67.824310\n",
            "Train Epoch: 6 [ 4096/60000 (  7%)]\tLoss: 66.743027\n",
            "Train Epoch: 6 [ 4608/60000 (  8%)]\tLoss: 67.339691\n",
            "Train Epoch: 6 [ 5120/60000 (  9%)]\tLoss: 65.891693\n",
            "Train Epoch: 6 [ 5632/60000 (  9%)]\tLoss: 67.698540\n",
            "Train Epoch: 6 [ 6144/60000 ( 10%)]\tLoss: 65.014099\n",
            "Train Epoch: 6 [ 6656/60000 ( 11%)]\tLoss: 66.813538\n",
            "Train Epoch: 6 [ 7168/60000 ( 12%)]\tLoss: 66.263718\n",
            "Train Epoch: 6 [ 7680/60000 ( 13%)]\tLoss: 69.547615\n",
            "Train Epoch: 6 [ 8192/60000 ( 14%)]\tLoss: 68.995758\n",
            "Train Epoch: 6 [ 8704/60000 ( 14%)]\tLoss: 72.835907\n",
            "Train Epoch: 6 [ 9216/60000 ( 15%)]\tLoss: 65.759048\n",
            "Train Epoch: 6 [ 9728/60000 ( 16%)]\tLoss: 67.582954\n",
            "Train Epoch: 6 [10240/60000 ( 17%)]\tLoss: 66.029877\n",
            "Train Epoch: 6 [10752/60000 ( 18%)]\tLoss: 63.997818\n",
            "Train Epoch: 6 [11264/60000 ( 19%)]\tLoss: 68.095535\n",
            "Train Epoch: 6 [11776/60000 ( 20%)]\tLoss: 64.497528\n",
            "Train Epoch: 6 [12288/60000 ( 20%)]\tLoss: 64.281616\n",
            "Train Epoch: 6 [12800/60000 ( 21%)]\tLoss: 67.408043\n",
            "Train Epoch: 6 [13312/60000 ( 22%)]\tLoss: 69.036285\n",
            "Train Epoch: 6 [13824/60000 ( 23%)]\tLoss: 69.317078\n",
            "Train Epoch: 6 [14336/60000 ( 24%)]\tLoss: 70.997482\n",
            "Train Epoch: 6 [14848/60000 ( 25%)]\tLoss: 66.586075\n",
            "Train Epoch: 6 [15360/60000 ( 26%)]\tLoss: 69.813889\n",
            "Train Epoch: 6 [15872/60000 ( 26%)]\tLoss: 68.132179\n",
            "Train Epoch: 6 [16384/60000 ( 27%)]\tLoss: 67.671127\n",
            "Train Epoch: 6 [16896/60000 ( 28%)]\tLoss: 70.499252\n",
            "Train Epoch: 6 [17408/60000 ( 29%)]\tLoss: 66.824677\n",
            "Train Epoch: 6 [17920/60000 ( 30%)]\tLoss: 67.850616\n",
            "Train Epoch: 6 [18432/60000 ( 31%)]\tLoss: 70.594368\n",
            "Train Epoch: 6 [18944/60000 ( 31%)]\tLoss: 70.902588\n",
            "Train Epoch: 6 [19456/60000 ( 32%)]\tLoss: 66.260178\n",
            "Train Epoch: 6 [19968/60000 ( 33%)]\tLoss: 71.606949\n",
            "Train Epoch: 6 [20480/60000 ( 34%)]\tLoss: 74.441055\n",
            "Train Epoch: 6 [20992/60000 ( 35%)]\tLoss: 67.921616\n",
            "Train Epoch: 6 [21504/60000 ( 36%)]\tLoss: 68.842018\n",
            "Train Epoch: 6 [22016/60000 ( 37%)]\tLoss: 68.669113\n",
            "Train Epoch: 6 [22528/60000 ( 37%)]\tLoss: 67.213196\n",
            "Train Epoch: 6 [23040/60000 ( 38%)]\tLoss: 70.887878\n",
            "Train Epoch: 6 [23552/60000 ( 39%)]\tLoss: 65.712494\n",
            "Train Epoch: 6 [24064/60000 ( 40%)]\tLoss: 71.784515\n",
            "Train Epoch: 6 [24576/60000 ( 41%)]\tLoss: 66.454117\n",
            "Train Epoch: 6 [25088/60000 ( 42%)]\tLoss: 69.749878\n",
            "Train Epoch: 6 [25600/60000 ( 43%)]\tLoss: 69.651634\n",
            "Train Epoch: 6 [26112/60000 ( 43%)]\tLoss: 67.953537\n",
            "Train Epoch: 6 [26624/60000 ( 44%)]\tLoss: 66.595100\n",
            "Train Epoch: 6 [27136/60000 ( 45%)]\tLoss: 65.120911\n",
            "Train Epoch: 6 [27648/60000 ( 46%)]\tLoss: 63.791634\n",
            "Train Epoch: 6 [28160/60000 ( 47%)]\tLoss: 66.762047\n",
            "Train Epoch: 6 [28672/60000 ( 48%)]\tLoss: 65.921921\n",
            "Train Epoch: 6 [29184/60000 ( 49%)]\tLoss: 73.186760\n",
            "Train Epoch: 6 [29696/60000 ( 49%)]\tLoss: 64.308899\n",
            "Train Epoch: 6 [30208/60000 ( 50%)]\tLoss: 66.391876\n",
            "Train Epoch: 6 [30720/60000 ( 51%)]\tLoss: 69.158600\n",
            "Train Epoch: 6 [31232/60000 ( 52%)]\tLoss: 67.202530\n",
            "Train Epoch: 6 [31744/60000 ( 53%)]\tLoss: 70.232399\n",
            "Train Epoch: 6 [32256/60000 ( 54%)]\tLoss: 67.566895\n",
            "Train Epoch: 6 [32768/60000 ( 54%)]\tLoss: 67.142929\n",
            "Train Epoch: 6 [33280/60000 ( 55%)]\tLoss: 68.010025\n",
            "Train Epoch: 6 [33792/60000 ( 56%)]\tLoss: 62.555458\n",
            "Train Epoch: 6 [34304/60000 ( 57%)]\tLoss: 66.184914\n",
            "Train Epoch: 6 [34816/60000 ( 58%)]\tLoss: 67.063316\n",
            "Train Epoch: 6 [35328/60000 ( 59%)]\tLoss: 65.507767\n",
            "Train Epoch: 6 [35840/60000 ( 60%)]\tLoss: 72.037514\n",
            "Train Epoch: 6 [36352/60000 ( 60%)]\tLoss: 66.148293\n",
            "Train Epoch: 6 [36864/60000 ( 61%)]\tLoss: 67.515167\n",
            "Train Epoch: 6 [37376/60000 ( 62%)]\tLoss: 63.068192\n",
            "Train Epoch: 6 [37888/60000 ( 63%)]\tLoss: 67.604095\n",
            "Train Epoch: 6 [38400/60000 ( 64%)]\tLoss: 69.675819\n",
            "Train Epoch: 6 [38912/60000 ( 65%)]\tLoss: 66.921951\n",
            "Train Epoch: 6 [39424/60000 ( 66%)]\tLoss: 67.986549\n",
            "Train Epoch: 6 [39936/60000 ( 66%)]\tLoss: 68.298294\n",
            "Train Epoch: 6 [40448/60000 ( 67%)]\tLoss: 69.742035\n",
            "Train Epoch: 6 [40960/60000 ( 68%)]\tLoss: 71.867851\n",
            "Train Epoch: 6 [41472/60000 ( 69%)]\tLoss: 68.408707\n",
            "Train Epoch: 6 [41984/60000 ( 70%)]\tLoss: 69.116699\n",
            "Train Epoch: 6 [42496/60000 ( 71%)]\tLoss: 69.447121\n",
            "Train Epoch: 6 [43008/60000 ( 71%)]\tLoss: 67.316315\n",
            "Train Epoch: 6 [43520/60000 ( 72%)]\tLoss: 71.693085\n",
            "Train Epoch: 6 [44032/60000 ( 73%)]\tLoss: 66.119125\n",
            "Train Epoch: 6 [44544/60000 ( 74%)]\tLoss: 67.610184\n",
            "Train Epoch: 6 [45056/60000 ( 75%)]\tLoss: 63.547466\n",
            "Train Epoch: 6 [45568/60000 ( 76%)]\tLoss: 69.065369\n",
            "Train Epoch: 6 [46080/60000 ( 77%)]\tLoss: 67.921539\n",
            "Train Epoch: 6 [46592/60000 ( 77%)]\tLoss: 70.513474\n",
            "Train Epoch: 6 [47104/60000 ( 78%)]\tLoss: 67.534775\n",
            "Train Epoch: 6 [47616/60000 ( 79%)]\tLoss: 67.649544\n",
            "Train Epoch: 6 [48128/60000 ( 80%)]\tLoss: 66.788239\n",
            "Train Epoch: 6 [48640/60000 ( 81%)]\tLoss: 65.483482\n",
            "Train Epoch: 6 [49152/60000 ( 82%)]\tLoss: 69.962463\n",
            "Train Epoch: 6 [49664/60000 ( 83%)]\tLoss: 64.761917\n",
            "Train Epoch: 6 [50176/60000 ( 83%)]\tLoss: 67.796066\n",
            "Train Epoch: 6 [50688/60000 ( 84%)]\tLoss: 67.128807\n",
            "Train Epoch: 6 [51200/60000 ( 85%)]\tLoss: 68.973709\n",
            "Train Epoch: 6 [51712/60000 ( 86%)]\tLoss: 64.991806\n",
            "Train Epoch: 6 [52224/60000 ( 87%)]\tLoss: 65.529289\n",
            "Train Epoch: 6 [52736/60000 ( 88%)]\tLoss: 67.229691\n",
            "Train Epoch: 6 [53248/60000 ( 89%)]\tLoss: 66.588112\n",
            "Train Epoch: 6 [53760/60000 ( 89%)]\tLoss: 65.717575\n",
            "Train Epoch: 6 [54272/60000 ( 90%)]\tLoss: 65.821228\n",
            "Train Epoch: 6 [54784/60000 ( 91%)]\tLoss: 64.103989\n",
            "Train Epoch: 6 [55296/60000 ( 92%)]\tLoss: 69.664429\n",
            "Train Epoch: 6 [55808/60000 ( 93%)]\tLoss: 70.424675\n",
            "Train Epoch: 6 [56320/60000 ( 94%)]\tLoss: 66.047951\n",
            "Train Epoch: 6 [56832/60000 ( 94%)]\tLoss: 72.077026\n",
            "Train Epoch: 6 [57344/60000 ( 95%)]\tLoss: 69.832848\n",
            "Train Epoch: 6 [57856/60000 ( 96%)]\tLoss: 61.842159\n",
            "Train Epoch: 6 [58368/60000 ( 97%)]\tLoss: 66.021240\n",
            "Train Epoch: 6 [58880/60000 ( 98%)]\tLoss: 69.707520\n",
            "Train Epoch: 6 [59392/60000 ( 99%)]\tLoss: 67.404785\n",
            "Train Epoch: 6 [22464/60000 (100%)]\tLoss: 69.314657\n",
            "====> Epoch: 6 Average loss: 67.8349\n",
            "====> Test set loss: 68.8274\n",
            "Train Epoch: 7 [    0/60000 (  0%)]\tLoss: 67.347122\n",
            "Train Epoch: 7 [  512/60000 (  1%)]\tLoss: 66.070549\n",
            "Train Epoch: 7 [ 1024/60000 (  2%)]\tLoss: 68.678314\n",
            "Train Epoch: 7 [ 1536/60000 (  3%)]\tLoss: 65.672974\n",
            "Train Epoch: 7 [ 2048/60000 (  3%)]\tLoss: 63.807487\n",
            "Train Epoch: 7 [ 2560/60000 (  4%)]\tLoss: 62.088673\n",
            "Train Epoch: 7 [ 3072/60000 (  5%)]\tLoss: 65.997879\n",
            "Train Epoch: 7 [ 3584/60000 (  6%)]\tLoss: 64.104172\n",
            "Train Epoch: 7 [ 4096/60000 (  7%)]\tLoss: 67.053268\n",
            "Train Epoch: 7 [ 4608/60000 (  8%)]\tLoss: 67.727295\n",
            "Train Epoch: 7 [ 5120/60000 (  9%)]\tLoss: 63.995525\n",
            "Train Epoch: 7 [ 5632/60000 (  9%)]\tLoss: 67.163025\n",
            "Train Epoch: 7 [ 6144/60000 ( 10%)]\tLoss: 67.438553\n",
            "Train Epoch: 7 [ 6656/60000 ( 11%)]\tLoss: 64.197647\n",
            "Train Epoch: 7 [ 7168/60000 ( 12%)]\tLoss: 65.093384\n",
            "Train Epoch: 7 [ 7680/60000 ( 13%)]\tLoss: 66.346344\n",
            "Train Epoch: 7 [ 8192/60000 ( 14%)]\tLoss: 68.049789\n",
            "Train Epoch: 7 [ 8704/60000 ( 14%)]\tLoss: 67.924339\n",
            "Train Epoch: 7 [ 9216/60000 ( 15%)]\tLoss: 66.029114\n",
            "Train Epoch: 7 [ 9728/60000 ( 16%)]\tLoss: 61.360497\n",
            "Train Epoch: 7 [10240/60000 ( 17%)]\tLoss: 68.073883\n",
            "Train Epoch: 7 [10752/60000 ( 18%)]\tLoss: 68.716293\n",
            "Train Epoch: 7 [11264/60000 ( 19%)]\tLoss: 67.394516\n",
            "Train Epoch: 7 [11776/60000 ( 20%)]\tLoss: 67.769730\n",
            "Train Epoch: 7 [12288/60000 ( 20%)]\tLoss: 66.185715\n",
            "Train Epoch: 7 [12800/60000 ( 21%)]\tLoss: 63.797680\n",
            "Train Epoch: 7 [13312/60000 ( 22%)]\tLoss: 66.563072\n",
            "Train Epoch: 7 [13824/60000 ( 23%)]\tLoss: 70.445091\n",
            "Train Epoch: 7 [14336/60000 ( 24%)]\tLoss: 66.960175\n",
            "Train Epoch: 7 [14848/60000 ( 25%)]\tLoss: 64.108711\n",
            "Train Epoch: 7 [15360/60000 ( 26%)]\tLoss: 69.344254\n",
            "Train Epoch: 7 [15872/60000 ( 26%)]\tLoss: 68.534523\n",
            "Train Epoch: 7 [16384/60000 ( 27%)]\tLoss: 71.450607\n",
            "Train Epoch: 7 [16896/60000 ( 28%)]\tLoss: 65.162560\n",
            "Train Epoch: 7 [17408/60000 ( 29%)]\tLoss: 66.694641\n",
            "Train Epoch: 7 [17920/60000 ( 30%)]\tLoss: 65.573654\n",
            "Train Epoch: 7 [18432/60000 ( 31%)]\tLoss: 65.126144\n",
            "Train Epoch: 7 [18944/60000 ( 31%)]\tLoss: 66.972885\n",
            "Train Epoch: 7 [19456/60000 ( 32%)]\tLoss: 69.316933\n",
            "Train Epoch: 7 [19968/60000 ( 33%)]\tLoss: 66.024887\n",
            "Train Epoch: 7 [20480/60000 ( 34%)]\tLoss: 70.046204\n",
            "Train Epoch: 7 [20992/60000 ( 35%)]\tLoss: 71.558937\n",
            "Train Epoch: 7 [21504/60000 ( 36%)]\tLoss: 64.778839\n",
            "Train Epoch: 7 [22016/60000 ( 37%)]\tLoss: 63.856739\n",
            "Train Epoch: 7 [22528/60000 ( 37%)]\tLoss: 62.991806\n",
            "Train Epoch: 7 [23040/60000 ( 38%)]\tLoss: 61.387924\n",
            "Train Epoch: 7 [23552/60000 ( 39%)]\tLoss: 65.423218\n",
            "Train Epoch: 7 [24064/60000 ( 40%)]\tLoss: 65.674240\n",
            "Train Epoch: 7 [24576/60000 ( 41%)]\tLoss: 65.779114\n",
            "Train Epoch: 7 [25088/60000 ( 42%)]\tLoss: 66.254204\n",
            "Train Epoch: 7 [25600/60000 ( 43%)]\tLoss: 61.362835\n",
            "Train Epoch: 7 [26112/60000 ( 43%)]\tLoss: 64.857758\n",
            "Train Epoch: 7 [26624/60000 ( 44%)]\tLoss: 64.052658\n",
            "Train Epoch: 7 [27136/60000 ( 45%)]\tLoss: 65.378403\n",
            "Train Epoch: 7 [27648/60000 ( 46%)]\tLoss: 66.590431\n",
            "Train Epoch: 7 [28160/60000 ( 47%)]\tLoss: 66.333710\n",
            "Train Epoch: 7 [28672/60000 ( 48%)]\tLoss: 65.497330\n",
            "Train Epoch: 7 [29184/60000 ( 49%)]\tLoss: 64.976532\n",
            "Train Epoch: 7 [29696/60000 ( 49%)]\tLoss: 65.865936\n",
            "Train Epoch: 7 [30208/60000 ( 50%)]\tLoss: 65.277420\n",
            "Train Epoch: 7 [30720/60000 ( 51%)]\tLoss: 71.370361\n",
            "Train Epoch: 7 [31232/60000 ( 52%)]\tLoss: 63.320869\n",
            "Train Epoch: 7 [31744/60000 ( 53%)]\tLoss: 69.679893\n",
            "Train Epoch: 7 [32256/60000 ( 54%)]\tLoss: 67.331375\n",
            "Train Epoch: 7 [32768/60000 ( 54%)]\tLoss: 66.396904\n",
            "Train Epoch: 7 [33280/60000 ( 55%)]\tLoss: 69.058266\n",
            "Train Epoch: 7 [33792/60000 ( 56%)]\tLoss: 67.165245\n",
            "Train Epoch: 7 [34304/60000 ( 57%)]\tLoss: 67.264297\n",
            "Train Epoch: 7 [34816/60000 ( 58%)]\tLoss: 66.375061\n",
            "Train Epoch: 7 [35328/60000 ( 59%)]\tLoss: 65.008049\n",
            "Train Epoch: 7 [35840/60000 ( 60%)]\tLoss: 66.156311\n",
            "Train Epoch: 7 [36352/60000 ( 60%)]\tLoss: 63.932899\n",
            "Train Epoch: 7 [36864/60000 ( 61%)]\tLoss: 66.873825\n",
            "Train Epoch: 7 [37376/60000 ( 62%)]\tLoss: 64.795044\n",
            "Train Epoch: 7 [37888/60000 ( 63%)]\tLoss: 63.899426\n",
            "Train Epoch: 7 [38400/60000 ( 64%)]\tLoss: 65.769760\n",
            "Train Epoch: 7 [38912/60000 ( 65%)]\tLoss: 64.962952\n",
            "Train Epoch: 7 [39424/60000 ( 66%)]\tLoss: 64.111206\n",
            "Train Epoch: 7 [39936/60000 ( 66%)]\tLoss: 67.168274\n",
            "Train Epoch: 7 [40448/60000 ( 67%)]\tLoss: 67.280640\n",
            "Train Epoch: 7 [40960/60000 ( 68%)]\tLoss: 63.450085\n",
            "Train Epoch: 7 [41472/60000 ( 69%)]\tLoss: 64.724564\n",
            "Train Epoch: 7 [41984/60000 ( 70%)]\tLoss: 68.913437\n",
            "Train Epoch: 7 [42496/60000 ( 71%)]\tLoss: 65.476898\n",
            "Train Epoch: 7 [43008/60000 ( 71%)]\tLoss: 64.396866\n",
            "Train Epoch: 7 [43520/60000 ( 72%)]\tLoss: 63.509476\n",
            "Train Epoch: 7 [44032/60000 ( 73%)]\tLoss: 64.156143\n",
            "Train Epoch: 7 [44544/60000 ( 74%)]\tLoss: 67.145729\n",
            "Train Epoch: 7 [45056/60000 ( 75%)]\tLoss: 63.755684\n",
            "Train Epoch: 7 [45568/60000 ( 76%)]\tLoss: 65.092529\n",
            "Train Epoch: 7 [46080/60000 ( 77%)]\tLoss: 70.242516\n",
            "Train Epoch: 7 [46592/60000 ( 77%)]\tLoss: 66.518593\n",
            "Train Epoch: 7 [47104/60000 ( 78%)]\tLoss: 67.187408\n",
            "Train Epoch: 7 [47616/60000 ( 79%)]\tLoss: 67.346939\n",
            "Train Epoch: 7 [48128/60000 ( 80%)]\tLoss: 68.444687\n",
            "Train Epoch: 7 [48640/60000 ( 81%)]\tLoss: 63.461861\n",
            "Train Epoch: 7 [49152/60000 ( 82%)]\tLoss: 68.192886\n",
            "Train Epoch: 7 [49664/60000 ( 83%)]\tLoss: 65.688408\n",
            "Train Epoch: 7 [50176/60000 ( 83%)]\tLoss: 66.129677\n",
            "Train Epoch: 7 [50688/60000 ( 84%)]\tLoss: 67.822464\n",
            "Train Epoch: 7 [51200/60000 ( 85%)]\tLoss: 66.618225\n",
            "Train Epoch: 7 [51712/60000 ( 86%)]\tLoss: 65.630203\n",
            "Train Epoch: 7 [52224/60000 ( 87%)]\tLoss: 65.603935\n",
            "Train Epoch: 7 [52736/60000 ( 88%)]\tLoss: 62.232025\n",
            "Train Epoch: 7 [53248/60000 ( 89%)]\tLoss: 65.832397\n",
            "Train Epoch: 7 [53760/60000 ( 89%)]\tLoss: 67.852715\n",
            "Train Epoch: 7 [54272/60000 ( 90%)]\tLoss: 68.543518\n",
            "Train Epoch: 7 [54784/60000 ( 91%)]\tLoss: 68.384407\n",
            "Train Epoch: 7 [55296/60000 ( 92%)]\tLoss: 63.567890\n",
            "Train Epoch: 7 [55808/60000 ( 93%)]\tLoss: 64.183640\n",
            "Train Epoch: 7 [56320/60000 ( 94%)]\tLoss: 63.877480\n",
            "Train Epoch: 7 [56832/60000 ( 94%)]\tLoss: 63.871849\n",
            "Train Epoch: 7 [57344/60000 ( 95%)]\tLoss: 67.655273\n",
            "Train Epoch: 7 [57856/60000 ( 96%)]\tLoss: 67.660355\n",
            "Train Epoch: 7 [58368/60000 ( 97%)]\tLoss: 62.067009\n",
            "Train Epoch: 7 [58880/60000 ( 98%)]\tLoss: 63.526943\n",
            "Train Epoch: 7 [59392/60000 ( 99%)]\tLoss: 65.916473\n",
            "Train Epoch: 7 [22464/60000 (100%)]\tLoss: 65.010824\n",
            "====> Epoch: 7 Average loss: 66.1150\n",
            "====> Test set loss: 67.6499\n",
            "Train Epoch: 8 [    0/60000 (  0%)]\tLoss: 63.831486\n",
            "Train Epoch: 8 [  512/60000 (  1%)]\tLoss: 64.298264\n",
            "Train Epoch: 8 [ 1024/60000 (  2%)]\tLoss: 64.270020\n",
            "Train Epoch: 8 [ 1536/60000 (  3%)]\tLoss: 63.634418\n",
            "Train Epoch: 8 [ 2048/60000 (  3%)]\tLoss: 64.310684\n",
            "Train Epoch: 8 [ 2560/60000 (  4%)]\tLoss: 67.023666\n",
            "Train Epoch: 8 [ 3072/60000 (  5%)]\tLoss: 66.257950\n",
            "Train Epoch: 8 [ 3584/60000 (  6%)]\tLoss: 67.314697\n",
            "Train Epoch: 8 [ 4096/60000 (  7%)]\tLoss: 65.562256\n",
            "Train Epoch: 8 [ 4608/60000 (  8%)]\tLoss: 67.944382\n",
            "Train Epoch: 8 [ 5120/60000 (  9%)]\tLoss: 67.631989\n",
            "Train Epoch: 8 [ 5632/60000 (  9%)]\tLoss: 63.619694\n",
            "Train Epoch: 8 [ 6144/60000 ( 10%)]\tLoss: 66.993530\n",
            "Train Epoch: 8 [ 6656/60000 ( 11%)]\tLoss: 66.085999\n",
            "Train Epoch: 8 [ 7168/60000 ( 12%)]\tLoss: 67.572922\n",
            "Train Epoch: 8 [ 7680/60000 ( 13%)]\tLoss: 64.126144\n",
            "Train Epoch: 8 [ 8192/60000 ( 14%)]\tLoss: 63.386314\n",
            "Train Epoch: 8 [ 8704/60000 ( 14%)]\tLoss: 63.169350\n",
            "Train Epoch: 8 [ 9216/60000 ( 15%)]\tLoss: 65.774757\n",
            "Train Epoch: 8 [ 9728/60000 ( 16%)]\tLoss: 67.178741\n",
            "Train Epoch: 8 [10240/60000 ( 17%)]\tLoss: 64.560181\n",
            "Train Epoch: 8 [10752/60000 ( 18%)]\tLoss: 62.608410\n",
            "Train Epoch: 8 [11264/60000 ( 19%)]\tLoss: 67.309822\n",
            "Train Epoch: 8 [11776/60000 ( 20%)]\tLoss: 64.030136\n",
            "Train Epoch: 8 [12288/60000 ( 20%)]\tLoss: 62.344036\n",
            "Train Epoch: 8 [12800/60000 ( 21%)]\tLoss: 60.335976\n",
            "Train Epoch: 8 [13312/60000 ( 22%)]\tLoss: 64.359238\n",
            "Train Epoch: 8 [13824/60000 ( 23%)]\tLoss: 63.449039\n",
            "Train Epoch: 8 [14336/60000 ( 24%)]\tLoss: 66.157700\n",
            "Train Epoch: 8 [14848/60000 ( 25%)]\tLoss: 64.722786\n",
            "Train Epoch: 8 [15360/60000 ( 26%)]\tLoss: 61.808697\n",
            "Train Epoch: 8 [15872/60000 ( 26%)]\tLoss: 65.706680\n",
            "Train Epoch: 8 [16384/60000 ( 27%)]\tLoss: 62.808319\n",
            "Train Epoch: 8 [16896/60000 ( 28%)]\tLoss: 62.417542\n",
            "Train Epoch: 8 [17408/60000 ( 29%)]\tLoss: 66.085274\n",
            "Train Epoch: 8 [17920/60000 ( 30%)]\tLoss: 64.107681\n",
            "Train Epoch: 8 [18432/60000 ( 31%)]\tLoss: 67.142456\n",
            "Train Epoch: 8 [18944/60000 ( 31%)]\tLoss: 63.554554\n",
            "Train Epoch: 8 [19456/60000 ( 32%)]\tLoss: 60.042793\n",
            "Train Epoch: 8 [19968/60000 ( 33%)]\tLoss: 66.235458\n",
            "Train Epoch: 8 [20480/60000 ( 34%)]\tLoss: 63.408161\n",
            "Train Epoch: 8 [20992/60000 ( 35%)]\tLoss: 66.291992\n",
            "Train Epoch: 8 [21504/60000 ( 36%)]\tLoss: 65.509117\n",
            "Train Epoch: 8 [22016/60000 ( 37%)]\tLoss: 66.548866\n",
            "Train Epoch: 8 [22528/60000 ( 37%)]\tLoss: 68.842873\n",
            "Train Epoch: 8 [23040/60000 ( 38%)]\tLoss: 63.181839\n",
            "Train Epoch: 8 [23552/60000 ( 39%)]\tLoss: 62.741871\n",
            "Train Epoch: 8 [24064/60000 ( 40%)]\tLoss: 63.305943\n",
            "Train Epoch: 8 [24576/60000 ( 41%)]\tLoss: 62.459930\n",
            "Train Epoch: 8 [25088/60000 ( 42%)]\tLoss: 61.416031\n",
            "Train Epoch: 8 [25600/60000 ( 43%)]\tLoss: 67.624283\n",
            "Train Epoch: 8 [26112/60000 ( 43%)]\tLoss: 67.243164\n",
            "Train Epoch: 8 [26624/60000 ( 44%)]\tLoss: 62.900463\n",
            "Train Epoch: 8 [27136/60000 ( 45%)]\tLoss: 60.691315\n",
            "Train Epoch: 8 [27648/60000 ( 46%)]\tLoss: 65.050407\n",
            "Train Epoch: 8 [28160/60000 ( 47%)]\tLoss: 63.918213\n",
            "Train Epoch: 8 [28672/60000 ( 48%)]\tLoss: 64.498901\n",
            "Train Epoch: 8 [29184/60000 ( 49%)]\tLoss: 65.093185\n",
            "Train Epoch: 8 [29696/60000 ( 49%)]\tLoss: 66.595627\n",
            "Train Epoch: 8 [30208/60000 ( 50%)]\tLoss: 64.416656\n",
            "Train Epoch: 8 [30720/60000 ( 51%)]\tLoss: 61.457298\n",
            "Train Epoch: 8 [31232/60000 ( 52%)]\tLoss: 61.105026\n",
            "Train Epoch: 8 [31744/60000 ( 53%)]\tLoss: 64.252647\n",
            "Train Epoch: 8 [32256/60000 ( 54%)]\tLoss: 65.079651\n",
            "Train Epoch: 8 [32768/60000 ( 54%)]\tLoss: 64.570206\n",
            "Train Epoch: 8 [33280/60000 ( 55%)]\tLoss: 64.781349\n",
            "Train Epoch: 8 [33792/60000 ( 56%)]\tLoss: 63.765987\n",
            "Train Epoch: 8 [34304/60000 ( 57%)]\tLoss: 62.281620\n",
            "Train Epoch: 8 [34816/60000 ( 58%)]\tLoss: 62.128181\n",
            "Train Epoch: 8 [35328/60000 ( 59%)]\tLoss: 67.426338\n",
            "Train Epoch: 8 [35840/60000 ( 60%)]\tLoss: 65.370071\n",
            "Train Epoch: 8 [36352/60000 ( 60%)]\tLoss: 65.943314\n",
            "Train Epoch: 8 [36864/60000 ( 61%)]\tLoss: 62.146454\n",
            "Train Epoch: 8 [37376/60000 ( 62%)]\tLoss: 61.303539\n",
            "Train Epoch: 8 [37888/60000 ( 63%)]\tLoss: 63.549118\n",
            "Train Epoch: 8 [38400/60000 ( 64%)]\tLoss: 64.569244\n",
            "Train Epoch: 8 [38912/60000 ( 65%)]\tLoss: 64.463585\n",
            "Train Epoch: 8 [39424/60000 ( 66%)]\tLoss: 61.517830\n",
            "Train Epoch: 8 [39936/60000 ( 66%)]\tLoss: 62.716568\n",
            "Train Epoch: 8 [40448/60000 ( 67%)]\tLoss: 65.451500\n",
            "Train Epoch: 8 [40960/60000 ( 68%)]\tLoss: 63.433086\n",
            "Train Epoch: 8 [41472/60000 ( 69%)]\tLoss: 67.343842\n",
            "Train Epoch: 8 [41984/60000 ( 70%)]\tLoss: 66.626427\n",
            "Train Epoch: 8 [42496/60000 ( 71%)]\tLoss: 66.833633\n",
            "Train Epoch: 8 [43008/60000 ( 71%)]\tLoss: 67.101746\n",
            "Train Epoch: 8 [43520/60000 ( 72%)]\tLoss: 64.005005\n",
            "Train Epoch: 8 [44032/60000 ( 73%)]\tLoss: 62.665245\n",
            "Train Epoch: 8 [44544/60000 ( 74%)]\tLoss: 64.431763\n",
            "Train Epoch: 8 [45056/60000 ( 75%)]\tLoss: 65.114754\n",
            "Train Epoch: 8 [45568/60000 ( 76%)]\tLoss: 64.034424\n",
            "Train Epoch: 8 [46080/60000 ( 77%)]\tLoss: 66.370941\n",
            "Train Epoch: 8 [46592/60000 ( 77%)]\tLoss: 63.684837\n",
            "Train Epoch: 8 [47104/60000 ( 78%)]\tLoss: 63.787277\n",
            "Train Epoch: 8 [47616/60000 ( 79%)]\tLoss: 64.623795\n",
            "Train Epoch: 8 [48128/60000 ( 80%)]\tLoss: 63.264420\n",
            "Train Epoch: 8 [48640/60000 ( 81%)]\tLoss: 65.779510\n",
            "Train Epoch: 8 [49152/60000 ( 82%)]\tLoss: 66.561600\n",
            "Train Epoch: 8 [49664/60000 ( 83%)]\tLoss: 63.628761\n",
            "Train Epoch: 8 [50176/60000 ( 83%)]\tLoss: 61.774929\n",
            "Train Epoch: 8 [50688/60000 ( 84%)]\tLoss: 63.939148\n",
            "Train Epoch: 8 [51200/60000 ( 85%)]\tLoss: 60.100979\n",
            "Train Epoch: 8 [51712/60000 ( 86%)]\tLoss: 63.779118\n",
            "Train Epoch: 8 [52224/60000 ( 87%)]\tLoss: 61.366859\n",
            "Train Epoch: 8 [52736/60000 ( 88%)]\tLoss: 62.075539\n",
            "Train Epoch: 8 [53248/60000 ( 89%)]\tLoss: 63.651104\n",
            "Train Epoch: 8 [53760/60000 ( 89%)]\tLoss: 59.446678\n",
            "Train Epoch: 8 [54272/60000 ( 90%)]\tLoss: 66.112335\n",
            "Train Epoch: 8 [54784/60000 ( 91%)]\tLoss: 62.285599\n",
            "Train Epoch: 8 [55296/60000 ( 92%)]\tLoss: 64.156181\n",
            "Train Epoch: 8 [55808/60000 ( 93%)]\tLoss: 60.172840\n",
            "Train Epoch: 8 [56320/60000 ( 94%)]\tLoss: 61.276329\n",
            "Train Epoch: 8 [56832/60000 ( 94%)]\tLoss: 60.584183\n",
            "Train Epoch: 8 [57344/60000 ( 95%)]\tLoss: 61.029022\n",
            "Train Epoch: 8 [57856/60000 ( 96%)]\tLoss: 64.822533\n",
            "Train Epoch: 8 [58368/60000 ( 97%)]\tLoss: 63.007805\n",
            "Train Epoch: 8 [58880/60000 ( 98%)]\tLoss: 64.351059\n",
            "Train Epoch: 8 [59392/60000 ( 99%)]\tLoss: 65.195549\n",
            "Train Epoch: 8 [22464/60000 (100%)]\tLoss: 67.636831\n",
            "====> Epoch: 8 Average loss: 64.6216\n",
            "====> Test set loss: 65.6953\n",
            "Train Epoch: 9 [    0/60000 (  0%)]\tLoss: 62.686470\n",
            "Train Epoch: 9 [  512/60000 (  1%)]\tLoss: 64.602310\n",
            "Train Epoch: 9 [ 1024/60000 (  2%)]\tLoss: 63.841850\n",
            "Train Epoch: 9 [ 1536/60000 (  3%)]\tLoss: 62.714485\n",
            "Train Epoch: 9 [ 2048/60000 (  3%)]\tLoss: 67.233093\n",
            "Train Epoch: 9 [ 2560/60000 (  4%)]\tLoss: 63.193878\n",
            "Train Epoch: 9 [ 3072/60000 (  5%)]\tLoss: 63.821606\n",
            "Train Epoch: 9 [ 3584/60000 (  6%)]\tLoss: 62.837536\n",
            "Train Epoch: 9 [ 4096/60000 (  7%)]\tLoss: 62.242519\n",
            "Train Epoch: 9 [ 4608/60000 (  8%)]\tLoss: 65.157974\n",
            "Train Epoch: 9 [ 5120/60000 (  9%)]\tLoss: 63.485569\n",
            "Train Epoch: 9 [ 5632/60000 (  9%)]\tLoss: 62.557678\n",
            "Train Epoch: 9 [ 6144/60000 ( 10%)]\tLoss: 58.962387\n",
            "Train Epoch: 9 [ 6656/60000 ( 11%)]\tLoss: 65.258659\n",
            "Train Epoch: 9 [ 7168/60000 ( 12%)]\tLoss: 65.702522\n",
            "Train Epoch: 9 [ 7680/60000 ( 13%)]\tLoss: 64.823532\n",
            "Train Epoch: 9 [ 8192/60000 ( 14%)]\tLoss: 61.676689\n",
            "Train Epoch: 9 [ 8704/60000 ( 14%)]\tLoss: 62.751530\n",
            "Train Epoch: 9 [ 9216/60000 ( 15%)]\tLoss: 58.382961\n",
            "Train Epoch: 9 [ 9728/60000 ( 16%)]\tLoss: 63.806416\n",
            "Train Epoch: 9 [10240/60000 ( 17%)]\tLoss: 60.459869\n",
            "Train Epoch: 9 [10752/60000 ( 18%)]\tLoss: 62.636070\n",
            "Train Epoch: 9 [11264/60000 ( 19%)]\tLoss: 60.126137\n",
            "Train Epoch: 9 [11776/60000 ( 20%)]\tLoss: 62.973511\n",
            "Train Epoch: 9 [12288/60000 ( 20%)]\tLoss: 63.920227\n",
            "Train Epoch: 9 [12800/60000 ( 21%)]\tLoss: 66.203400\n",
            "Train Epoch: 9 [13312/60000 ( 22%)]\tLoss: 66.975555\n",
            "Train Epoch: 9 [13824/60000 ( 23%)]\tLoss: 62.186714\n",
            "Train Epoch: 9 [14336/60000 ( 24%)]\tLoss: 63.189072\n",
            "Train Epoch: 9 [14848/60000 ( 25%)]\tLoss: 61.408562\n",
            "Train Epoch: 9 [15360/60000 ( 26%)]\tLoss: 63.350700\n",
            "Train Epoch: 9 [15872/60000 ( 26%)]\tLoss: 65.777641\n",
            "Train Epoch: 9 [16384/60000 ( 27%)]\tLoss: 66.536438\n",
            "Train Epoch: 9 [16896/60000 ( 28%)]\tLoss: 65.796509\n",
            "Train Epoch: 9 [17408/60000 ( 29%)]\tLoss: 65.585037\n",
            "Train Epoch: 9 [17920/60000 ( 30%)]\tLoss: 64.022865\n",
            "Train Epoch: 9 [18432/60000 ( 31%)]\tLoss: 64.250259\n",
            "Train Epoch: 9 [18944/60000 ( 31%)]\tLoss: 63.526329\n",
            "Train Epoch: 9 [19456/60000 ( 32%)]\tLoss: 65.955109\n",
            "Train Epoch: 9 [19968/60000 ( 33%)]\tLoss: 62.717545\n",
            "Train Epoch: 9 [20480/60000 ( 34%)]\tLoss: 59.716011\n",
            "Train Epoch: 9 [20992/60000 ( 35%)]\tLoss: 62.259270\n",
            "Train Epoch: 9 [21504/60000 ( 36%)]\tLoss: 65.459679\n",
            "Train Epoch: 9 [22016/60000 ( 37%)]\tLoss: 63.226532\n",
            "Train Epoch: 9 [22528/60000 ( 37%)]\tLoss: 63.883423\n",
            "Train Epoch: 9 [23040/60000 ( 38%)]\tLoss: 59.077900\n",
            "Train Epoch: 9 [23552/60000 ( 39%)]\tLoss: 65.149429\n",
            "Train Epoch: 9 [24064/60000 ( 40%)]\tLoss: 63.705948\n",
            "Train Epoch: 9 [24576/60000 ( 41%)]\tLoss: 62.000702\n",
            "Train Epoch: 9 [25088/60000 ( 42%)]\tLoss: 63.767929\n",
            "Train Epoch: 9 [25600/60000 ( 43%)]\tLoss: 62.379887\n",
            "Train Epoch: 9 [26112/60000 ( 43%)]\tLoss: 60.869980\n",
            "Train Epoch: 9 [26624/60000 ( 44%)]\tLoss: 62.163872\n",
            "Train Epoch: 9 [27136/60000 ( 45%)]\tLoss: 65.864861\n",
            "Train Epoch: 9 [27648/60000 ( 46%)]\tLoss: 62.309891\n",
            "Train Epoch: 9 [28160/60000 ( 47%)]\tLoss: 65.328651\n",
            "Train Epoch: 9 [28672/60000 ( 48%)]\tLoss: 62.402641\n",
            "Train Epoch: 9 [29184/60000 ( 49%)]\tLoss: 64.625336\n",
            "Train Epoch: 9 [29696/60000 ( 49%)]\tLoss: 64.254509\n",
            "Train Epoch: 9 [30208/60000 ( 50%)]\tLoss: 62.338058\n",
            "Train Epoch: 9 [30720/60000 ( 51%)]\tLoss: 59.237286\n",
            "Train Epoch: 9 [31232/60000 ( 52%)]\tLoss: 63.724998\n",
            "Train Epoch: 9 [31744/60000 ( 53%)]\tLoss: 59.513882\n",
            "Train Epoch: 9 [32256/60000 ( 54%)]\tLoss: 63.083965\n",
            "Train Epoch: 9 [32768/60000 ( 54%)]\tLoss: 68.495537\n",
            "Train Epoch: 9 [33280/60000 ( 55%)]\tLoss: 64.708969\n",
            "Train Epoch: 9 [33792/60000 ( 56%)]\tLoss: 62.025322\n",
            "Train Epoch: 9 [34304/60000 ( 57%)]\tLoss: 64.412544\n",
            "Train Epoch: 9 [34816/60000 ( 58%)]\tLoss: 63.422550\n",
            "Train Epoch: 9 [35328/60000 ( 59%)]\tLoss: 65.134941\n",
            "Train Epoch: 9 [35840/60000 ( 60%)]\tLoss: 61.329231\n",
            "Train Epoch: 9 [36352/60000 ( 60%)]\tLoss: 64.608643\n",
            "Train Epoch: 9 [36864/60000 ( 61%)]\tLoss: 64.835785\n",
            "Train Epoch: 9 [37376/60000 ( 62%)]\tLoss: 67.389259\n",
            "Train Epoch: 9 [37888/60000 ( 63%)]\tLoss: 60.874901\n",
            "Train Epoch: 9 [38400/60000 ( 64%)]\tLoss: 63.604332\n",
            "Train Epoch: 9 [38912/60000 ( 65%)]\tLoss: 65.549622\n",
            "Train Epoch: 9 [39424/60000 ( 66%)]\tLoss: 64.341255\n",
            "Train Epoch: 9 [39936/60000 ( 66%)]\tLoss: 65.031166\n",
            "Train Epoch: 9 [40448/60000 ( 67%)]\tLoss: 58.464073\n",
            "Train Epoch: 9 [40960/60000 ( 68%)]\tLoss: 64.122818\n",
            "Train Epoch: 9 [41472/60000 ( 69%)]\tLoss: 61.963223\n",
            "Train Epoch: 9 [41984/60000 ( 70%)]\tLoss: 65.706329\n",
            "Train Epoch: 9 [42496/60000 ( 71%)]\tLoss: 60.474014\n",
            "Train Epoch: 9 [43008/60000 ( 71%)]\tLoss: 61.354607\n",
            "Train Epoch: 9 [43520/60000 ( 72%)]\tLoss: 64.444061\n",
            "Train Epoch: 9 [44032/60000 ( 73%)]\tLoss: 64.530846\n",
            "Train Epoch: 9 [44544/60000 ( 74%)]\tLoss: 65.172882\n",
            "Train Epoch: 9 [45056/60000 ( 75%)]\tLoss: 61.677132\n",
            "Train Epoch: 9 [45568/60000 ( 76%)]\tLoss: 63.851719\n",
            "Train Epoch: 9 [46080/60000 ( 77%)]\tLoss: 62.053604\n",
            "Train Epoch: 9 [46592/60000 ( 77%)]\tLoss: 62.899765\n",
            "Train Epoch: 9 [47104/60000 ( 78%)]\tLoss: 62.656796\n",
            "Train Epoch: 9 [47616/60000 ( 79%)]\tLoss: 65.249947\n",
            "Train Epoch: 9 [48128/60000 ( 80%)]\tLoss: 64.673424\n",
            "Train Epoch: 9 [48640/60000 ( 81%)]\tLoss: 61.829372\n",
            "Train Epoch: 9 [49152/60000 ( 82%)]\tLoss: 63.240616\n",
            "Train Epoch: 9 [49664/60000 ( 83%)]\tLoss: 61.854088\n",
            "Train Epoch: 9 [50176/60000 ( 83%)]\tLoss: 63.894497\n",
            "Train Epoch: 9 [50688/60000 ( 84%)]\tLoss: 62.709351\n",
            "Train Epoch: 9 [51200/60000 ( 85%)]\tLoss: 63.265690\n",
            "Train Epoch: 9 [51712/60000 ( 86%)]\tLoss: 62.020889\n",
            "Train Epoch: 9 [52224/60000 ( 87%)]\tLoss: 60.079189\n",
            "Train Epoch: 9 [52736/60000 ( 88%)]\tLoss: 63.782093\n",
            "Train Epoch: 9 [53248/60000 ( 89%)]\tLoss: 64.068474\n",
            "Train Epoch: 9 [53760/60000 ( 89%)]\tLoss: 63.293724\n",
            "Train Epoch: 9 [54272/60000 ( 90%)]\tLoss: 60.560417\n",
            "Train Epoch: 9 [54784/60000 ( 91%)]\tLoss: 64.330307\n",
            "Train Epoch: 9 [55296/60000 ( 92%)]\tLoss: 63.911674\n",
            "Train Epoch: 9 [55808/60000 ( 93%)]\tLoss: 64.248871\n",
            "Train Epoch: 9 [56320/60000 ( 94%)]\tLoss: 60.159843\n",
            "Train Epoch: 9 [56832/60000 ( 94%)]\tLoss: 65.893631\n",
            "Train Epoch: 9 [57344/60000 ( 95%)]\tLoss: 64.622322\n",
            "Train Epoch: 9 [57856/60000 ( 96%)]\tLoss: 62.816879\n",
            "Train Epoch: 9 [58368/60000 ( 97%)]\tLoss: 62.577911\n",
            "Train Epoch: 9 [58880/60000 ( 98%)]\tLoss: 65.722191\n",
            "Train Epoch: 9 [59392/60000 ( 99%)]\tLoss: 61.262211\n",
            "Train Epoch: 9 [22464/60000 (100%)]\tLoss: 62.663137\n",
            "====> Epoch: 9 Average loss: 63.4722\n",
            "====> Test set loss: 64.7847\n",
            "Train Epoch: 10 [    0/60000 (  0%)]\tLoss: 63.768658\n",
            "Train Epoch: 10 [  512/60000 (  1%)]\tLoss: 63.966740\n",
            "Train Epoch: 10 [ 1024/60000 (  2%)]\tLoss: 63.099129\n",
            "Train Epoch: 10 [ 1536/60000 (  3%)]\tLoss: 62.983547\n",
            "Train Epoch: 10 [ 2048/60000 (  3%)]\tLoss: 64.397057\n",
            "Train Epoch: 10 [ 2560/60000 (  4%)]\tLoss: 61.743694\n",
            "Train Epoch: 10 [ 3072/60000 (  5%)]\tLoss: 68.545372\n",
            "Train Epoch: 10 [ 3584/60000 (  6%)]\tLoss: 67.346832\n",
            "Train Epoch: 10 [ 4096/60000 (  7%)]\tLoss: 63.040192\n",
            "Train Epoch: 10 [ 4608/60000 (  8%)]\tLoss: 59.840099\n",
            "Train Epoch: 10 [ 5120/60000 (  9%)]\tLoss: 61.930470\n",
            "Train Epoch: 10 [ 5632/60000 (  9%)]\tLoss: 65.264297\n",
            "Train Epoch: 10 [ 6144/60000 ( 10%)]\tLoss: 62.812164\n",
            "Train Epoch: 10 [ 6656/60000 ( 11%)]\tLoss: 58.926331\n",
            "Train Epoch: 10 [ 7168/60000 ( 12%)]\tLoss: 61.514671\n",
            "Train Epoch: 10 [ 7680/60000 ( 13%)]\tLoss: 66.672760\n",
            "Train Epoch: 10 [ 8192/60000 ( 14%)]\tLoss: 64.556953\n",
            "Train Epoch: 10 [ 8704/60000 ( 14%)]\tLoss: 60.963032\n",
            "Train Epoch: 10 [ 9216/60000 ( 15%)]\tLoss: 62.423561\n",
            "Train Epoch: 10 [ 9728/60000 ( 16%)]\tLoss: 62.190987\n",
            "Train Epoch: 10 [10240/60000 ( 17%)]\tLoss: 60.713970\n",
            "Train Epoch: 10 [10752/60000 ( 18%)]\tLoss: 61.610023\n",
            "Train Epoch: 10 [11264/60000 ( 19%)]\tLoss: 59.757088\n",
            "Train Epoch: 10 [11776/60000 ( 20%)]\tLoss: 61.389198\n",
            "Train Epoch: 10 [12288/60000 ( 20%)]\tLoss: 59.412041\n",
            "Train Epoch: 10 [12800/60000 ( 21%)]\tLoss: 62.681728\n",
            "Train Epoch: 10 [13312/60000 ( 22%)]\tLoss: 62.691971\n",
            "Train Epoch: 10 [13824/60000 ( 23%)]\tLoss: 66.949051\n",
            "Train Epoch: 10 [14336/60000 ( 24%)]\tLoss: 62.793732\n",
            "Train Epoch: 10 [14848/60000 ( 25%)]\tLoss: 60.066551\n",
            "Train Epoch: 10 [15360/60000 ( 26%)]\tLoss: 59.131187\n",
            "Train Epoch: 10 [15872/60000 ( 26%)]\tLoss: 63.913460\n",
            "Train Epoch: 10 [16384/60000 ( 27%)]\tLoss: 61.562408\n",
            "Train Epoch: 10 [16896/60000 ( 28%)]\tLoss: 60.120438\n",
            "Train Epoch: 10 [17408/60000 ( 29%)]\tLoss: 63.362839\n",
            "Train Epoch: 10 [17920/60000 ( 30%)]\tLoss: 62.603111\n",
            "Train Epoch: 10 [18432/60000 ( 31%)]\tLoss: 60.867119\n",
            "Train Epoch: 10 [18944/60000 ( 31%)]\tLoss: 60.037743\n",
            "Train Epoch: 10 [19456/60000 ( 32%)]\tLoss: 64.062653\n",
            "Train Epoch: 10 [19968/60000 ( 33%)]\tLoss: 65.416321\n",
            "Train Epoch: 10 [20480/60000 ( 34%)]\tLoss: 62.999863\n",
            "Train Epoch: 10 [20992/60000 ( 35%)]\tLoss: 59.088413\n",
            "Train Epoch: 10 [21504/60000 ( 36%)]\tLoss: 64.004471\n",
            "Train Epoch: 10 [22016/60000 ( 37%)]\tLoss: 64.106888\n",
            "Train Epoch: 10 [22528/60000 ( 37%)]\tLoss: 60.343182\n",
            "Train Epoch: 10 [23040/60000 ( 38%)]\tLoss: 60.156883\n",
            "Train Epoch: 10 [23552/60000 ( 39%)]\tLoss: 66.110321\n",
            "Train Epoch: 10 [24064/60000 ( 40%)]\tLoss: 61.715519\n",
            "Train Epoch: 10 [24576/60000 ( 41%)]\tLoss: 61.001270\n",
            "Train Epoch: 10 [25088/60000 ( 42%)]\tLoss: 63.948456\n",
            "Train Epoch: 10 [25600/60000 ( 43%)]\tLoss: 63.145874\n",
            "Train Epoch: 10 [26112/60000 ( 43%)]\tLoss: 65.027367\n",
            "Train Epoch: 10 [26624/60000 ( 44%)]\tLoss: 60.990814\n",
            "Train Epoch: 10 [27136/60000 ( 45%)]\tLoss: 64.637863\n",
            "Train Epoch: 10 [27648/60000 ( 46%)]\tLoss: 61.674812\n",
            "Train Epoch: 10 [28160/60000 ( 47%)]\tLoss: 57.536743\n",
            "Train Epoch: 10 [28672/60000 ( 48%)]\tLoss: 61.383339\n",
            "Train Epoch: 10 [29184/60000 ( 49%)]\tLoss: 63.169163\n",
            "Train Epoch: 10 [29696/60000 ( 49%)]\tLoss: 61.453876\n",
            "Train Epoch: 10 [30208/60000 ( 50%)]\tLoss: 60.289371\n",
            "Train Epoch: 10 [30720/60000 ( 51%)]\tLoss: 64.432983\n",
            "Train Epoch: 10 [31232/60000 ( 52%)]\tLoss: 64.024796\n",
            "Train Epoch: 10 [31744/60000 ( 53%)]\tLoss: 67.023811\n",
            "Train Epoch: 10 [32256/60000 ( 54%)]\tLoss: 63.294151\n",
            "Train Epoch: 10 [32768/60000 ( 54%)]\tLoss: 65.492584\n",
            "Train Epoch: 10 [33280/60000 ( 55%)]\tLoss: 62.868961\n",
            "Train Epoch: 10 [33792/60000 ( 56%)]\tLoss: 63.034309\n",
            "Train Epoch: 10 [34304/60000 ( 57%)]\tLoss: 62.068775\n",
            "Train Epoch: 10 [34816/60000 ( 58%)]\tLoss: 64.551468\n",
            "Train Epoch: 10 [35328/60000 ( 59%)]\tLoss: 60.411358\n",
            "Train Epoch: 10 [35840/60000 ( 60%)]\tLoss: 63.987465\n",
            "Train Epoch: 10 [36352/60000 ( 60%)]\tLoss: 63.385342\n",
            "Train Epoch: 10 [36864/60000 ( 61%)]\tLoss: 65.820648\n",
            "Train Epoch: 10 [37376/60000 ( 62%)]\tLoss: 63.880890\n",
            "Train Epoch: 10 [37888/60000 ( 63%)]\tLoss: 61.461861\n",
            "Train Epoch: 10 [38400/60000 ( 64%)]\tLoss: 62.154701\n",
            "Train Epoch: 10 [38912/60000 ( 65%)]\tLoss: 62.037270\n",
            "Train Epoch: 10 [39424/60000 ( 66%)]\tLoss: 64.490005\n",
            "Train Epoch: 10 [39936/60000 ( 66%)]\tLoss: 64.913582\n",
            "Train Epoch: 10 [40448/60000 ( 67%)]\tLoss: 60.502892\n",
            "Train Epoch: 10 [40960/60000 ( 68%)]\tLoss: 64.645493\n",
            "Train Epoch: 10 [41472/60000 ( 69%)]\tLoss: 61.998943\n",
            "Train Epoch: 10 [41984/60000 ( 70%)]\tLoss: 63.354515\n",
            "Train Epoch: 10 [42496/60000 ( 71%)]\tLoss: 62.541683\n",
            "Train Epoch: 10 [43008/60000 ( 71%)]\tLoss: 58.839142\n",
            "Train Epoch: 10 [43520/60000 ( 72%)]\tLoss: 62.468018\n",
            "Train Epoch: 10 [44032/60000 ( 73%)]\tLoss: 61.728504\n",
            "Train Epoch: 10 [44544/60000 ( 74%)]\tLoss: 63.903549\n",
            "Train Epoch: 10 [45056/60000 ( 75%)]\tLoss: 60.961449\n",
            "Train Epoch: 10 [45568/60000 ( 76%)]\tLoss: 60.088333\n",
            "Train Epoch: 10 [46080/60000 ( 77%)]\tLoss: 58.465736\n",
            "Train Epoch: 10 [46592/60000 ( 77%)]\tLoss: 62.805351\n",
            "Train Epoch: 10 [47104/60000 ( 78%)]\tLoss: 63.475700\n",
            "Train Epoch: 10 [47616/60000 ( 79%)]\tLoss: 62.891659\n",
            "Train Epoch: 10 [48128/60000 ( 80%)]\tLoss: 61.587349\n",
            "Train Epoch: 10 [48640/60000 ( 81%)]\tLoss: 60.050819\n",
            "Train Epoch: 10 [49152/60000 ( 82%)]\tLoss: 59.753017\n",
            "Train Epoch: 10 [49664/60000 ( 83%)]\tLoss: 59.307198\n",
            "Train Epoch: 10 [50176/60000 ( 83%)]\tLoss: 59.642715\n",
            "Train Epoch: 10 [50688/60000 ( 84%)]\tLoss: 61.664001\n",
            "Train Epoch: 10 [51200/60000 ( 85%)]\tLoss: 61.560760\n",
            "Train Epoch: 10 [51712/60000 ( 86%)]\tLoss: 63.869995\n",
            "Train Epoch: 10 [52224/60000 ( 87%)]\tLoss: 58.759666\n",
            "Train Epoch: 10 [52736/60000 ( 88%)]\tLoss: 62.469391\n",
            "Train Epoch: 10 [53248/60000 ( 89%)]\tLoss: 59.767036\n",
            "Train Epoch: 10 [53760/60000 ( 89%)]\tLoss: 63.961212\n",
            "Train Epoch: 10 [54272/60000 ( 90%)]\tLoss: 58.623329\n",
            "Train Epoch: 10 [54784/60000 ( 91%)]\tLoss: 62.924057\n",
            "Train Epoch: 10 [55296/60000 ( 92%)]\tLoss: 64.346237\n",
            "Train Epoch: 10 [55808/60000 ( 93%)]\tLoss: 62.625401\n",
            "Train Epoch: 10 [56320/60000 ( 94%)]\tLoss: 59.771675\n",
            "Train Epoch: 10 [56832/60000 ( 94%)]\tLoss: 64.546074\n",
            "Train Epoch: 10 [57344/60000 ( 95%)]\tLoss: 65.546814\n",
            "Train Epoch: 10 [57856/60000 ( 96%)]\tLoss: 63.501499\n",
            "Train Epoch: 10 [58368/60000 ( 97%)]\tLoss: 60.132103\n",
            "Train Epoch: 10 [58880/60000 ( 98%)]\tLoss: 59.126183\n",
            "Train Epoch: 10 [59392/60000 ( 99%)]\tLoss: 63.442818\n",
            "Train Epoch: 10 [22464/60000 (100%)]\tLoss: 65.705973\n",
            "====> Epoch: 10 Average loss: 62.3159\n",
            "====> Test set loss: 62.9579\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Dirichlet Variational Auto-Encoder (Dir-VAE) implementation for MNIST\n",
        "Based on \"Autoencodeing Variational Inference for Topic Model\" (ICLR2017)\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for Dir-VAE training\"\"\"\n",
        "\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 10\n",
        "    learning_rate: float = 1e-3\n",
        "    no_cuda: bool = False\n",
        "    seed: int = 10\n",
        "    log_interval: int = 2\n",
        "    category: int = 10  # Number of latent categories (K)\n",
        "    alpha: float = 0.3  # Dirichlet hyperparameter\n",
        "    data_dir: str = \"./data\"\n",
        "    output_dir: str = \"./image\"\n",
        "\n",
        "    # Network architecture parameters\n",
        "    encoder_channels: int = 64\n",
        "    decoder_channels: int = 64\n",
        "    input_channels: int = 1\n",
        "    latent_dim: int = 1024\n",
        "    hidden_dim: int = 512\n",
        "\n",
        "\n",
        "def create_argument_parser() -> argparse.ArgumentParser:\n",
        "    \"\"\"Create and configure argument parser\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Dir-VAE MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=256,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 256)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 10)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning-rate\",\n",
        "        type=float,\n",
        "        default=1e-3,\n",
        "        help=\"learning rate (default: 1e-3)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no-cuda\", action=\"store_true\", default=False, help=\"disable CUDA training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=10, metavar=\"S\", help=\"random seed (default: 10)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=2,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--category\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"K\",\n",
        "        help=\"the number of categories in the dataset\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--alpha\",\n",
        "        type=float,\n",
        "        default=0.3,\n",
        "        help=\"Dirichlet hyperparameter alpha (default: 0.3)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data-dir\",\n",
        "        type=str,\n",
        "        default=\"./data\",\n",
        "        help=\"directory for dataset (default: ./data)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output-dir\",\n",
        "        type=str,\n",
        "        default=\"./image\",\n",
        "        help=\"directory for output images (default: ./image)\",\n",
        "    )\n",
        "    return parser\n",
        "\n",
        "\n",
        "def setup_device_and_seed(config: Config) -> torch.device:\n",
        "    \"\"\"Setup device and random seeds\"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config.seed)\n",
        "\n",
        "    # Determine device\n",
        "    use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def create_data_loaders(\n",
        "    config: Config,\n",
        ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
        "    \"\"\"Create train and test data loaders\"\"\"\n",
        "    # Create data directory if it doesn't exist\n",
        "    os.makedirs(config.data_dir, exist_ok=True)\n",
        "\n",
        "    # Data loader kwargs\n",
        "    kwargs = (\n",
        "        {\"num_workers\": 1, \"pin_memory\": True}\n",
        "        if not config.no_cuda and torch.cuda.is_available()\n",
        "        else {}\n",
        "    )\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(\n",
        "            config.data_dir, train=True, download=True, transform=transforms.ToTensor()\n",
        "        ),\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(config.data_dir, train=False, transform=transforms.ToTensor()),\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def compute_dirichlet_prior(\n",
        "    K: int, alpha: float, device: torch.device\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute Dirichlet prior parameters using Laplace approximation.\n",
        "\n",
        "    Args:\n",
        "        K: Number of categories\n",
        "        alpha: Dirichlet hyperparameter\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (mean, variance) tensors for the approximated normal distribution\n",
        "    \"\"\"\n",
        "    # Laplace approximation to convert Dirichlet to multivariate normal\n",
        "    a = torch.full((1, K), alpha, dtype=torch.float, device=device)\n",
        "    mean = a.log().t() - a.log().mean(1, keepdim=True)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K**2) * a.reciprocal().sum(\n",
        "        1, keepdim=True\n",
        "    )\n",
        "    return mean.t(), var.t()\n",
        "\n",
        "\n",
        "class DirVAEEncoder(nn.Module):\n",
        "    \"\"\"Encoder part of Dir-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super(DirVAEEncoder, self).__init__()\n",
        "        ndf = config.encoder_channels\n",
        "        nc = config.input_channels\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # input: (nc) x 28 x 28\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf) x 14 x 14\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf*2) x 7 x 7\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf*4) x 4 x 4\n",
        "            nn.Conv2d(ndf * 4, config.latent_dim, 4, 1, 0, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(config.latent_dim, config.hidden_dim)\n",
        "        self.fc_mu = nn.Linear(config.hidden_dim, config.category)\n",
        "        self.fc_logvar = nn.Linear(config.hidden_dim, config.category)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        conv_out = self.conv_layers(x)\n",
        "        h1 = self.fc1(conv_out.view(conv_out.size(0), -1))\n",
        "        mu = self.fc_mu(h1)\n",
        "        logvar = self.fc_logvar(h1)\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "class DirVAEDecoder(nn.Module):\n",
        "    \"\"\"Decoder part of Dir-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super(DirVAEDecoder, self).__init__()\n",
        "        ngf = config.decoder_channels\n",
        "        nc = config.input_channels\n",
        "\n",
        "        self.fc_decode = nn.Linear(config.category, config.hidden_dim)\n",
        "        self.fc_deconv = nn.Linear(config.hidden_dim, config.latent_dim)\n",
        "\n",
        "        self.deconv_layers = nn.Sequential(\n",
        "            # input: latent_dim x 1 x 1\n",
        "            nn.ConvTranspose2d(config.latent_dim, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "            # output: (nc) x 32 x 32\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
        "        # Apply softmax to satisfy simplex constraint (Dirichlet distribution)\n",
        "        dir_z = F.softmax(gauss_z, dim=1)\n",
        "\n",
        "        h3 = self.relu(self.fc_decode(dir_z))\n",
        "        deconv_input = self.fc_deconv(h3)\n",
        "        deconv_input = deconv_input.view(-1, deconv_input.size(1), 1, 1)\n",
        "\n",
        "        return self.deconv_layers(deconv_input)\n",
        "\n",
        "\n",
        "class DirVAE(nn.Module):\n",
        "    \"\"\"Dirichlet Variational Auto-Encoder\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config, device: torch.device):\n",
        "        super(DirVAE, self).__init__()\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = DirVAEEncoder(config)\n",
        "        self.decoder = DirVAEDecoder(config)\n",
        "\n",
        "        # Setup Dirichlet prior\n",
        "        self._setup_dirichlet_prior()\n",
        "\n",
        "    def _setup_dirichlet_prior(self):\n",
        "        \"\"\"Setup Dirichlet prior parameters\"\"\"\n",
        "        prior_mean, prior_var = compute_dirichlet_prior(\n",
        "            self.config.category, self.config.alpha, self.device\n",
        "        )\n",
        "        self.register_buffer(\"prior_mean\", prior_mean)\n",
        "        self.register_buffer(\"prior_var\", prior_var)\n",
        "        self.register_buffer(\"prior_logvar\", prior_var.log())\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Encode input to latent parameters\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Decode latent variables to reconstruction\"\"\"\n",
        "        return self.decoder(gauss_z)\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Reparameterization trick for backpropagation through stochastic nodes\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # gauss_z follows multivariate normal distribution\n",
        "        # Applying softmax gives us Dirichlet-distributed variables\n",
        "        dir_z = F.softmax(gauss_z, dim=1)\n",
        "        recon_x = self.decode(gauss_z)\n",
        "\n",
        "        return recon_x, mu, logvar, gauss_z, dir_z\n",
        "\n",
        "    def loss_function(\n",
        "        self,\n",
        "        recon_x: torch.Tensor,\n",
        "        x: torch.Tensor,\n",
        "        mu: torch.Tensor,\n",
        "        logvar: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the loss function: reconstruction loss + KL divergence\n",
        "\n",
        "        Args:\n",
        "            recon_x: Reconstructed images\n",
        "            x: Original images\n",
        "            mu: Mean of latent distribution\n",
        "            logvar: Log variance of latent distribution\n",
        "\n",
        "        Returns:\n",
        "            Total loss (sum over batch)\n",
        "        \"\"\"\n",
        "        # Reconstruction loss (Binary Cross Entropy)\n",
        "        BCE = F.binary_cross_entropy(\n",
        "            recon_x.view(-1, 784), x.view(-1, 784), reduction=\"sum\"\n",
        "        )\n",
        "\n",
        "        # KL divergence between Dirichlet prior and variational posterior\n",
        "        # Based on the original paper: \"Autoencodeing variational inference for topic model\"\n",
        "        prior_mean = self.prior_mean.expand_as(mu)\n",
        "        prior_var = self.prior_var.expand_as(logvar)\n",
        "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
        "\n",
        "        var_division = logvar.exp() / prior_var  # Σ_0 / Σ_1\n",
        "        diff = mu - prior_mean  # μ_1 - μ_0\n",
        "        diff_term = diff * diff / prior_var  # (μ_1 - μ_0)² / Σ_1\n",
        "        logvar_division = prior_logvar - logvar  # log|Σ_1| - log|Σ_0|\n",
        "\n",
        "        # KL divergence\n",
        "        KLD = 0.5 * (\n",
        "            var_division + diff_term + logvar_division - self.config.category\n",
        "        ).sum(dim=1)\n",
        "\n",
        "        return BCE + KLD.sum()\n",
        "\n",
        "\n",
        "class DirVAETrainer:\n",
        "    \"\"\"Trainer class for Dir-VAE\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: DirVAE,\n",
        "        config: Config,\n",
        "        device: torch.device,\n",
        "        train_loader: torch.utils.data.DataLoader,\n",
        "        test_loader: torch.utils.data.DataLoader,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "    def train_epoch(self, epoch: int) -> float:\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
        "            data = data.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            recon_batch, mu, logvar, gauss_z, dir_z = self.model(data)\n",
        "            loss = self.model.loss_function(recon_batch, data, mu, logvar)\n",
        "\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if batch_idx % self.config.log_interval == 0:\n",
        "                print(\n",
        "                    f\"Train Epoch: {epoch} [{batch_idx * len(data):5d}/\"\n",
        "                    f\"{len(self.train_loader.dataset)} \"\n",
        "                    f\"({100. * batch_idx / len(self.train_loader):3.0f}%)]\"\n",
        "                    f\"\\tLoss: {loss.item() / len(data):.6f}\"\n",
        "                )\n",
        "\n",
        "        avg_loss = train_loss / len(self.train_loader.dataset)\n",
        "        print(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def test_epoch(self, epoch: int) -> float:\n",
        "        \"\"\"Test for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (data, _) in enumerate(self.test_loader):\n",
        "                data = data.to(self.device)\n",
        "                recon_batch, mu, logvar, gauss_z, dir_z = self.model(data)\n",
        "                loss = self.model.loss_function(recon_batch, data, mu, logvar)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                if i == 0:\n",
        "                    n = min(data.size(0), 18)\n",
        "                    # Properly reshape for comparison\n",
        "                    comparison = torch.cat(\n",
        "                        [data[:n], recon_batch.view(data.size(0), 1, 28, 28)[:n]]\n",
        "                    )\n",
        "                    save_image(\n",
        "                        comparison.cpu(),\n",
        "                        os.path.join(self.config.output_dir, f\"recon_{epoch}.png\"),\n",
        "                        nrow=n,\n",
        "                    )\n",
        "\n",
        "        avg_loss = test_loss / len(self.test_loader.dataset)\n",
        "        print(f\"====> Test set loss: {avg_loss:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def generate_samples(self, epoch: int, num_samples: int = 64):\n",
        "        \"\"\"Generate samples from the model\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Sample from latent space\n",
        "            sample = torch.randn(num_samples, self.config.category).to(self.device)\n",
        "            sample = self.model.decode(sample).cpu()\n",
        "            save_image(\n",
        "                sample.view(num_samples, 1, 28, 28),\n",
        "                os.path.join(self.config.output_dir, f\"sample_{epoch}.png\"),\n",
        "            )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        print(f\"Configuration: {self.config}\")\n",
        "\n",
        "        for epoch in range(1, self.config.epochs + 1):\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "            test_loss = self.test_epoch(epoch)\n",
        "            self.generate_samples(epoch)\n",
        "\n",
        "\n",
        "\"\"\"Main function\"\"\"\n",
        "# Parse arguments\n",
        "parser = create_argument_parser()\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "# Create configuration\n",
        "config = Config(\n",
        "    batch_size=args.batch_size,\n",
        "    epochs=args.epochs,\n",
        "    learning_rate=args.learning_rate,\n",
        "    no_cuda=args.no_cuda,\n",
        "    seed=args.seed,\n",
        "    log_interval=args.log_interval,\n",
        "    category=args.category,\n",
        "    alpha=args.alpha,\n",
        "    data_dir=args.data_dir,\n",
        "    output_dir=args.output_dir,\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Setup device and random seeds\n",
        "    device = setup_device_and_seed(config)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader, test_loader = create_data_loaders(config)\n",
        "\n",
        "    # Create model\n",
        "    model = DirVAE(config, device).to(device)\n",
        "    print(\n",
        "        f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\"\n",
        "    )\n",
        "\n",
        "    # Create trainer and start training\n",
        "    trainer = DirVAETrainer(model, config, device, train_loader, test_loader)\n",
        "    trainer.train()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\", file=sys.stderr)\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Gauss try (doesnt work yet)\n",
        "\n",
        "\"\"\"\n",
        "Gauss Variational Auto-Encoder (Gauss-VAE) implementation for MNIST\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for Gauss-VAE training\"\"\"\n",
        "\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 10\n",
        "    learning_rate: float = 1e-3\n",
        "    no_cuda: bool = False\n",
        "    seed: int = 10\n",
        "    log_interval: int = 2\n",
        "    category: int = 10  # Number of latent categories (K)\n",
        "    data_dir: str = \"./data\"\n",
        "    output_dir: str = \"./image\"\n",
        "\n",
        "    # Network architecture parameters\n",
        "    encoder_channels: int = 64\n",
        "    decoder_channels: int = 64\n",
        "    input_channels: int = 1\n",
        "    latent_dim: int = 1024\n",
        "    hidden_dim: int = 512\n",
        "\n",
        "\n",
        "def create_argument_parser() -> argparse.ArgumentParser:\n",
        "    \"\"\"Create and configure argument parser\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Gauss-VAE MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=256,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 256)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 10)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning-rate\",\n",
        "        type=float,\n",
        "        default=1e-3,\n",
        "        help=\"learning rate (default: 1e-3)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no-cuda\", action=\"store_true\", default=False, help=\"disable CUDA training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=10, metavar=\"S\", help=\"random seed (default: 10)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=2,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--category\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"K\",\n",
        "        help=\"the number of categories in the dataset\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--data-dir\",\n",
        "        type=str,\n",
        "        default=\"./data\",\n",
        "        help=\"directory for dataset (default: ./data)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output-dir\",\n",
        "        type=str,\n",
        "        default=\"./image\",\n",
        "        help=\"directory for output images (default: ./image)\",\n",
        "    )\n",
        "    return parser\n",
        "\n",
        "\n",
        "def setup_device_and_seed(config: Config) -> torch.device:\n",
        "    \"\"\"Setup device and random seeds\"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config.seed)\n",
        "\n",
        "    # Determine device\n",
        "    use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def create_data_loaders(\n",
        "    config: Config,\n",
        ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
        "    \"\"\"Create train and test data loaders\"\"\"\n",
        "    # Create data directory if it doesn't exist\n",
        "    os.makedirs(config.data_dir, exist_ok=True)\n",
        "\n",
        "    # Data loader kwargs\n",
        "    kwargs = (\n",
        "        {\"num_workers\": 1, \"pin_memory\": True}\n",
        "        if not config.no_cuda and torch.cuda.is_available()\n",
        "        else {}\n",
        "    )\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(\n",
        "            config.data_dir, train=True, download=True, transform=transforms.ToTensor()\n",
        "        ),\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(config.data_dir, train=False, transform=transforms.ToTensor()),\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "\n",
        "class GaussVAEEncoder(nn.Module):\n",
        "    \"\"\"Encoder part of Gauss-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super(GaussVAEEncoder, self).__init__()\n",
        "        ndf = config.encoder_channels\n",
        "        nc = config.input_channels\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # input: (nc) x 28 x 28\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf) x 14 x 14\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf*2) x 7 x 7\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf*4) x 4 x 4\n",
        "            nn.Conv2d(ndf * 4, config.latent_dim, 4, 1, 0, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(config.latent_dim, config.hidden_dim)\n",
        "        self.fc_mu = nn.Linear(config.hidden_dim, config.category)\n",
        "        self.fc_logvar = nn.Linear(config.hidden_dim, config.category)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        conv_out = self.conv_layers(x)\n",
        "        h1 = self.fc1(conv_out.view(conv_out.size(0), -1))\n",
        "        mu = self.fc_mu(h1)\n",
        "        logvar = self.fc_logvar(h1)\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "class GaussVAEDecoder(nn.Module):\n",
        "    \"\"\"Decoder part of Gauss-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super(GaussVAEDecoder, self).__init__()\n",
        "        ngf = config.decoder_channels\n",
        "        nc = config.input_channels\n",
        "\n",
        "        self.fc_decode = nn.Linear(config.category, config.hidden_dim)\n",
        "        self.fc_deconv = nn.Linear(config.hidden_dim, config.latent_dim)\n",
        "\n",
        "        self.deconv_layers = nn.Sequential(\n",
        "            # input: latent_dim x 1 x 1\n",
        "            nn.ConvTranspose2d(config.latent_dim, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "            # output: (nc) x 32 x 32\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
        "        h3 = self.relu(self.fc_decode(gauss_z))\n",
        "        deconv_input = self.fc_deconv(h3)\n",
        "        deconv_input = deconv_input.view(-1, deconv_input.size(1), 1, 1)\n",
        "\n",
        "        return self.deconv_layers(deconv_input)\n",
        "\n",
        "\n",
        "class GaussVAE(nn.Module):\n",
        "    \"\"\"Gauss Variational Auto-Encoder\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config, device: torch.device):\n",
        "        super(GaussVAE, self).__init__()\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = GaussVAEEncoder(config)\n",
        "        self.decoder = GaussVAEDecoder(config)\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Encode input to latent parameters\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Decode latent variables to reconstruction\"\"\"\n",
        "        return self.decoder(gauss_z)\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Reparameterization trick for backpropagation through stochastic nodes\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(gauss_z)\n",
        "\n",
        "        return recon_x, mu, logvar, gauss_z\n",
        "\n",
        "    def loss_function(\n",
        "        self,\n",
        "        recon_x: torch.Tensor,\n",
        "        x: torch.Tensor,\n",
        "        mu: torch.Tensor,\n",
        "        logvar: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the loss function: reconstruction loss + KL divergence\n",
        "\n",
        "        Args:\n",
        "            recon_x: Reconstructed images\n",
        "            x: Original images\n",
        "            mu: Mean of latent distribution\n",
        "            logvar: Log variance of latent distribution\n",
        "\n",
        "        Returns:\n",
        "            Total loss (sum over batch)\n",
        "        \"\"\"\n",
        "        # Reconstruction loss (Binary Cross Entropy)\n",
        "        BCE = F.binary_cross_entropy(\n",
        "            recon_x.view(-1, 784), x.view(-1, 784), reduction=\"sum\"\n",
        "        )\n",
        "\n",
        "\n",
        "        # KL divergence\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return BCE + KLD\n",
        "\n",
        "\n",
        "class GaussVAETrainer:\n",
        "    \"\"\"Trainer class for Gauss-VAE\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: GaussVAE,\n",
        "        config: Config,\n",
        "        device: torch.device,\n",
        "        train_loader: torch.utils.data.DataLoader,\n",
        "        test_loader: torch.utils.data.DataLoader,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "    def train_epoch(self, epoch: int) -> float:\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
        "            data = data.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            recon_batch, mu, logvar, gauss_z= self.model(data)\n",
        "            loss = self.model.loss_function(recon_batch, data, mu, logvar)\n",
        "\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if batch_idx % self.config.log_interval == 0:\n",
        "                print(\n",
        "                    f\"Train Epoch: {epoch} [{batch_idx * len(data):5d}/\"\n",
        "                    f\"{len(self.train_loader.dataset)} \"\n",
        "                    f\"({100. * batch_idx / len(self.train_loader):3.0f}%)]\"\n",
        "                    f\"\\tLoss: {loss.item() / len(data):.6f}\"\n",
        "                )\n",
        "\n",
        "        avg_loss = train_loss / len(self.train_loader.dataset)\n",
        "        print(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def test_epoch(self, epoch: int) -> float:\n",
        "        \"\"\"Test for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (data, _) in enumerate(self.test_loader):\n",
        "                data = data.to(self.device)\n",
        "                recon_batch, mu, logvar, gauss_z = self.model(data)\n",
        "                loss = self.model.loss_function(recon_batch, data, mu, logvar)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                if i == 0:\n",
        "                    n = min(data.size(0), 18)\n",
        "                    # Properly reshape for comparison\n",
        "                    comparison = torch.cat(\n",
        "                        [data[:n], recon_batch.view(data.size(0), 1, 28, 28)[:n]]\n",
        "                    )\n",
        "                    save_image(\n",
        "                        comparison.cpu(),\n",
        "                        os.path.join(self.config.output_dir, f\"recon_{epoch}.png\"),\n",
        "                        nrow=n,\n",
        "                    )\n",
        "\n",
        "        avg_loss = test_loss / len(self.test_loader.dataset)\n",
        "        print(f\"====> Test set loss: {avg_loss:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def generate_samples(self, epoch: int, num_samples: int = 64):\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        # sample from standard Gaussian prior\n",
        "        z = torch.randn(num_samples, self.config.latent_dim, device=self.device)\n",
        "        sample = self.model.decode(z).cpu()\n",
        "        save_image(\n",
        "            sample.view(num_samples, 1, 28, 28),\n",
        "            os.path.join(self.config.output_dir, f\"sample_{epoch}.png\"),\n",
        "            nrow=8\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        print(f\"Configuration: {self.config}\")\n",
        "\n",
        "        for epoch in range(1, self.config.epochs + 1):\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "            test_loss = self.test_epoch(epoch)\n",
        "            self.generate_samples(epoch)\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    parser = create_argument_parser()\n",
        "\n",
        "    # When running in notebook, argv should be empty to use defaults.\n",
        "    if argv is None:\n",
        "        # Detect running in IPython/Jupyter and avoid taking notebook argv\n",
        "        if \"ipykernel\" in sys.modules:\n",
        "            args = parser.parse_args([])\n",
        "        else:\n",
        "            args = parser.parse_args()\n",
        "    else:\n",
        "        args = parser.parse_args(argv)\n",
        "\n",
        "    config = Config(\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        learning_rate=args.learning_rate,\n",
        "        no_cuda=args.no_cuda,\n",
        "        seed=args.seed,\n",
        "        log_interval=args.log_interval,\n",
        "        category=args.category,\n",
        "        data_dir=args.data_dir,\n",
        "        output_dir=args.output_dir,\n",
        "    )\n",
        "\n",
        "    device = setup_device_and_seed(config)\n",
        "    train_loader, test_loader = create_data_loaders(config)\n",
        "\n",
        "    model = GaussVAE(config, device).to(device)\n",
        "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model created with {n_params} trainable parameters\")\n",
        "\n",
        "    trainer = GaussVAETrainer(model, config, device, train_loader, test_loader)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqaqSUb7YyAK",
        "outputId": "2675e9d1-a018-4dd1-d022-3917ea5c6c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model created with 15784468 trainable parameters\n",
            "Starting training...\n",
            "Configuration: Config(batch_size=256, epochs=10, learning_rate=0.001, no_cuda=False, seed=10, log_interval=2, category=10, data_dir='./data', output_dir='./image', encoder_channels=64, decoder_channels=64, input_channels=1, latent_dim=1024, hidden_dim=512)\n",
            "Train Epoch: 1 [    0/60000 (  0%)]\tLoss: 664.023376\n",
            "Train Epoch: 1 [  512/60000 (  1%)]\tLoss: 1024.685303\n",
            "Train Epoch: 1 [ 1024/60000 (  2%)]\tLoss: 387.771484\n",
            "Train Epoch: 1 [ 1536/60000 (  3%)]\tLoss: 277.235809\n",
            "Train Epoch: 1 [ 2048/60000 (  3%)]\tLoss: 259.702667\n",
            "Train Epoch: 1 [ 2560/60000 (  4%)]\tLoss: 240.780960\n",
            "Train Epoch: 1 [ 3072/60000 (  5%)]\tLoss: 236.565521\n",
            "Train Epoch: 1 [ 3584/60000 (  6%)]\tLoss: 226.450684\n",
            "Train Epoch: 1 [ 4096/60000 (  7%)]\tLoss: 227.766541\n",
            "Train Epoch: 1 [ 4608/60000 (  8%)]\tLoss: 213.367126\n",
            "Train Epoch: 1 [ 5120/60000 (  9%)]\tLoss: 211.869003\n",
            "Train Epoch: 1 [ 5632/60000 (  9%)]\tLoss: 208.035919\n",
            "Train Epoch: 1 [ 6144/60000 ( 10%)]\tLoss: 212.186783\n",
            "Train Epoch: 1 [ 6656/60000 ( 11%)]\tLoss: 207.046753\n",
            "Train Epoch: 1 [ 7168/60000 ( 12%)]\tLoss: 199.860352\n",
            "Train Epoch: 1 [ 7680/60000 ( 13%)]\tLoss: 202.235535\n",
            "Train Epoch: 1 [ 8192/60000 ( 14%)]\tLoss: 206.863174\n",
            "Train Epoch: 1 [ 8704/60000 ( 14%)]\tLoss: 201.291656\n",
            "Train Epoch: 1 [ 9216/60000 ( 15%)]\tLoss: 203.741608\n",
            "Train Epoch: 1 [ 9728/60000 ( 16%)]\tLoss: 198.589096\n",
            "Train Epoch: 1 [10240/60000 ( 17%)]\tLoss: 202.989899\n",
            "Train Epoch: 1 [10752/60000 ( 18%)]\tLoss: 202.083633\n",
            "Train Epoch: 1 [11264/60000 ( 19%)]\tLoss: 198.783524\n",
            "Train Epoch: 1 [11776/60000 ( 20%)]\tLoss: 197.298279\n",
            "Train Epoch: 1 [12288/60000 ( 20%)]\tLoss: 196.004288\n",
            "Train Epoch: 1 [12800/60000 ( 21%)]\tLoss: 196.221893\n",
            "Train Epoch: 1 [13312/60000 ( 22%)]\tLoss: 198.466995\n",
            "Train Epoch: 1 [13824/60000 ( 23%)]\tLoss: 194.781418\n",
            "Train Epoch: 1 [14336/60000 ( 24%)]\tLoss: 192.966660\n",
            "Train Epoch: 1 [14848/60000 ( 25%)]\tLoss: 192.227982\n",
            "Train Epoch: 1 [15360/60000 ( 26%)]\tLoss: 195.970978\n",
            "Train Epoch: 1 [15872/60000 ( 26%)]\tLoss: 192.829437\n",
            "Train Epoch: 1 [16384/60000 ( 27%)]\tLoss: 188.759094\n",
            "Train Epoch: 1 [16896/60000 ( 28%)]\tLoss: 195.433853\n",
            "Train Epoch: 1 [17408/60000 ( 29%)]\tLoss: 193.483749\n",
            "Train Epoch: 1 [17920/60000 ( 30%)]\tLoss: 188.734756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Forsøg på DIR fra den nye artikel\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "EPS = 1e-8\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for Dir-VAE training\"\"\"\n",
        "\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 10\n",
        "    learning_rate: float = 1e-3\n",
        "    no_cuda: bool = False\n",
        "    seed: int = 10\n",
        "    log_interval: int = 2\n",
        "    category: int = 10  # Number of latent categories (K)\n",
        "    alpha: float = 0.3  # Dirichlet hyperparameter\n",
        "    data_dir: str = \"./data\"\n",
        "    output_dir: str = \"./image\"\n",
        "    beta=1.0\n",
        "\n",
        "    # Network architecture parameters\n",
        "    encoder_channels: int = 64\n",
        "    decoder_channels: int = 64\n",
        "    input_channels: int = 1\n",
        "    latent_dim: int = 1024\n",
        "    hidden_dim: int = 512\n",
        "\n",
        "\n",
        "def create_argument_parser() -> argparse.ArgumentParser:\n",
        "    \"\"\"Create and configure argument parser\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Dir-VAE MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=256,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 256)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 10)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning-rate\",\n",
        "        type=float,\n",
        "        default=1e-3,\n",
        "        help=\"learning rate (default: 1e-3)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no-cuda\", action=\"store_true\", default=False, help=\"disable CUDA training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=10, metavar=\"S\", help=\"random seed (default: 10)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=2,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--category\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"K\",\n",
        "        help=\"the number of categories in the dataset\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--alpha\",\n",
        "        type=float,\n",
        "        default=0.3,\n",
        "        help=\"Dirichlet hyperparameter alpha (default: 0.3)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data-dir\",\n",
        "        type=str,\n",
        "        default=\"./data\",\n",
        "        help=\"directory for dataset (default: ./data)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output-dir\",\n",
        "        type=str,\n",
        "        default=\"./image\",\n",
        "        help=\"directory for output images (default: ./image)\",\n",
        "    )\n",
        "    return parser\n",
        "\n",
        "\n",
        "def setup_device_and_seed(config: Config) -> torch.device:\n",
        "    \"\"\"Setup device and random seeds\"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config.seed)\n",
        "\n",
        "    # Determine device\n",
        "    use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def create_data_loaders(\n",
        "    config: Config,\n",
        ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
        "    \"\"\"Create train and test data loaders\"\"\"\n",
        "    # Create data directory if it doesn't exist\n",
        "    os.makedirs(config.data_dir, exist_ok=True)\n",
        "\n",
        "    # Data loader kwargs\n",
        "    kwargs = (\n",
        "        {\"num_workers\": 1, \"pin_memory\": True}\n",
        "        if not config.no_cuda and torch.cuda.is_available()\n",
        "        else {}\n",
        "    )\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(\n",
        "            config.data_dir, train=True, download=True, transform=transforms.ToTensor()\n",
        "        ),\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(config.data_dir, train=False, transform=transforms.ToTensor()),\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def compute_dirichlet_prior(\n",
        "    K: int, alpha: float, device: torch.device\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute Dirichlet prior parameters using Laplace approximation.\n",
        "\n",
        "    Args:\n",
        "        K: Number of categories\n",
        "        alpha: Dirichlet hyperparameter\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (mean, variance) tensors for the approximated normal distribution\n",
        "    \"\"\"\n",
        "    # Laplace approximation to convert Dirichlet to multivariate normal\n",
        "    a = torch.full((1, K), alpha, dtype=torch.float, device=device)\n",
        "    mean = a.log().t() - a.log().mean(1, keepdim=True)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K**2) * a.reciprocal().sum(\n",
        "        1, keepdim=True\n",
        "    )\n",
        "    return mean.t(), var.t()\n",
        "\n",
        "\n",
        "class DirVAEEncoder(nn.Module):\n",
        "    \"\"\"Encoder part of Dir-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super(DirVAEEncoder, self).__init__()\n",
        "        ndf = config.encoder_channels\n",
        "        nc = config.input_channels\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # input: (nc) x 28 x 28\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf) x 14 x 14\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf*2) x 7 x 7\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state: (ndf*4) x 4 x 4\n",
        "            nn.Conv2d(ndf * 4, config.latent_dim, 4, 1, 0, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(config.latent_dim, config.hidden_dim)\n",
        "        self.fc_mu = nn.Linear(config.hidden_dim, config.category)\n",
        "        self.fc_logvar = nn.Linear(config.hidden_dim, config.category)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        conv_out = self.conv_layers(x)\n",
        "        h1 = self.fc1(conv_out.view(conv_out.size(0), -1))\n",
        "        mu = self.fc_mu(h1)\n",
        "        alpha_hat=F.softplus(mu)+1e-6\n",
        "        return alpha_hat\n",
        "\n",
        "\n",
        "class DirVAEDecoder(nn.Module):\n",
        "    \"\"\"Decoder part of Dir-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super(DirVAEDecoder, self).__init__()\n",
        "        ngf = config.decoder_channels\n",
        "        nc = config.input_channels\n",
        "\n",
        "        self.fc_decode = nn.Linear(config.category, config.hidden_dim)\n",
        "        self.fc_deconv = nn.Linear(config.hidden_dim, config.latent_dim)\n",
        "\n",
        "        self.deconv_layers = nn.Sequential(\n",
        "            # input: latent_dim x 1 x 1\n",
        "            nn.ConvTranspose2d(config.latent_dim, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state: (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "            # output: (nc) x 32 x 32\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
        "        # Apply softmax to satisfy simplex constraint (Dirichlet distribution)\n",
        "        dir_z = F.softmax(gauss_z, dim=1)\n",
        "\n",
        "        h3 = self.relu(self.fc_decode(dir_z))\n",
        "        deconv_input = self.fc_deconv(h3)\n",
        "        deconv_input = deconv_input.view(-1, deconv_input.size(1), 1, 1)\n",
        "\n",
        "        return self.deconv_layers(deconv_input)\n",
        "\n",
        "\n",
        "class DirVAE(nn.Module):\n",
        "    \"\"\"Dirichlet Variational Auto-Encoder\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config, device: torch.device):\n",
        "        super(DirVAE, self).__init__()\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = DirVAEEncoder(config)\n",
        "        self.decoder = DirVAEDecoder(config)\n",
        "        # Initialize prior_alpha\n",
        "        if not hasattr(config, \"prior_alpha\") or config.alpha is None:\n",
        "            # default weak symmetric prior\n",
        "            prior_alpha = torch.ones(config.category) * 0.98\n",
        "        else:\n",
        "            # use config.alpha (scalar or tensor)\n",
        "            prior_alpha = torch.tensor(config.alpha).repeat(config.category)\n",
        "\n",
        "        self.register_buffer('prior_alpha', prior_alpha.float())\n",
        "        self.register_buffer('beta', torch.tensor(float(config.beta)))\n",
        "\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Encode input to latent parameters\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Decode latent variables to reconstruction\"\"\"\n",
        "        return self.decoder(gauss_z)\n",
        "\n",
        "    def inverse_gamma_cdf_approx(self, u, alpha):\n",
        "        \"\"\"\n",
        "        Approximate inverse CDF for X ~ Gamma(alpha, beta) using:\n",
        "        F^{-1}(u; alpha, beta) ≈ beta^{-1} * (u * alpha * Gamma(alpha))^{1/alpha}\n",
        "        u: uniform samples in (0,1), shape (batch, K)\n",
        "        alpha: shape (batch, K) or (K,)\n",
        "        returns: approx gamma samples shape (batch, K)\n",
        "        \"\"\"\n",
        "        # alpha * Gamma(alpha) = alpha * exp(lgamma(alpha))\n",
        "        # note: torch.lgamma for log Gamma\n",
        "        # shapes broadcast\n",
        "        log_gamma = torch.lgamma(alpha)\n",
        "        a_gamma = alpha * torch.exp(log_gamma)  # shape (batch, K)\n",
        "        # clamp u to (eps, 1)\n",
        "        u = u.clamp(min=EPS, max=1.0 - 1e-12)\n",
        "        base = u * a_gamma\n",
        "        # to avoid negative/zero values due to numerical issues, clamp base\n",
        "        base = base.clamp(min=EPS)\n",
        "        # power 1/alpha\n",
        "        samples = (base) ** (1.0 / alpha)\n",
        "        # divide by beta (rate)\n",
        "        samples = samples / (self.beta + 0.0)\n",
        "        return samples\n",
        "\n",
        "    def sample_dirichlet_from_alpha(self, alpha_hat):\n",
        "        \"\"\"\n",
        "        Given alpha_hat (batch, K), produce reparam samples z on simplex:\n",
        "          1) draw u ~ Uniform(0,1) per component\n",
        "          2) approximate gamma sample via inverse Gamma CDF approx\n",
        "          3) normalize v -> z = v / sum_k v_k\n",
        "        Returns z (batch, K), v (batch, K), u (for reproducibility)\n",
        "        \"\"\"\n",
        "        assert torch.all(alpha_hat > 0), \"alpha_hat must be positive\"\n",
        "        batch = alpha_hat.shape[0]\n",
        "        # Uniform draws per component\n",
        "        u = torch.rand_like(alpha_hat)  # Uniform(0,1)\n",
        "        v = self.inverse_gamma_cdf_approx(u, alpha_hat)\n",
        "        # Normalize to get Dirichlet sample\n",
        "        denom = v.sum(dim=1, keepdim=True).clamp(min=EPS)\n",
        "        z = v / denom\n",
        "        return z, v, u\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        alpha_hat = self.encode(x)\n",
        "        z, v, u = self.sample_dirichlet_from_alpha(alpha_hat)\n",
        "        recon_x = self.decode(z)\n",
        "\n",
        "        return recon_x, z,alpha_hat,v\n",
        "\n",
        "    def loss_function(\n",
        "        self,\n",
        "        recon_x: torch.Tensor,\n",
        "        x: torch.Tensor,\n",
        "        alpha_hat: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the loss function: reconstruction loss + KL divergence\n",
        "\n",
        "        Args:\n",
        "            recon_x: Reconstructed images\n",
        "            x: Original images\n",
        "            mu: Mean of latent distribution\n",
        "            logvar: Log variance of latent distribution\n",
        "\n",
        "        Returns:\n",
        "            Total loss (sum over batch)\n",
        "        \"\"\"\n",
        "        # Reconstruction loss (Binary Cross Entropy)\n",
        "        BCE = F.binary_cross_entropy(\n",
        "            recon_x.view(-1, 784), x.view(-1, 784), reduction=\"sum\"\n",
        "        )\n",
        "\n",
        "        # KL divergence\n",
        "        # broadcast prior_alpha to batch if necessary\n",
        "        if self.prior_alpha.dim() == 1:\n",
        "            prior = self.prior_alpha.unsqueeze(0).expand_as(alpha_hat)\n",
        "        else:\n",
        "            prior = self.prior_alpha\n",
        "        term1 = torch.lgamma(prior) - torch.lgamma(alpha_hat)\n",
        "        term2 = (alpha_hat - prior) * torch.digamma(alpha_hat)\n",
        "        kl_comp = term1 + term2\n",
        "        kl = kl_comp.sum(dim=1)  # per example sum over K\n",
        "        KLD=kl.mean()\n",
        "        return BCE + KLD\n",
        "\n",
        "\n",
        "class DirVAETrainer:\n",
        "    \"\"\"Trainer class for Dir-VAE\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: DirVAE,\n",
        "        config: Config,\n",
        "        device: torch.device,\n",
        "        train_loader: torch.utils.data.DataLoader,\n",
        "        test_loader: torch.utils.data.DataLoader,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "    def train_epoch(self, epoch: int) -> float:\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
        "            data = data.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            recon_batch, z,alpha_hat,v = self.model(data)\n",
        "            loss = self.model.loss_function(recon_batch,data, alpha_hat)\n",
        "\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if batch_idx % self.config.log_interval == 0:\n",
        "                print(\n",
        "                    f\"Train Epoch: {epoch} [{batch_idx * len(data):5d}/\"\n",
        "                    f\"{len(self.train_loader.dataset)} \"\n",
        "                    f\"({100. * batch_idx / len(self.train_loader):3.0f}%)]\"\n",
        "                    f\"\\tLoss: {loss.item() / len(data):.6f}\"\n",
        "                )\n",
        "\n",
        "        avg_loss = train_loss / len(self.train_loader.dataset)\n",
        "        print(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def test_epoch(self, epoch: int) -> float:\n",
        "        \"\"\"Test for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (data, _) in enumerate(self.test_loader):\n",
        "                data = data.to(self.device)\n",
        "                recon_batch,z, alpha_hat,v = self.model(data)\n",
        "                loss = self.model.loss_function(recon_batch,data, alpha_hat)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                if i == 0:\n",
        "                    n = min(data.size(0), 18)\n",
        "                    # Properly reshape for comparison\n",
        "                    comparison = torch.cat(\n",
        "                        [data[:n], recon_batch.view(data.size(0), 1, 28, 28)[:n]]\n",
        "                    )\n",
        "                    save_image(\n",
        "                        comparison.cpu(),\n",
        "                        os.path.join(self.config.output_dir, f\"recon_{epoch}.png\"),\n",
        "                        nrow=n,\n",
        "                    )\n",
        "\n",
        "        avg_loss = test_loss / len(self.test_loader.dataset)\n",
        "        print(f\"====> Test set loss: {avg_loss:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def generate_samples(self, epoch: int, num_samples: int = 64):\n",
        "        \"\"\"Generate samples from the model\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Sample from latent space\n",
        "            sample = torch.randn(num_samples, self.config.category).to(self.device)\n",
        "            sample = self.model.decode(sample).cpu()\n",
        "            save_image(\n",
        "                sample.view(num_samples, 1, 28, 28),\n",
        "                os.path.join(self.config.output_dir, f\"sample_{epoch}.png\"),\n",
        "            )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        print(f\"Configuration: {self.config}\")\n",
        "\n",
        "        for epoch in range(1, self.config.epochs + 1):\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "            test_loss = self.test_epoch(epoch)\n",
        "            self.generate_samples(epoch)\n",
        "\n",
        "\n",
        "\"\"\"Main function\"\"\"\n",
        "# Parse arguments\n",
        "parser = create_argument_parser()\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "# Create configuration\n",
        "config = Config(\n",
        "    batch_size=args.batch_size,\n",
        "    epochs=args.epochs,\n",
        "    learning_rate=args.learning_rate,\n",
        "    no_cuda=args.no_cuda,\n",
        "    seed=args.seed,\n",
        "    log_interval=args.log_interval,\n",
        "    category=args.category,\n",
        "    alpha=args.alpha,\n",
        "    data_dir=args.data_dir,\n",
        "    output_dir=args.output_dir,\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Setup device and random seeds\n",
        "    device = setup_device_and_seed(config)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader, test_loader = create_data_loaders(config)\n",
        "\n",
        "    # Create model\n",
        "    model = DirVAE(config, device).to(device)\n",
        "    print(\n",
        "        f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\"\n",
        "    )\n",
        "\n",
        "    # Create trainer and start training\n",
        "    trainer = DirVAETrainer(model, config, device, train_loader, test_loader)\n",
        "    trainer.train()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\", file=sys.stderr)\n",
        "    sys.exit(1)\n"
      ],
      "metadata": {
        "id": "-BQWTMx5RxP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}