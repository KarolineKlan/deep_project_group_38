{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP60Men83xfUZMxA+zzAn6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarolineKlan/deep_project_group_38/blob/main/ExperimentSetup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ2EA9ucY7JL",
        "outputId": "fd142441-e497-4e34-e55d-c8e7e4e8b406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 20.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 477kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.43MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.87MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split sizes → train: 45000, val: 5000, test: 10000\n",
            "Device: cuda\n",
            "\n",
            "=== Training GVAE ===\n",
            "Epoch 05 | Train ELBO 105.86 (BCE 87.24, KL 18.63)  | Val ELBO 106.22 (BCE 87.70, KL 18.52)\n",
            "Epoch 10 | Train ELBO 101.29 (BCE 82.05, KL 19.24)  | Val ELBO 102.23 (BCE 83.08, KL 19.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # MNIST — GVAE vs DirVAE (inverse Gamma CDF) vs CC-Placeholder\n",
        "# One model is trained **at a time**, results are saved, then a final comparison is plotted.\n",
        "# Output dir: /content/outputs\n",
        "#\n",
        "# DirVAE notes (paper equations referenced below):\n",
        "# • Dirichlet = normalized composition of K independent Gamma(α_k, β) variables. (Sec. 2.2)\n",
        "# • KL between MultiGamma posteriors/prior (Eq. (3)):  Σ_k[ log Γ(α_k) − log Γ(α̂_k) + (α̂_k−α_k) ψ(α̂_k) ].\n",
        "# • ELBO (Eq. (7)): reconstruction (BCE) + the MultiGamma KL term above.\n",
        "# • “Fair” prior vs. softmax–Gaussian Laplace note (Eq. (5)): α_k = 1 − 1/K when μ=0, Σ=I; we adopt that prior.\n",
        "# • **Sampling path**: inverse Gamma CDF approximation (Knowles 2015) — NO Laplace reparam. (Sec. 3)\n",
        "#     X ~ Gamma(α, β): F^{-1}(u; α, β) ≈ β^{-1} (u^α Γ(α))^{1/α}, with u ~ Uniform(0,1).\n",
        "#     Implemented in log-space for stability:  log X = log u + lgamma(α)/α − log β.\n",
        "#\n",
        "# Citations: DirVAE paper and the inverse-CDF approach are from your uploaded PDF.  :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "import os, math, random\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, utils as vutils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Config (all knobs in one place)\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_root: str = \"/content/data\"\n",
        "    out_dir: str = \"/content/outputs\"\n",
        "    # training\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 10\n",
        "    lr: float = 1e-3\n",
        "    seed: int = 42\n",
        "    # model\n",
        "    latent_dim: int = 10         # K (match classes) for all models\n",
        "    beta_gamma_rate: float = 1.0 # β for DirVAE Gamma rate (shared across dims)\n",
        "    enc_ch: int = 32\n",
        "    dec_ch: int = 32\n",
        "    # eval\n",
        "    tsne_samples: int = 5000\n",
        "    # dataloader (keep simple & robust in Colab)\n",
        "    num_workers: int = 0\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "# Reproducibility & device\n",
        "def set_seeds(s: int):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seeds(cfg.seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# -----------------------------\n",
        "# Data (45k/5k/10k split as requested)\n",
        "# -----------------------------\n",
        "transform = transforms.ToTensor()\n",
        "mnist_full = datasets.MNIST(cfg.data_root, train=True, download=True, transform=transform)\n",
        "test = datasets.MNIST(cfg.data_root, train=False, download=True, transform=transform)\n",
        "\n",
        "# 45k train / 5k val / (drop 10k from train set); official 10k test used for test\n",
        "train_len = 45_000\n",
        "rest_len = len(mnist_full) - train_len\n",
        "train, rest = random_split(mnist_full, [train_len, rest_len],\n",
        "                           generator=torch.Generator().manual_seed(cfg.seed))\n",
        "val_len, drop_len = 5_000, rest_len - 5_000\n",
        "val, _ = random_split(rest, [val_len, drop_len],\n",
        "                      generator=torch.Generator().manual_seed(cfg.seed + 1))\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=cfg.batch_size, shuffle=True,\n",
        "                          num_workers=cfg.num_workers, pin_memory=False)\n",
        "val_loader   = DataLoader(val,   batch_size=cfg.batch_size, shuffle=False,\n",
        "                          num_workers=cfg.num_workers, pin_memory=False)\n",
        "test_loader  = DataLoader(test,  batch_size=cfg.batch_size, shuffle=False,\n",
        "                          num_workers=cfg.num_workers, pin_memory=False)\n",
        "\n",
        "print(f\"Split sizes → train: {len(train)}, val: {len(val)}, test: {len(test)}\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Shared CNN encoder/decoder\n",
        "# -----------------------------\n",
        "class EncoderCNN(nn.Module):\n",
        "    \"\"\"Small CNN encoder → 256-d hidden.\"\"\"\n",
        "    def __init__(self, ch: int = 32):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, ch, 4, 2, 1),  # 28→14\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ch, ch*2, 4, 2, 1),  # 14→7\n",
        "            nn.BatchNorm2d(ch*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ch*2, ch*4, 3, 1, 1),  # stay 7\n",
        "            nn.BatchNorm2d(ch*4),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(ch*4*7*7, 256), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):  # (B,1,28,28) → (B,256)\n",
        "        return self.fc(self.net(x))\n",
        "\n",
        "class DecoderCNN(nn.Module):\n",
        "    \"\"\"Deterministically outputs (B,1,28,28) logits (use BCEWithLogitsLoss).\"\"\"\n",
        "    def __init__(self, latent_dim: int, ch: int = 32):\n",
        "        super().__init__()\n",
        "        C = ch * 4\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, C*7*7),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(C, C//2, 4, 2, 1),  # 7→14\n",
        "            nn.BatchNorm2d(C//2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(C//2, 1, 4, 2, 1),  # 14→28\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = self.fc(z).view(z.size(0), -1, 7, 7)\n",
        "        return self.deconv(h)  # logits\n",
        "\n",
        "# -----------------------------\n",
        "# Bottlenecks\n",
        "# -----------------------------\n",
        "class GaussianBottleneck(nn.Module):\n",
        "    \"\"\"Diagonal Gaussian posterior; prior N(0,I).\"\"\"\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.mu = nn.Linear(256, latent_dim)\n",
        "        self.logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "    def forward(self, h):\n",
        "        mu, logvar = self.mu(h), self.logvar(h)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z = mu + std * torch.randn_like(std)\n",
        "        return z, {\"mu\": mu, \"logvar\": logvar}\n",
        "\n",
        "    def kl(self, aux):  # per-sample KL\n",
        "        mu, logvar = aux[\"mu\"], aux[\"logvar\"]\n",
        "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
        "\n",
        "    def embed(self, aux):  # for t-SNE, use μ\n",
        "        return aux[\"mu\"]\n",
        "\n",
        "class DirichletBottleneck(nn.Module):\n",
        "    r\"\"\"\n",
        "    DirVAE bottleneck using inverse Gamma CDF (Knowles 2015) with numerical stabilizations.\n",
        "    - Sample per-dim Gamma via:  log X = log u + lgamma(α̂)/α̂ − log β, u~U(0,1).\n",
        "    - Normalize in log-space: y = softmax(log_v) to avoid overflow.\n",
        "    - Clamp α̂ into [α_min, α_max] so lgamma/ψ are well-behaved.\n",
        "    - Prior α_k from Eq. (5) fairness note: α_k = 1 − 1/K, β = 1 by default.\n",
        "    - KL between MultiGamma’s (Eq. (3)).\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, beta_rate: float = 1.0, eps: float = 1e-6,\n",
        "                 alpha_min: float = 1e-1, alpha_max: float = 50.0):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.beta = beta_rate\n",
        "        self.eps = eps\n",
        "        self.alpha_min = alpha_min\n",
        "        self.alpha_max = alpha_max\n",
        "\n",
        "        self.alpha_raw = nn.Linear(256, latent_dim)  # α̂(x) > 0 via softplus\n",
        "        alpha0 = 1.0 - 1.0 / float(latent_dim)       # prior α_k (Eq. (5))\n",
        "        self.register_buffer(\"alpha_prior\", torch.full((latent_dim,), alpha0))\n",
        "\n",
        "    def _alpha_hat(self, h):\n",
        "        # Positive + clamped α̂ for stability in sampler & KL\n",
        "        ah = F.softplus(self.alpha_raw(h)) + 1e-6\n",
        "        return ah.clamp(min=self.alpha_min, max=self.alpha_max)\n",
        "\n",
        "    def _sample_multi_gamma_log(self, alpha_hat: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample log v for K independent Gamma(α̂_k, β) using the inverse CDF approximation.\n",
        "        Return log_v to allow log-space normalization (stable).\n",
        "        \"\"\"\n",
        "        B, K = alpha_hat.shape\n",
        "        u = torch.clamp(torch.rand(B, K, device=alpha_hat.device), 1e-6, 1.0 - 1e-6)\n",
        "        # log v = log u + lgamma(α̂)/α̂ − log β\n",
        "        log_v = torch.log(u) + torch.lgamma(alpha_hat) / alpha_hat - math.log(self.beta)\n",
        "        # Optional clamp to a safe range to avoid inf during backprop if something goes wrong\n",
        "        log_v = torch.clamp(log_v, min=-60.0, max=60.0)\n",
        "        return log_v\n",
        "\n",
        "    def forward(self, h):\n",
        "        alpha_hat = self._alpha_hat(h)           # (B,K) stabilized α̂\n",
        "        log_v = self._sample_multi_gamma_log(alpha_hat)\n",
        "        y = F.softmax(log_v, dim=1)              # normalize in log-space (stable)\n",
        "        return y, {\"alpha_hat\": alpha_hat}\n",
        "\n",
        "    def kl(self, aux):\n",
        "        \"\"\"\n",
        "        KL(Q||P) between MultiGamma(α̂, β) and MultiGamma(α, β):\n",
        "          Σ_k [ log Γ(α_k) − log Γ(α̂_k) + (α̂_k − α_k) ψ(α̂_k) ]  (Eq. (3))\n",
        "        \"\"\"\n",
        "        alpha_hat = aux[\"alpha_hat\"]  # already stabilized\n",
        "        alpha = self.alpha_prior.view(1, -1).expand_as(alpha_hat)\n",
        "        term = torch.lgamma(alpha) - torch.lgamma(alpha_hat) + (alpha_hat - alpha) * torch.digamma(alpha_hat)\n",
        "        return torch.sum(term, dim=1)\n",
        "\n",
        "    def embed(self, aux):\n",
        "        # Dirichlet mean for visualization: α̂ / Σ α̂\n",
        "        ah = aux[\"alpha_hat\"]\n",
        "        return ah / (ah.sum(dim=1, keepdim=True) + self.eps)\n",
        "\n",
        "\n",
        "class CCPlaceholderBottleneck(nn.Module):\n",
        "    r\"\"\"\n",
        "    Continuous–Categorical placeholder:\n",
        "      • Produce logits g(x); return softmax(g/τ) as simplex latent.\n",
        "      • KL = 0.5 * ||g||^2 (small regularizer).  TODO: replace with true CC sampler & KL.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, temperature: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.logits = nn.Linear(256, latent_dim)\n",
        "\n",
        "    def forward(self, h):\n",
        "        g = self.logits(h)\n",
        "        y = F.softmax(g / self.temperature, dim=1)\n",
        "        return y, {\"logits\": g}\n",
        "\n",
        "    def kl(self, aux):\n",
        "        mu = aux[\"logits\"]\n",
        "        return 0.5 * torch.sum(mu.pow(2), dim=1)\n",
        "\n",
        "    def embed(self, aux):  # for t-SNE\n",
        "        return F.softmax(aux[\"logits\"] / self.temperature, dim=1)\n",
        "\n",
        "# -----------------------------\n",
        "# VAE wrapper\n",
        "# -----------------------------\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, bottleneck: nn.Module, latent_dim: int, enc_ch: int = 32, dec_ch: int = 32):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN(enc_ch)\n",
        "        self.bottleneck = bottleneck\n",
        "        self.decoder = DecoderCNN(latent_dim, dec_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)               # (B,256)\n",
        "        z, aux = self.bottleneck(h)       # (B,K)\n",
        "        x_logits = self.decoder(z)        # (B,1,28,28)\n",
        "        return x_logits, z, aux\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval\n",
        "# -----------------------------\n",
        "def bce_recon_loss(x_logits, x):\n",
        "    if x_logits.shape[2:] != (28, 28):\n",
        "        raise ValueError(f\"Decoder produced {tuple(x_logits.shape)}; expected (B,1,28,28).\")\n",
        "    return F.binary_cross_entropy_with_logits(\n",
        "        x_logits.view(x.size(0), -1),\n",
        "        x.view(x.size(0), -1),\n",
        "        reduction=\"sum\"\n",
        "    )\n",
        "\n",
        "def run_epoch(model, loader, optimizer=None):\n",
        "    train = optimizer is not None\n",
        "    model.train(train)\n",
        "    bce_total, kl_total, n = 0.0, 0.0, 0\n",
        "    for x, _ in loader:\n",
        "        x = x.to(device)\n",
        "        if train: optimizer.zero_grad()\n",
        "        x_logits, _, aux = model(x)\n",
        "        bce = bce_recon_loss(x_logits, x)                 # sum over batch\n",
        "        kl  = model.bottleneck.kl(aux).sum()              # sum over batch\n",
        "        loss = bce + kl\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        bsz = x.size(0)\n",
        "        bce_total += float(bce.item()); kl_total += float(kl.item()); n += bsz\n",
        "    return {\"bce\": bce_total / n, \"kl\": kl_total / n, \"total\": (bce_total + kl_total) / n}\n",
        "\n",
        "@torch.no_grad()\n",
        "def make_recon_grid(model, loader, path: str, n: int = 10):\n",
        "    model.eval()\n",
        "    x, _ = next(iter(loader))\n",
        "    x = x.to(device)[:n]\n",
        "    x_logits, _, _ = model(x)\n",
        "    x_rec = torch.sigmoid(x_logits)\n",
        "    grid = torch.cat([x, x_rec], dim=0)\n",
        "    grid = vutils.make_grid(grid, nrow=n, padding=2)\n",
        "    vutils.save_image(grid, path)\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings(model, loader, max_n: int = 5000):\n",
        "    model.eval()\n",
        "    xs, ys, total = [], [], 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device); b = x.size(0)\n",
        "        h = model.encoder(x)\n",
        "        # Compute bottleneck stats without sampling the decoder\n",
        "        if isinstance(model.bottleneck, GaussianBottleneck):\n",
        "            aux = {\"mu\": model.bottleneck.mu(h), \"logvar\": model.bottleneck.logvar(h)}\n",
        "            emb = model.bottleneck.embed(aux)\n",
        "        elif isinstance(model.bottleneck, DirichletBottleneck):\n",
        "            ah = model.bottleneck._alpha_hat(h)  # use same stabilized α̂\n",
        "            emb = ah / (ah.sum(dim=1, keepdim=True) + 1e-6)\n",
        "        elif isinstance(model.bottleneck, CCPlaceholderBottleneck):\n",
        "            logits = model.bottleneck.logits(h)\n",
        "            emb = F.softmax(logits / model.bottleneck.temperature, dim=1)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown bottleneck.\")\n",
        "        xs.append(emb.detach().cpu()); ys.append(y); total += b\n",
        "        if total >= max_n: break\n",
        "    X = torch.cat(xs, 0).numpy()[:max_n]\n",
        "    Y = torch.cat(ys, 0).numpy()[:max_n]\n",
        "    return X, Y\n",
        "\n",
        "def tsne_plot(X, Y, title: str, path: str):\n",
        "    Z = TSNE(n_components=2, init=\"pca\", perplexity=30, n_iter=1000,\n",
        "             learning_rate=\"auto\", random_state=cfg.seed).fit_transform(X)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(Z[:,0], Z[:,1], c=Y, s=5, cmap=\"tab10\", alpha=0.8)\n",
        "    plt.title(title); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Run ONE model (training + eval + save)\n",
        "# -----------------------------\n",
        "def run_single(model_name: str) -> Dict[str, List[float]]:\n",
        "    if model_name == \"gvae\":\n",
        "        bottleneck = GaussianBottleneck(cfg.latent_dim)\n",
        "    elif model_name == \"dirvae\":\n",
        "        bottleneck = DirichletBottleneck(cfg.latent_dim, beta_rate=cfg.beta_gamma_rate)\n",
        "    elif model_name == \"cc\":\n",
        "        bottleneck = CCPlaceholderBottleneck(cfg.latent_dim, temperature=0.5)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model name.\")\n",
        "\n",
        "    model = VAE(bottleneck, cfg.latent_dim, enc_ch=cfg.enc_ch, dec_ch=cfg.dec_ch).to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "    history = {\"train_bce\": [], \"train_kl\": [], \"train_total\": [],\n",
        "               \"val_bce\": [], \"val_kl\": [], \"val_total\": []}\n",
        "\n",
        "    print(f\"\\n=== Training {model_name.upper()} ===\")\n",
        "    for epoch in range(1, cfg.epochs+1):\n",
        "        tr = run_epoch(model, train_loader, optimizer=opt)\n",
        "        va = run_epoch(model, val_loader, optimizer=None)\n",
        "        history[\"train_bce\"].append(tr[\"bce\"]); history[\"train_kl\"].append(tr[\"kl\"]); history[\"train_total\"].append(tr[\"total\"])\n",
        "        history[\"val_bce\"].append(va[\"bce\"]);   history[\"val_kl\"].append(va[\"kl\"]);   history[\"val_total\"].append(va[\"total\"])\n",
        "        if epoch % 5 == 0 or epoch == cfg.epochs:\n",
        "            print(f\"Epoch {epoch:02d} | Train ELBO {tr['total']:.2f} (BCE {tr['bce']:.2f}, KL {tr['kl']:.2f})  \"\n",
        "                  f\"| Val ELBO {va['total']:.2f} (BCE {va['bce']:.2f}, KL {va['kl']:.2f})\")\n",
        "\n",
        "    # Recon grid\n",
        "    make_recon_grid(model, val_loader, os.path.join(cfg.out_dir, f\"recon_{model_name}.png\"), n=10)\n",
        "\n",
        "    # t-SNE\n",
        "    X, Y = collect_embeddings(model, test_loader, max_n=cfg.tsne_samples)\n",
        "    tsne_plot(X, Y, f\"t-SNE ({model_name.upper()})\", os.path.join(cfg.out_dir, f\"tsne_{model_name}.png\"))\n",
        "\n",
        "    # Test metrics\n",
        "    te = run_epoch(model, test_loader, optimizer=None)\n",
        "    history[\"test_bce\"]   = [te[\"bce\"]]\n",
        "    history[\"test_kl\"]    = [te[\"kl\"]]\n",
        "    history[\"test_total\"] = [te[\"total\"]]\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save({\"model_state\": model.state_dict(),\n",
        "                \"cfg\": asdict(cfg),\n",
        "                \"history\": history},\n",
        "               os.path.join(cfg.out_dir, f\"ckpt_{model_name}.pt\"))\n",
        "\n",
        "    # tidy GPU mem between runs\n",
        "    del model; torch.cuda.empty_cache()\n",
        "    return history\n",
        "\n",
        "# -----------------------------\n",
        "# Run models sequentially (one at a time)\n",
        "# -----------------------------\n",
        "histories = {}\n",
        "for name in [\"gvae\", \"dirvae\", \"cc\"]:\n",
        "    histories[name] = run_single(name)\n",
        "\n",
        "# -----------------------------\n",
        "# Final comparison plots & table\n",
        "# -----------------------------\n",
        "epochs = range(1, cfg.epochs + 1)\n",
        "plt.figure(figsize=(12,4))\n",
        "# BCE\n",
        "plt.subplot(1,3,1)\n",
        "for k, lab in [(\"gvae\",\"GVAE\"), (\"dirvae\",\"DirVAE\"), (\"cc\",\"CC\")]:\n",
        "    plt.plot(epochs, histories[k][\"train_bce\"], label=f\"{lab} Train\")\n",
        "    plt.plot(epochs, histories[k][\"val_bce\"],   linestyle=\"--\", label=f\"{lab} Val\")\n",
        "plt.title(\"Reconstruction (BCE) ↓\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Avg per sample\"); plt.legend(fontsize=8)\n",
        "\n",
        "# KL\n",
        "plt.subplot(1,3,2)\n",
        "for k, lab in [(\"gvae\",\"GVAE\"), (\"dirvae\",\"DirVAE\"), (\"cc\",\"CC\")]:\n",
        "    plt.plot(epochs, histories[k][\"train_kl\"], label=f\"{lab} Train\")\n",
        "    plt.plot(epochs, histories[k][\"val_kl\"],   linestyle=\"--\", label=f\"{lab} Val\")\n",
        "plt.title(\"KL term ↓\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Avg per sample\"); plt.legend(fontsize=8)\n",
        "\n",
        "# Total\n",
        "plt.subplot(1,3,3)\n",
        "for k, lab in [(\"gvae\",\"GVAE\"), (\"dirvae\",\"DirVAE\"), (\"cc\",\"CC\")]:\n",
        "    plt.plot(epochs, histories[k][\"train_total\"], label=f\"{lab} Train\")\n",
        "    plt.plot(epochs, histories[k][\"val_total\"],   linestyle=\"--\", label=f\"{lab} Val\")\n",
        "plt.title(\"ELBO (BCE+KL) ↓\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Avg per sample\"); plt.legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout(); plt.savefig(os.path.join(cfg.out_dir, \"loss_curves.png\"), dpi=150); plt.show()\n",
        "\n",
        "# Tiny summary table (final test metrics)\n",
        "def last(arr): return float(arr[-1])\n",
        "summary = {\n",
        "    \"Model\": [\"GVAE\", \"DirVAE\", \"CC\"],\n",
        "    \"Test BCE\":   [last(histories[\"gvae\"][\"test_bce\"]),   last(histories[\"dirvae\"][\"test_bce\"]),   last(histories[\"cc\"][\"test_bce\"])],\n",
        "    \"Test KL\":    [last(histories[\"gvae\"][\"test_kl\"]),    last(histories[\"dirvae\"][\"test_kl\"]),    last(histories[\"cc\"][\"test_kl\"])],\n",
        "    \"Test ELBO\":  [last(histories[\"gvae\"][\"test_total\"]), last(histories[\"dirvae\"][\"test_total\"]), last(histories[\"cc\"][\"test_total\"])],\n",
        "}\n",
        "df = pd.DataFrame(summary)\n",
        "print(\"\\nFinal test metrics (avg per sample):\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nSaved files in {cfg.out_dir}:\")\n",
        "for name in [\"recon_gvae.png\", \"recon_dirvae.png\", \"recon_cc.png\",\n",
        "             \"tsne_gvae.png\", \"tsne_dirvae.png\", \"tsne_cc.png\", \"loss_curves.png\",\n",
        "             \"ckpt_gvae.pt\", \"ckpt_dirvae.pt\", \"ckpt_cc.pt\"]:\n",
        "    print(\" -\", os.path.join(cfg.out_dir, name))\n"
      ]
    }
  ]
}