{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c60963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dirichlet Variational Auto-Encoder (Dir-VAE) implementation for MNIST\n",
    "Based on \"Autoencodeing Variational Inference for Topic Model\" (ICLR2017)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4047596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for Dir-VAE training\"\"\"\n",
    "\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-3\n",
    "    no_cuda: bool = False\n",
    "    seed: int = 10\n",
    "    log_interval: int = 2\n",
    "    category: int = 10  # Number of latent categories (K)\n",
    "    alpha: float = 0.3  # Dirichlet hyperparameter\n",
    "    data_dir: str = \"./data\"\n",
    "    output_dir: str = \"./image\"\n",
    "\n",
    "    # Network architecture parameters\n",
    "    encoder_channels: int = 64\n",
    "    decoder_channels: int = 64\n",
    "    input_channels: int = 1\n",
    "    latent_dim: int = 1024\n",
    "    hidden_dim: int = 512\n",
    "\n",
    "\n",
    "def create_argument_parser() -> argparse.ArgumentParser:\n",
    "    \"\"\"Create and configure argument parser\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Dir-VAE MNIST Example\")\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=256,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 256)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning-rate\",\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help=\"learning rate (default: 1e-3)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-cuda\", action=\"store_true\", default=False, help=\"disable CUDA training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=10, metavar=\"S\", help=\"random seed (default: 10)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log-interval\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many batches to wait before logging training status\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--category\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"K\",\n",
    "        help=\"the number of categories in the dataset\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"Dirichlet hyperparameter alpha (default: 0.3)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"./data\",\n",
    "        help=\"directory for dataset (default: ./data)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        default=\"./image\",\n",
    "        help=\"directory for output images (default: ./image)\",\n",
    "    )\n",
    "    return parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180db517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_device_and_seed(config: Config) -> torch.device:\n",
    "    \"\"\"Setup device and random seeds\"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(config.seed)\n",
    "\n",
    "    # Determine device\n",
    "    use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def create_data_loaders(\n",
    "    config: Config,\n",
    ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    \"\"\"Create train and test data loaders\"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(config.data_dir, exist_ok=True)\n",
    "\n",
    "    # Data loader kwargs\n",
    "    kwargs = (\n",
    "        {\"num_workers\": 1, \"pin_memory\": True}\n",
    "        if not config.no_cuda and torch.cuda.is_available()\n",
    "        else {}\n",
    "    )\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            config.data_dir, train=True, download=True, transform=transforms.ToTensor()\n",
    "        ),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(config.data_dir, train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def compute_dirichlet_prior(\n",
    "    K: int, alpha: float, device: torch.device\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute Dirichlet prior parameters using Laplace approximation.\n",
    "\n",
    "    Args:\n",
    "        K: Number of categories\n",
    "        alpha: Dirichlet hyperparameter\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (mean, variance) tensors for the approximated normal distribution\n",
    "    \"\"\"\n",
    "    # Laplace approximation to convert Dirichlet to multivariate normal\n",
    "    a = torch.full((1, K), alpha, dtype=torch.float, device=device)\n",
    "    mean = a.log().t() - a.log().mean(1, keepdim=True)\n",
    "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K**2) * a.reciprocal().sum(\n",
    "        1, keepdim=True\n",
    "    )\n",
    "    return mean.t(), var.t()\n",
    "\n",
    "\n",
    "class DirVAEEncoder(nn.Module):\n",
    "    \"\"\"Encoder part of Dir-VAE\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super(DirVAEEncoder, self).__init__()\n",
    "        ndf = config.encoder_channels\n",
    "        nc = config.input_channels\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # input: (nc) x 28 x 28\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state: (ndf) x 14 x 14\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state: (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state: (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, config.latent_dim, 4, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(config.latent_dim, config.hidden_dim)\n",
    "        self.fc_mu = nn.Linear(config.hidden_dim, config.category)\n",
    "        self.fc_logvar = nn.Linear(config.hidden_dim, config.category)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        conv_out = self.conv_layers(x)\n",
    "        h1 = self.fc1(conv_out.view(conv_out.size(0), -1))\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class DirVAEDecoder(nn.Module):\n",
    "    \"\"\"Decoder part of Dir-VAE\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super(DirVAEDecoder, self).__init__()\n",
    "        ngf = config.decoder_channels\n",
    "        nc = config.input_channels\n",
    "\n",
    "        self.fc_decode = nn.Linear(config.category, config.hidden_dim)\n",
    "        self.fc_deconv = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            # input: latent_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(config.latent_dim, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state: (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state: (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state: (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "            # output: (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply softmax to satisfy simplex constraint (Dirichlet distribution)\n",
    "        dir_z = F.softmax(gauss_z, dim=1)\n",
    "\n",
    "        h3 = self.relu(self.fc_decode(dir_z))\n",
    "        deconv_input = self.fc_deconv(h3)\n",
    "        deconv_input = deconv_input.view(-1, deconv_input.size(1), 1, 1)\n",
    "\n",
    "        return self.deconv_layers(deconv_input)\n",
    "\n",
    "\n",
    "class DirVAE(nn.Module):\n",
    "    \"\"\"Dirichlet Variational Auto-Encoder\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, device: torch.device):\n",
    "        super(DirVAE, self).__init__()\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize encoder and decoder\n",
    "        self.encoder = DirVAEEncoder(config)\n",
    "        self.decoder = DirVAEDecoder(config)\n",
    "\n",
    "        # Setup Dirichlet prior\n",
    "        self._setup_dirichlet_prior()\n",
    "\n",
    "    def _setup_dirichlet_prior(self):\n",
    "        \"\"\"Setup Dirichlet prior parameters\"\"\"\n",
    "        prior_mean, prior_var = compute_dirichlet_prior(\n",
    "            self.config.category, self.config.alpha, self.device\n",
    "        )\n",
    "        self.register_buffer(\"prior_mean\", prior_mean)\n",
    "        self.register_buffer(\"prior_var\", prior_var)\n",
    "        self.register_buffer(\"prior_logvar\", prior_var.log())\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Encode input to latent parameters\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, gauss_z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode latent variables to reconstruction\"\"\"\n",
    "        return self.decoder(gauss_z)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Reparameterization trick for backpropagation through stochastic nodes\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        gauss_z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # gauss_z follows multivariate normal distribution\n",
    "        # Applying softmax gives us Dirichlet-distributed variables\n",
    "        dir_z = F.softmax(gauss_z, dim=1)\n",
    "        recon_x = self.decode(gauss_z)\n",
    "\n",
    "        return recon_x, mu, logvar, gauss_z, dir_z\n",
    "\n",
    "    def loss_function(\n",
    "        self,\n",
    "        recon_x: torch.Tensor,\n",
    "        x: torch.Tensor,\n",
    "        mu: torch.Tensor,\n",
    "        logvar: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss function: reconstruction loss + KL divergence\n",
    "\n",
    "        Args:\n",
    "            recon_x: Reconstructed images\n",
    "            x: Original images\n",
    "            mu: Mean of latent distribution\n",
    "            logvar: Log variance of latent distribution\n",
    "\n",
    "        Returns:\n",
    "            Total loss (sum over batch)\n",
    "        \"\"\"\n",
    "        # Reconstruction loss (Binary Cross Entropy)\n",
    "        BCE = F.binary_cross_entropy(\n",
    "            recon_x.view(-1, 784), x.view(-1, 784), reduction=\"sum\"\n",
    "        )\n",
    "\n",
    "        # KL divergence between Dirichlet prior and variational posterior\n",
    "        # Based on the original paper: \"Autoencodeing variational inference for topic model\"\n",
    "        prior_mean = self.prior_mean.expand_as(mu)\n",
    "        prior_var = self.prior_var.expand_as(logvar)\n",
    "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
    "\n",
    "        var_division = logvar.exp() / prior_var  # Σ_0 / Σ_1\n",
    "        diff = mu - prior_mean  # μ_1 - μ_0\n",
    "        diff_term = diff * diff / prior_var  # (μ_1 - μ_0)² / Σ_1\n",
    "        logvar_division = prior_logvar - logvar  # log|Σ_1| - log|Σ_0|\n",
    "\n",
    "        # KL divergence\n",
    "        KLD = 0.5 * (\n",
    "            var_division + diff_term + logvar_division - self.config.category\n",
    "        ).sum(dim=1)\n",
    "\n",
    "        return BCE + KLD.sum()\n",
    "\n",
    "\n",
    "class DirVAETrainer:\n",
    "    \"\"\"Trainer class for Dir-VAE\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: DirVAE,\n",
    "        config: Config,\n",
    "        device: torch.device,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        test_loader: torch.utils.data.DataLoader,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        # Create output directory\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    def train_epoch(self, epoch: int) -> float:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
    "            data = data.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar, gauss_z, dir_z = self.model(data)\n",
    "            loss = self.model.loss_function(recon_batch, data, mu, logvar)\n",
    "\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch_idx % self.config.log_interval == 0:\n",
    "                print(\n",
    "                    f\"Train Epoch: {epoch} [{batch_idx * len(data):5d}/\"\n",
    "                    f\"{len(self.train_loader.dataset)} \"\n",
    "                    f\"({100. * batch_idx / len(self.train_loader):3.0f}%)]\"\n",
    "                    f\"\\tLoss: {loss.item() / len(data):.6f}\"\n",
    "                )\n",
    "\n",
    "        avg_loss = train_loss / len(self.train_loader.dataset)\n",
    "        print(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "\n",
    "    def test_epoch(self, epoch: int) -> float:\n",
    "        \"\"\"Test for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, _) in enumerate(self.test_loader):\n",
    "                data = data.to(self.device)\n",
    "                recon_batch, mu, logvar, gauss_z, dir_z = self.model(data)\n",
    "                loss = self.model.loss_function(recon_batch, data, mu, logvar)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                if i == 0:\n",
    "                    n = min(data.size(0), 18)\n",
    "                    # Properly reshape for comparison\n",
    "                    comparison = torch.cat(\n",
    "                        [data[:n], recon_batch.view(data.size(0), 1, 28, 28)[:n]]\n",
    "                    )\n",
    "                    save_image(\n",
    "                        comparison.cpu(),\n",
    "                        os.path.join(self.config.output_dir, f\"recon_{epoch}.png\"),\n",
    "                        nrow=n,\n",
    "                    )\n",
    "\n",
    "        avg_loss = test_loss / len(self.test_loader.dataset)\n",
    "        print(f\"====> Test set loss: {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "\n",
    "    def generate_samples(self, epoch: int, num_samples: int = 64):\n",
    "        \"\"\"Generate samples from the model\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Sample from latent space\n",
    "            sample = torch.randn(num_samples, self.config.category).to(self.device)\n",
    "            sample = self.model.decode(sample).cpu()\n",
    "            save_image(\n",
    "                sample.view(num_samples, 1, 28, 28),\n",
    "                os.path.join(self.config.output_dir, f\"sample_{epoch}.png\"),\n",
    "            )\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Configuration: {self.config}\")\n",
    "\n",
    "        for epoch in range(1, self.config.epochs + 1):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            test_loss = self.test_epoch(epoch)\n",
    "            self.generate_samples(epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6fc2546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.37MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 302kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.21MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 571kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 15784468 trainable parameters\n",
      "Starting training...\n",
      "Configuration: Config(batch_size=256, epochs=10, learning_rate=0.001, no_cuda=False, seed=10, log_interval=2, category=10, alpha=0.3, data_dir='./data', output_dir='./image', encoder_channels=64, decoder_channels=64, input_channels=1, latent_dim=1024, hidden_dim=512)\n",
      "Train Epoch: 1 [    0/60000 (  0%)]\tLoss: 630.061096\n",
      "Train Epoch: 1 [  512/60000 (  1%)]\tLoss: 591.744568\n",
      "Train Epoch: 1 [ 1024/60000 (  2%)]\tLoss: 274.623688\n",
      "Train Epoch: 1 [ 1536/60000 (  3%)]\tLoss: 226.805923\n",
      "Train Epoch: 1 [ 2048/60000 (  3%)]\tLoss: 207.053741\n",
      "Train Epoch: 1 [ 2560/60000 (  4%)]\tLoss: 187.743301\n",
      "Train Epoch: 1 [ 3072/60000 (  5%)]\tLoss: 184.579910\n",
      "Train Epoch: 1 [ 3584/60000 (  6%)]\tLoss: 174.551437\n",
      "Train Epoch: 1 [ 4096/60000 (  7%)]\tLoss: 176.807678\n",
      "Train Epoch: 1 [ 4608/60000 (  8%)]\tLoss: 165.662674\n",
      "Train Epoch: 1 [ 5120/60000 (  9%)]\tLoss: 165.263458\n",
      "Train Epoch: 1 [ 5632/60000 (  9%)]\tLoss: 162.259918\n",
      "Train Epoch: 1 [ 6144/60000 ( 10%)]\tLoss: 169.351501\n",
      "Train Epoch: 1 [ 6656/60000 ( 11%)]\tLoss: 165.018906\n",
      "Train Epoch: 1 [ 7168/60000 ( 12%)]\tLoss: 157.475418\n",
      "Train Epoch: 1 [ 7680/60000 ( 13%)]\tLoss: 161.533905\n",
      "Train Epoch: 1 [ 8192/60000 ( 14%)]\tLoss: 165.934280\n",
      "Train Epoch: 1 [ 8704/60000 ( 14%)]\tLoss: 161.867981\n",
      "Train Epoch: 1 [ 9216/60000 ( 15%)]\tLoss: 165.194214\n",
      "Train Epoch: 1 [ 9728/60000 ( 16%)]\tLoss: 160.342758\n",
      "Train Epoch: 1 [10240/60000 ( 17%)]\tLoss: 164.545135\n",
      "Train Epoch: 1 [10752/60000 ( 18%)]\tLoss: 165.262589\n",
      "Train Epoch: 1 [11264/60000 ( 19%)]\tLoss: 160.280579\n",
      "Train Epoch: 1 [11776/60000 ( 20%)]\tLoss: 161.139847\n",
      "Train Epoch: 1 [12288/60000 ( 20%)]\tLoss: 161.651215\n",
      "Train Epoch: 1 [12800/60000 ( 21%)]\tLoss: 160.478104\n",
      "Train Epoch: 1 [13312/60000 ( 22%)]\tLoss: 162.798813\n",
      "Train Epoch: 1 [13824/60000 ( 23%)]\tLoss: 160.914337\n",
      "Train Epoch: 1 [14336/60000 ( 24%)]\tLoss: 159.549759\n",
      "Train Epoch: 1 [14848/60000 ( 25%)]\tLoss: 159.436707\n",
      "Train Epoch: 1 [15360/60000 ( 26%)]\tLoss: 160.053406\n",
      "Train Epoch: 1 [15872/60000 ( 26%)]\tLoss: 159.586380\n",
      "Train Epoch: 1 [16384/60000 ( 27%)]\tLoss: 153.764511\n",
      "Train Epoch: 1 [16896/60000 ( 28%)]\tLoss: 159.905289\n",
      "Train Epoch: 1 [17408/60000 ( 29%)]\tLoss: 156.703766\n",
      "Train Epoch: 1 [17920/60000 ( 30%)]\tLoss: 152.806442\n",
      "Train Epoch: 1 [18432/60000 ( 31%)]\tLoss: 157.414871\n",
      "Train Epoch: 1 [18944/60000 ( 31%)]\tLoss: 158.387680\n",
      "Train Epoch: 1 [19456/60000 ( 32%)]\tLoss: 152.053604\n",
      "Train Epoch: 1 [19968/60000 ( 33%)]\tLoss: 157.762390\n",
      "Train Epoch: 1 [20480/60000 ( 34%)]\tLoss: 151.203644\n",
      "Train Epoch: 1 [20992/60000 ( 35%)]\tLoss: 150.368774\n",
      "Train Epoch: 1 [21504/60000 ( 36%)]\tLoss: 152.919891\n",
      "Train Epoch: 1 [22016/60000 ( 37%)]\tLoss: 156.887115\n",
      "Train Epoch: 1 [22528/60000 ( 37%)]\tLoss: 149.936218\n",
      "Train Epoch: 1 [23040/60000 ( 38%)]\tLoss: 148.579803\n",
      "Train Epoch: 1 [23552/60000 ( 39%)]\tLoss: 149.516113\n",
      "Train Epoch: 1 [24064/60000 ( 40%)]\tLoss: 155.607330\n",
      "Train Epoch: 1 [24576/60000 ( 41%)]\tLoss: 155.955429\n",
      "Train Epoch: 1 [25088/60000 ( 42%)]\tLoss: 151.198975\n",
      "Train Epoch: 1 [25600/60000 ( 43%)]\tLoss: 154.664597\n",
      "Train Epoch: 1 [26112/60000 ( 43%)]\tLoss: 147.336456\n",
      "Train Epoch: 1 [26624/60000 ( 44%)]\tLoss: 151.357132\n",
      "Train Epoch: 1 [27136/60000 ( 45%)]\tLoss: 146.008148\n",
      "Train Epoch: 1 [27648/60000 ( 46%)]\tLoss: 151.053818\n",
      "Train Epoch: 1 [28160/60000 ( 47%)]\tLoss: 148.343597\n",
      "Train Epoch: 1 [28672/60000 ( 48%)]\tLoss: 152.849609\n",
      "Train Epoch: 1 [29184/60000 ( 49%)]\tLoss: 149.169388\n",
      "Train Epoch: 1 [29696/60000 ( 49%)]\tLoss: 140.705750\n",
      "Train Epoch: 1 [30208/60000 ( 50%)]\tLoss: 145.535080\n",
      "Train Epoch: 1 [30720/60000 ( 51%)]\tLoss: 148.305573\n",
      "Train Epoch: 1 [31232/60000 ( 52%)]\tLoss: 149.305450\n",
      "Train Epoch: 1 [31744/60000 ( 53%)]\tLoss: 144.630127\n",
      "Train Epoch: 1 [32256/60000 ( 54%)]\tLoss: 147.391891\n",
      "Train Epoch: 1 [32768/60000 ( 54%)]\tLoss: 148.959137\n",
      "Train Epoch: 1 [33280/60000 ( 55%)]\tLoss: 144.786255\n",
      "Train Epoch: 1 [33792/60000 ( 56%)]\tLoss: 148.043091\n",
      "Train Epoch: 1 [34304/60000 ( 57%)]\tLoss: 146.748993\n",
      "Train Epoch: 1 [34816/60000 ( 58%)]\tLoss: 148.086151\n",
      "Train Epoch: 1 [35328/60000 ( 59%)]\tLoss: 144.283813\n",
      "Train Epoch: 1 [35840/60000 ( 60%)]\tLoss: 146.559875\n",
      "Train Epoch: 1 [36352/60000 ( 60%)]\tLoss: 148.175232\n",
      "Train Epoch: 1 [36864/60000 ( 61%)]\tLoss: 143.789322\n",
      "Train Epoch: 1 [37376/60000 ( 62%)]\tLoss: 147.840790\n",
      "Train Epoch: 1 [37888/60000 ( 63%)]\tLoss: 144.584137\n",
      "Train Epoch: 1 [38400/60000 ( 64%)]\tLoss: 144.415771\n",
      "Train Epoch: 1 [38912/60000 ( 65%)]\tLoss: 143.141922\n",
      "Train Epoch: 1 [39424/60000 ( 66%)]\tLoss: 143.528397\n",
      "Train Epoch: 1 [39936/60000 ( 66%)]\tLoss: 141.342682\n",
      "Train Epoch: 1 [40448/60000 ( 67%)]\tLoss: 143.818344\n",
      "Train Epoch: 1 [40960/60000 ( 68%)]\tLoss: 143.401917\n",
      "Train Epoch: 1 [41472/60000 ( 69%)]\tLoss: 140.268295\n",
      "Train Epoch: 1 [41984/60000 ( 70%)]\tLoss: 141.908081\n",
      "Train Epoch: 1 [42496/60000 ( 71%)]\tLoss: 140.898376\n",
      "Train Epoch: 1 [43008/60000 ( 71%)]\tLoss: 145.485504\n",
      "Train Epoch: 1 [43520/60000 ( 72%)]\tLoss: 144.476135\n",
      "Train Epoch: 1 [44032/60000 ( 73%)]\tLoss: 141.590622\n",
      "Train Epoch: 1 [44544/60000 ( 74%)]\tLoss: 139.327271\n",
      "Train Epoch: 1 [45056/60000 ( 75%)]\tLoss: 136.440323\n",
      "Train Epoch: 1 [45568/60000 ( 76%)]\tLoss: 134.455292\n",
      "Train Epoch: 1 [46080/60000 ( 77%)]\tLoss: 139.280914\n",
      "Train Epoch: 1 [46592/60000 ( 77%)]\tLoss: 138.252502\n",
      "Train Epoch: 1 [47104/60000 ( 78%)]\tLoss: 139.882172\n",
      "Train Epoch: 1 [47616/60000 ( 79%)]\tLoss: 140.621826\n",
      "Train Epoch: 1 [48128/60000 ( 80%)]\tLoss: 137.046768\n",
      "Train Epoch: 1 [48640/60000 ( 81%)]\tLoss: 133.863708\n",
      "Train Epoch: 1 [49152/60000 ( 82%)]\tLoss: 134.934570\n",
      "Train Epoch: 1 [49664/60000 ( 83%)]\tLoss: 143.160889\n",
      "Train Epoch: 1 [50176/60000 ( 83%)]\tLoss: 142.123764\n",
      "Train Epoch: 1 [50688/60000 ( 84%)]\tLoss: 136.685013\n",
      "Train Epoch: 1 [51200/60000 ( 85%)]\tLoss: 139.124146\n",
      "Train Epoch: 1 [51712/60000 ( 86%)]\tLoss: 134.746979\n",
      "Train Epoch: 1 [52224/60000 ( 87%)]\tLoss: 133.949463\n",
      "Train Epoch: 1 [52736/60000 ( 88%)]\tLoss: 138.467804\n",
      "Train Epoch: 1 [53248/60000 ( 89%)]\tLoss: 134.778168\n",
      "Train Epoch: 1 [53760/60000 ( 89%)]\tLoss: 131.960022\n",
      "Train Epoch: 1 [54272/60000 ( 90%)]\tLoss: 135.443359\n",
      "Train Epoch: 1 [54784/60000 ( 91%)]\tLoss: 131.829071\n",
      "Train Epoch: 1 [55296/60000 ( 92%)]\tLoss: 130.470398\n",
      "Train Epoch: 1 [55808/60000 ( 93%)]\tLoss: 132.475266\n",
      "Train Epoch: 1 [56320/60000 ( 94%)]\tLoss: 126.075142\n",
      "Train Epoch: 1 [56832/60000 ( 94%)]\tLoss: 136.129395\n",
      "Train Epoch: 1 [57344/60000 ( 95%)]\tLoss: 127.493774\n",
      "Train Epoch: 1 [57856/60000 ( 96%)]\tLoss: 126.880745\n",
      "Train Epoch: 1 [58368/60000 ( 97%)]\tLoss: 125.582222\n",
      "Train Epoch: 1 [58880/60000 ( 98%)]\tLoss: 127.731140\n",
      "Train Epoch: 1 [59392/60000 ( 99%)]\tLoss: 123.777191\n",
      "Train Epoch: 1 [22464/60000 (100%)]\tLoss: 128.140696\n",
      "====> Epoch: 1 Average loss: 160.5861\n",
      "====> Test set loss: 127.4808\n",
      "Train Epoch: 2 [    0/60000 (  0%)]\tLoss: 124.123047\n",
      "Train Epoch: 2 [  512/60000 (  1%)]\tLoss: 128.447739\n",
      "Train Epoch: 2 [ 1024/60000 (  2%)]\tLoss: 126.604431\n",
      "Train Epoch: 2 [ 1536/60000 (  3%)]\tLoss: 124.226135\n",
      "Train Epoch: 2 [ 2048/60000 (  3%)]\tLoss: 126.851349\n",
      "Train Epoch: 2 [ 2560/60000 (  4%)]\tLoss: 121.944160\n",
      "Train Epoch: 2 [ 3072/60000 (  5%)]\tLoss: 125.197014\n",
      "Train Epoch: 2 [ 3584/60000 (  6%)]\tLoss: 123.636871\n",
      "Train Epoch: 2 [ 4096/60000 (  7%)]\tLoss: 122.887100\n",
      "Train Epoch: 2 [ 4608/60000 (  8%)]\tLoss: 125.186203\n",
      "Train Epoch: 2 [ 5120/60000 (  9%)]\tLoss: 122.496292\n",
      "Train Epoch: 2 [ 5632/60000 (  9%)]\tLoss: 114.460075\n",
      "Train Epoch: 2 [ 6144/60000 ( 10%)]\tLoss: 117.393196\n",
      "Train Epoch: 2 [ 6656/60000 ( 11%)]\tLoss: 123.912575\n",
      "Train Epoch: 2 [ 7168/60000 ( 12%)]\tLoss: 122.113220\n",
      "Train Epoch: 2 [ 7680/60000 ( 13%)]\tLoss: 114.458267\n",
      "Train Epoch: 2 [ 8192/60000 ( 14%)]\tLoss: 123.600494\n",
      "Train Epoch: 2 [ 8704/60000 ( 14%)]\tLoss: 121.848854\n",
      "Train Epoch: 2 [ 9216/60000 ( 15%)]\tLoss: 122.174927\n",
      "Train Epoch: 2 [ 9728/60000 ( 16%)]\tLoss: 116.841019\n",
      "Train Epoch: 2 [10240/60000 ( 17%)]\tLoss: 115.953171\n",
      "Train Epoch: 2 [10752/60000 ( 18%)]\tLoss: 117.572433\n",
      "Train Epoch: 2 [11264/60000 ( 19%)]\tLoss: 116.437843\n",
      "Train Epoch: 2 [11776/60000 ( 20%)]\tLoss: 116.072495\n",
      "Train Epoch: 2 [12288/60000 ( 20%)]\tLoss: 119.766113\n",
      "Train Epoch: 2 [12800/60000 ( 21%)]\tLoss: 115.401184\n",
      "Train Epoch: 2 [13312/60000 ( 22%)]\tLoss: 115.719307\n",
      "Train Epoch: 2 [13824/60000 ( 23%)]\tLoss: 114.002121\n",
      "Train Epoch: 2 [14336/60000 ( 24%)]\tLoss: 115.471466\n",
      "Train Epoch: 2 [14848/60000 ( 25%)]\tLoss: 121.318207\n",
      "Train Epoch: 2 [15360/60000 ( 26%)]\tLoss: 112.635849\n",
      "Train Epoch: 2 [15872/60000 ( 26%)]\tLoss: 117.987976\n",
      "Train Epoch: 2 [16384/60000 ( 27%)]\tLoss: 108.507767\n",
      "Train Epoch: 2 [16896/60000 ( 28%)]\tLoss: 113.380966\n",
      "Train Epoch: 2 [17408/60000 ( 29%)]\tLoss: 108.754242\n",
      "Train Epoch: 2 [17920/60000 ( 30%)]\tLoss: 110.190598\n",
      "Train Epoch: 2 [18432/60000 ( 31%)]\tLoss: 109.948418\n",
      "Train Epoch: 2 [18944/60000 ( 31%)]\tLoss: 110.368469\n",
      "Train Epoch: 2 [19456/60000 ( 32%)]\tLoss: 105.066589\n",
      "Train Epoch: 2 [19968/60000 ( 33%)]\tLoss: 108.643066\n",
      "Train Epoch: 2 [20480/60000 ( 34%)]\tLoss: 108.730072\n",
      "Train Epoch: 2 [20992/60000 ( 35%)]\tLoss: 107.615005\n",
      "Train Epoch: 2 [21504/60000 ( 36%)]\tLoss: 109.612251\n",
      "Train Epoch: 2 [22016/60000 ( 37%)]\tLoss: 106.028656\n",
      "Train Epoch: 2 [22528/60000 ( 37%)]\tLoss: 106.924057\n",
      "Train Epoch: 2 [23040/60000 ( 38%)]\tLoss: 107.880203\n",
      "Train Epoch: 2 [23552/60000 ( 39%)]\tLoss: 105.471741\n",
      "Train Epoch: 2 [24064/60000 ( 40%)]\tLoss: 108.959244\n",
      "Train Epoch: 2 [24576/60000 ( 41%)]\tLoss: 107.415909\n",
      "Train Epoch: 2 [25088/60000 ( 42%)]\tLoss: 108.480453\n",
      "Train Epoch: 2 [25600/60000 ( 43%)]\tLoss: 104.090492\n",
      "Train Epoch: 2 [26112/60000 ( 43%)]\tLoss: 103.452637\n",
      "Train Epoch: 2 [26624/60000 ( 44%)]\tLoss: 107.099899\n",
      "Train Epoch: 2 [27136/60000 ( 45%)]\tLoss: 103.496017\n",
      "Train Epoch: 2 [27648/60000 ( 46%)]\tLoss: 104.522995\n",
      "Train Epoch: 2 [28160/60000 ( 47%)]\tLoss: 103.696198\n",
      "Train Epoch: 2 [28672/60000 ( 48%)]\tLoss: 105.647400\n",
      "Train Epoch: 2 [29184/60000 ( 49%)]\tLoss: 108.130081\n",
      "Train Epoch: 2 [29696/60000 ( 49%)]\tLoss: 102.350578\n",
      "Train Epoch: 2 [30208/60000 ( 50%)]\tLoss: 102.909805\n",
      "Train Epoch: 2 [30720/60000 ( 51%)]\tLoss: 102.208488\n",
      "Train Epoch: 2 [31232/60000 ( 52%)]\tLoss: 103.357651\n",
      "Train Epoch: 2 [31744/60000 ( 53%)]\tLoss: 99.775970\n",
      "Train Epoch: 2 [32256/60000 ( 54%)]\tLoss: 102.425995\n",
      "Train Epoch: 2 [32768/60000 ( 54%)]\tLoss: 98.403236\n",
      "Train Epoch: 2 [33280/60000 ( 55%)]\tLoss: 101.410126\n",
      "Train Epoch: 2 [33792/60000 ( 56%)]\tLoss: 96.301369\n",
      "Train Epoch: 2 [34304/60000 ( 57%)]\tLoss: 103.745972\n",
      "Train Epoch: 2 [34816/60000 ( 58%)]\tLoss: 96.686562\n",
      "Train Epoch: 2 [35328/60000 ( 59%)]\tLoss: 105.033302\n",
      "Train Epoch: 2 [35840/60000 ( 60%)]\tLoss: 100.718941\n",
      "Train Epoch: 2 [36352/60000 ( 60%)]\tLoss: 101.491035\n",
      "Train Epoch: 2 [36864/60000 ( 61%)]\tLoss: 100.241425\n",
      "Train Epoch: 2 [37376/60000 ( 62%)]\tLoss: 103.956345\n",
      "Train Epoch: 2 [37888/60000 ( 63%)]\tLoss: 103.041008\n",
      "Train Epoch: 2 [38400/60000 ( 64%)]\tLoss: 96.899055\n",
      "Train Epoch: 2 [38912/60000 ( 65%)]\tLoss: 99.898018\n",
      "Train Epoch: 2 [39424/60000 ( 66%)]\tLoss: 95.757370\n",
      "Train Epoch: 2 [39936/60000 ( 66%)]\tLoss: 99.917267\n",
      "Train Epoch: 2 [40448/60000 ( 67%)]\tLoss: 97.044754\n",
      "Train Epoch: 2 [40960/60000 ( 68%)]\tLoss: 101.417595\n",
      "Train Epoch: 2 [41472/60000 ( 69%)]\tLoss: 96.938080\n",
      "Train Epoch: 2 [41984/60000 ( 70%)]\tLoss: 102.072220\n",
      "Train Epoch: 2 [42496/60000 ( 71%)]\tLoss: 99.655167\n",
      "Train Epoch: 2 [43008/60000 ( 71%)]\tLoss: 100.225960\n",
      "Train Epoch: 2 [43520/60000 ( 72%)]\tLoss: 101.682198\n",
      "Train Epoch: 2 [44032/60000 ( 73%)]\tLoss: 93.701408\n",
      "Train Epoch: 2 [44544/60000 ( 74%)]\tLoss: 94.376236\n",
      "Train Epoch: 2 [45056/60000 ( 75%)]\tLoss: 94.397011\n",
      "Train Epoch: 2 [45568/60000 ( 76%)]\tLoss: 94.202209\n",
      "Train Epoch: 2 [46080/60000 ( 77%)]\tLoss: 98.278732\n",
      "Train Epoch: 2 [46592/60000 ( 77%)]\tLoss: 101.051552\n",
      "Train Epoch: 2 [47104/60000 ( 78%)]\tLoss: 95.410149\n",
      "Train Epoch: 2 [47616/60000 ( 79%)]\tLoss: 95.100609\n",
      "Train Epoch: 2 [48128/60000 ( 80%)]\tLoss: 89.128883\n",
      "Train Epoch: 2 [48640/60000 ( 81%)]\tLoss: 92.731400\n",
      "Train Epoch: 2 [49152/60000 ( 82%)]\tLoss: 91.962952\n",
      "Train Epoch: 2 [49664/60000 ( 83%)]\tLoss: 92.158676\n",
      "Train Epoch: 2 [50176/60000 ( 83%)]\tLoss: 92.947487\n",
      "Train Epoch: 2 [50688/60000 ( 84%)]\tLoss: 94.344597\n",
      "Train Epoch: 2 [51200/60000 ( 85%)]\tLoss: 93.946869\n",
      "Train Epoch: 2 [51712/60000 ( 86%)]\tLoss: 92.908478\n",
      "Train Epoch: 2 [52224/60000 ( 87%)]\tLoss: 88.458008\n",
      "Train Epoch: 2 [52736/60000 ( 88%)]\tLoss: 91.401550\n",
      "Train Epoch: 2 [53248/60000 ( 89%)]\tLoss: 92.206863\n",
      "Train Epoch: 2 [53760/60000 ( 89%)]\tLoss: 91.584976\n",
      "Train Epoch: 2 [54272/60000 ( 90%)]\tLoss: 93.737976\n",
      "Train Epoch: 2 [54784/60000 ( 91%)]\tLoss: 84.306480\n",
      "Train Epoch: 2 [55296/60000 ( 92%)]\tLoss: 90.850990\n",
      "Train Epoch: 2 [55808/60000 ( 93%)]\tLoss: 89.393204\n",
      "Train Epoch: 2 [56320/60000 ( 94%)]\tLoss: 92.596733\n",
      "Train Epoch: 2 [56832/60000 ( 94%)]\tLoss: 89.768745\n",
      "Train Epoch: 2 [57344/60000 ( 95%)]\tLoss: 92.812462\n",
      "Train Epoch: 2 [57856/60000 ( 96%)]\tLoss: 91.977730\n",
      "Train Epoch: 2 [58368/60000 ( 97%)]\tLoss: 86.937637\n",
      "Train Epoch: 2 [58880/60000 ( 98%)]\tLoss: 89.598053\n",
      "Train Epoch: 2 [59392/60000 ( 99%)]\tLoss: 90.318634\n",
      "Train Epoch: 2 [22464/60000 (100%)]\tLoss: 90.147196\n",
      "====> Epoch: 2 Average loss: 105.4204\n",
      "====> Test set loss: 93.5191\n",
      "Train Epoch: 3 [    0/60000 (  0%)]\tLoss: 92.701996\n",
      "Train Epoch: 3 [  512/60000 (  1%)]\tLoss: 92.948578\n",
      "Train Epoch: 3 [ 1024/60000 (  2%)]\tLoss: 92.444145\n",
      "Train Epoch: 3 [ 1536/60000 (  3%)]\tLoss: 87.446129\n",
      "Train Epoch: 3 [ 2048/60000 (  3%)]\tLoss: 94.881096\n",
      "Train Epoch: 3 [ 2560/60000 (  4%)]\tLoss: 89.924934\n",
      "Train Epoch: 3 [ 3072/60000 (  5%)]\tLoss: 94.341774\n",
      "Train Epoch: 3 [ 3584/60000 (  6%)]\tLoss: 96.323700\n",
      "Train Epoch: 3 [ 4096/60000 (  7%)]\tLoss: 93.780167\n",
      "Train Epoch: 3 [ 4608/60000 (  8%)]\tLoss: 92.243225\n",
      "Train Epoch: 3 [ 5120/60000 (  9%)]\tLoss: 86.249481\n",
      "Train Epoch: 3 [ 5632/60000 (  9%)]\tLoss: 82.356422\n",
      "Train Epoch: 3 [ 6144/60000 ( 10%)]\tLoss: 90.436287\n",
      "Train Epoch: 3 [ 6656/60000 ( 11%)]\tLoss: 86.622025\n",
      "Train Epoch: 3 [ 7168/60000 ( 12%)]\tLoss: 89.096107\n",
      "Train Epoch: 3 [ 7680/60000 ( 13%)]\tLoss: 85.318710\n",
      "Train Epoch: 3 [ 8192/60000 ( 14%)]\tLoss: 88.932388\n",
      "Train Epoch: 3 [ 8704/60000 ( 14%)]\tLoss: 89.078041\n",
      "Train Epoch: 3 [ 9216/60000 ( 15%)]\tLoss: 89.013733\n",
      "Train Epoch: 3 [ 9728/60000 ( 16%)]\tLoss: 87.625504\n",
      "Train Epoch: 3 [10240/60000 ( 17%)]\tLoss: 88.871971\n",
      "Train Epoch: 3 [10752/60000 ( 18%)]\tLoss: 85.758636\n",
      "Train Epoch: 3 [11264/60000 ( 19%)]\tLoss: 89.000221\n",
      "Train Epoch: 3 [11776/60000 ( 20%)]\tLoss: 86.545395\n",
      "Train Epoch: 3 [12288/60000 ( 20%)]\tLoss: 85.178635\n",
      "Train Epoch: 3 [12800/60000 ( 21%)]\tLoss: 83.684708\n",
      "Train Epoch: 3 [13312/60000 ( 22%)]\tLoss: 87.844109\n",
      "Train Epoch: 3 [13824/60000 ( 23%)]\tLoss: 88.615402\n",
      "Train Epoch: 3 [14336/60000 ( 24%)]\tLoss: 84.834335\n",
      "Train Epoch: 3 [14848/60000 ( 25%)]\tLoss: 89.316818\n",
      "Train Epoch: 3 [15360/60000 ( 26%)]\tLoss: 89.758354\n",
      "Train Epoch: 3 [15872/60000 ( 26%)]\tLoss: 89.239883\n",
      "Train Epoch: 3 [16384/60000 ( 27%)]\tLoss: 87.806290\n",
      "Train Epoch: 3 [16896/60000 ( 28%)]\tLoss: 92.754646\n",
      "Train Epoch: 3 [17408/60000 ( 29%)]\tLoss: 85.763885\n",
      "Train Epoch: 3 [17920/60000 ( 30%)]\tLoss: 83.019714\n",
      "Train Epoch: 3 [18432/60000 ( 31%)]\tLoss: 82.080757\n",
      "Train Epoch: 3 [18944/60000 ( 31%)]\tLoss: 80.299698\n",
      "Train Epoch: 3 [19456/60000 ( 32%)]\tLoss: 81.259407\n",
      "Train Epoch: 3 [19968/60000 ( 33%)]\tLoss: 80.568436\n",
      "Train Epoch: 3 [20480/60000 ( 34%)]\tLoss: 86.353920\n",
      "Train Epoch: 3 [20992/60000 ( 35%)]\tLoss: 83.217514\n",
      "Train Epoch: 3 [21504/60000 ( 36%)]\tLoss: 81.889580\n",
      "Train Epoch: 3 [22016/60000 ( 37%)]\tLoss: 82.143646\n",
      "Train Epoch: 3 [22528/60000 ( 37%)]\tLoss: 83.681396\n",
      "Train Epoch: 3 [23040/60000 ( 38%)]\tLoss: 84.535706\n",
      "Train Epoch: 3 [23552/60000 ( 39%)]\tLoss: 83.187286\n",
      "Train Epoch: 3 [24064/60000 ( 40%)]\tLoss: 84.386253\n",
      "Train Epoch: 3 [24576/60000 ( 41%)]\tLoss: 81.982101\n",
      "Train Epoch: 3 [25088/60000 ( 42%)]\tLoss: 82.662918\n",
      "Train Epoch: 3 [25600/60000 ( 43%)]\tLoss: 82.020386\n",
      "Train Epoch: 3 [26112/60000 ( 43%)]\tLoss: 84.497581\n",
      "Train Epoch: 3 [26624/60000 ( 44%)]\tLoss: 81.594421\n",
      "Train Epoch: 3 [27136/60000 ( 45%)]\tLoss: 80.379211\n",
      "Train Epoch: 3 [27648/60000 ( 46%)]\tLoss: 79.935379\n",
      "Train Epoch: 3 [28160/60000 ( 47%)]\tLoss: 85.511497\n",
      "Train Epoch: 3 [28672/60000 ( 48%)]\tLoss: 81.232964\n",
      "Train Epoch: 3 [29184/60000 ( 49%)]\tLoss: 85.103470\n",
      "Train Epoch: 3 [29696/60000 ( 49%)]\tLoss: 84.374054\n",
      "Train Epoch: 3 [30208/60000 ( 50%)]\tLoss: 85.595558\n",
      "Train Epoch: 3 [30720/60000 ( 51%)]\tLoss: 78.978134\n",
      "Train Epoch: 3 [31232/60000 ( 52%)]\tLoss: 79.846870\n",
      "Train Epoch: 3 [31744/60000 ( 53%)]\tLoss: 80.172516\n",
      "Train Epoch: 3 [32256/60000 ( 54%)]\tLoss: 80.630630\n",
      "Train Epoch: 3 [32768/60000 ( 54%)]\tLoss: 81.884407\n",
      "Train Epoch: 3 [33280/60000 ( 55%)]\tLoss: 81.894974\n",
      "Train Epoch: 3 [33792/60000 ( 56%)]\tLoss: 78.551041\n",
      "Train Epoch: 3 [34304/60000 ( 57%)]\tLoss: 82.940750\n",
      "Train Epoch: 3 [34816/60000 ( 58%)]\tLoss: 81.495384\n",
      "Train Epoch: 3 [35328/60000 ( 59%)]\tLoss: 82.511536\n",
      "Train Epoch: 3 [35840/60000 ( 60%)]\tLoss: 79.083069\n",
      "Train Epoch: 3 [36352/60000 ( 60%)]\tLoss: 76.098442\n",
      "Train Epoch: 3 [36864/60000 ( 61%)]\tLoss: 82.510689\n",
      "Train Epoch: 3 [37376/60000 ( 62%)]\tLoss: 77.656479\n",
      "Train Epoch: 3 [37888/60000 ( 63%)]\tLoss: 76.884598\n",
      "Train Epoch: 3 [38400/60000 ( 64%)]\tLoss: 81.273575\n",
      "Train Epoch: 3 [38912/60000 ( 65%)]\tLoss: 80.094749\n",
      "Train Epoch: 3 [39424/60000 ( 66%)]\tLoss: 76.301468\n",
      "Train Epoch: 3 [39936/60000 ( 66%)]\tLoss: 79.283051\n",
      "Train Epoch: 3 [40448/60000 ( 67%)]\tLoss: 73.684921\n",
      "Train Epoch: 3 [40960/60000 ( 68%)]\tLoss: 78.100174\n",
      "Train Epoch: 3 [41472/60000 ( 69%)]\tLoss: 79.139778\n",
      "Train Epoch: 3 [41984/60000 ( 70%)]\tLoss: 78.264023\n",
      "Train Epoch: 3 [42496/60000 ( 71%)]\tLoss: 76.226555\n",
      "Train Epoch: 3 [43008/60000 ( 71%)]\tLoss: 79.749557\n",
      "Train Epoch: 3 [43520/60000 ( 72%)]\tLoss: 80.622589\n",
      "Train Epoch: 3 [44032/60000 ( 73%)]\tLoss: 79.043884\n",
      "Train Epoch: 3 [44544/60000 ( 74%)]\tLoss: 76.433762\n",
      "Train Epoch: 3 [45056/60000 ( 75%)]\tLoss: 76.606026\n",
      "Train Epoch: 3 [45568/60000 ( 76%)]\tLoss: 78.351685\n",
      "Train Epoch: 3 [46080/60000 ( 77%)]\tLoss: 77.704208\n",
      "Train Epoch: 3 [46592/60000 ( 77%)]\tLoss: 78.752289\n",
      "Train Epoch: 3 [47104/60000 ( 78%)]\tLoss: 78.928215\n",
      "Train Epoch: 3 [47616/60000 ( 79%)]\tLoss: 73.599335\n",
      "Train Epoch: 3 [48128/60000 ( 80%)]\tLoss: 78.176987\n",
      "Train Epoch: 3 [48640/60000 ( 81%)]\tLoss: 78.727036\n",
      "Train Epoch: 3 [49152/60000 ( 82%)]\tLoss: 79.724106\n",
      "Train Epoch: 3 [49664/60000 ( 83%)]\tLoss: 78.454903\n",
      "Train Epoch: 3 [50176/60000 ( 83%)]\tLoss: 76.259125\n",
      "Train Epoch: 3 [50688/60000 ( 84%)]\tLoss: 75.473167\n",
      "Train Epoch: 3 [51200/60000 ( 85%)]\tLoss: 77.867462\n",
      "Train Epoch: 3 [51712/60000 ( 86%)]\tLoss: 77.629189\n",
      "Train Epoch: 3 [52224/60000 ( 87%)]\tLoss: 78.258240\n",
      "Train Epoch: 3 [52736/60000 ( 88%)]\tLoss: 78.767731\n",
      "Train Epoch: 3 [53248/60000 ( 89%)]\tLoss: 78.419655\n",
      "Train Epoch: 3 [53760/60000 ( 89%)]\tLoss: 75.730309\n",
      "Train Epoch: 3 [54272/60000 ( 90%)]\tLoss: 82.585350\n",
      "Train Epoch: 3 [54784/60000 ( 91%)]\tLoss: 80.409142\n",
      "Train Epoch: 3 [55296/60000 ( 92%)]\tLoss: 75.572273\n",
      "Train Epoch: 3 [55808/60000 ( 93%)]\tLoss: 76.463814\n",
      "Train Epoch: 3 [56320/60000 ( 94%)]\tLoss: 75.256767\n",
      "Train Epoch: 3 [56832/60000 ( 94%)]\tLoss: 77.140091\n",
      "Train Epoch: 3 [57344/60000 ( 95%)]\tLoss: 76.860519\n",
      "Train Epoch: 3 [57856/60000 ( 96%)]\tLoss: 76.248077\n",
      "Train Epoch: 3 [58368/60000 ( 97%)]\tLoss: 75.232742\n",
      "Train Epoch: 3 [58880/60000 ( 98%)]\tLoss: 73.899323\n",
      "Train Epoch: 3 [59392/60000 ( 99%)]\tLoss: 78.451950\n",
      "Train Epoch: 3 [22464/60000 (100%)]\tLoss: 77.646495\n",
      "====> Epoch: 3 Average loss: 82.1821\n",
      "====> Test set loss: 75.8010\n",
      "Train Epoch: 4 [    0/60000 (  0%)]\tLoss: 76.696808\n",
      "Train Epoch: 4 [  512/60000 (  1%)]\tLoss: 83.468155\n",
      "Train Epoch: 4 [ 1024/60000 (  2%)]\tLoss: 74.158997\n",
      "Train Epoch: 4 [ 1536/60000 (  3%)]\tLoss: 74.891739\n",
      "Train Epoch: 4 [ 2048/60000 (  3%)]\tLoss: 78.397537\n",
      "Train Epoch: 4 [ 2560/60000 (  4%)]\tLoss: 74.302193\n",
      "Train Epoch: 4 [ 3072/60000 (  5%)]\tLoss: 77.312782\n",
      "Train Epoch: 4 [ 3584/60000 (  6%)]\tLoss: 75.868301\n",
      "Train Epoch: 4 [ 4096/60000 (  7%)]\tLoss: 76.290329\n",
      "Train Epoch: 4 [ 4608/60000 (  8%)]\tLoss: 75.176971\n",
      "Train Epoch: 4 [ 5120/60000 (  9%)]\tLoss: 76.238663\n",
      "Train Epoch: 4 [ 5632/60000 (  9%)]\tLoss: 75.198502\n",
      "Train Epoch: 4 [ 6144/60000 ( 10%)]\tLoss: 69.764496\n",
      "Train Epoch: 4 [ 6656/60000 ( 11%)]\tLoss: 75.288933\n",
      "Train Epoch: 4 [ 7168/60000 ( 12%)]\tLoss: 75.675629\n",
      "Train Epoch: 4 [ 7680/60000 ( 13%)]\tLoss: 73.175430\n",
      "Train Epoch: 4 [ 8192/60000 ( 14%)]\tLoss: 77.665375\n",
      "Train Epoch: 4 [ 8704/60000 ( 14%)]\tLoss: 77.711411\n",
      "Train Epoch: 4 [ 9216/60000 ( 15%)]\tLoss: 77.880066\n",
      "Train Epoch: 4 [ 9728/60000 ( 16%)]\tLoss: 80.286476\n",
      "Train Epoch: 4 [10240/60000 ( 17%)]\tLoss: 77.212196\n",
      "Train Epoch: 4 [10752/60000 ( 18%)]\tLoss: 75.568695\n",
      "Train Epoch: 4 [11264/60000 ( 19%)]\tLoss: 71.561325\n",
      "Train Epoch: 4 [11776/60000 ( 20%)]\tLoss: 75.067612\n",
      "Train Epoch: 4 [12288/60000 ( 20%)]\tLoss: 70.878166\n",
      "Train Epoch: 4 [12800/60000 ( 21%)]\tLoss: 71.884254\n",
      "Train Epoch: 4 [13312/60000 ( 22%)]\tLoss: 77.213379\n",
      "Train Epoch: 4 [13824/60000 ( 23%)]\tLoss: 76.444290\n",
      "Train Epoch: 4 [14336/60000 ( 24%)]\tLoss: 69.804855\n",
      "Train Epoch: 4 [14848/60000 ( 25%)]\tLoss: 69.399506\n",
      "Train Epoch: 4 [15360/60000 ( 26%)]\tLoss: 79.833328\n",
      "Train Epoch: 4 [15872/60000 ( 26%)]\tLoss: 73.889473\n",
      "Train Epoch: 4 [16384/60000 ( 27%)]\tLoss: 69.269638\n",
      "Train Epoch: 4 [16896/60000 ( 28%)]\tLoss: 76.277191\n",
      "Train Epoch: 4 [17408/60000 ( 29%)]\tLoss: 70.346367\n",
      "Train Epoch: 4 [17920/60000 ( 30%)]\tLoss: 70.287117\n",
      "Train Epoch: 4 [18432/60000 ( 31%)]\tLoss: 70.334618\n",
      "Train Epoch: 4 [18944/60000 ( 31%)]\tLoss: 75.876541\n",
      "Train Epoch: 4 [19456/60000 ( 32%)]\tLoss: 71.387154\n",
      "Train Epoch: 4 [19968/60000 ( 33%)]\tLoss: 75.439102\n",
      "Train Epoch: 4 [20480/60000 ( 34%)]\tLoss: 75.186401\n",
      "Train Epoch: 4 [20992/60000 ( 35%)]\tLoss: 71.989838\n",
      "Train Epoch: 4 [21504/60000 ( 36%)]\tLoss: 73.823792\n",
      "Train Epoch: 4 [22016/60000 ( 37%)]\tLoss: 78.781502\n",
      "Train Epoch: 4 [22528/60000 ( 37%)]\tLoss: 68.917229\n",
      "Train Epoch: 4 [23040/60000 ( 38%)]\tLoss: 74.767197\n",
      "Train Epoch: 4 [23552/60000 ( 39%)]\tLoss: 70.852249\n",
      "Train Epoch: 4 [24064/60000 ( 40%)]\tLoss: 69.929062\n",
      "Train Epoch: 4 [24576/60000 ( 41%)]\tLoss: 79.674728\n",
      "Train Epoch: 4 [25088/60000 ( 42%)]\tLoss: 69.981735\n",
      "Train Epoch: 4 [25600/60000 ( 43%)]\tLoss: 72.827179\n",
      "Train Epoch: 4 [26112/60000 ( 43%)]\tLoss: 72.193352\n",
      "Train Epoch: 4 [26624/60000 ( 44%)]\tLoss: 73.299202\n",
      "Train Epoch: 4 [27136/60000 ( 45%)]\tLoss: 71.291176\n",
      "Train Epoch: 4 [27648/60000 ( 46%)]\tLoss: 76.410423\n",
      "Train Epoch: 4 [28160/60000 ( 47%)]\tLoss: 72.123863\n",
      "Train Epoch: 4 [28672/60000 ( 48%)]\tLoss: 73.025002\n",
      "Train Epoch: 4 [29184/60000 ( 49%)]\tLoss: 69.726875\n",
      "Train Epoch: 4 [29696/60000 ( 49%)]\tLoss: 71.831131\n",
      "Train Epoch: 4 [30208/60000 ( 50%)]\tLoss: 70.335999\n",
      "Train Epoch: 4 [30720/60000 ( 51%)]\tLoss: 69.994614\n",
      "Train Epoch: 4 [31232/60000 ( 52%)]\tLoss: 72.689537\n",
      "Train Epoch: 4 [31744/60000 ( 53%)]\tLoss: 69.760979\n",
      "Train Epoch: 4 [32256/60000 ( 54%)]\tLoss: 71.956131\n",
      "Train Epoch: 4 [32768/60000 ( 54%)]\tLoss: 78.238144\n",
      "Train Epoch: 4 [33280/60000 ( 55%)]\tLoss: 74.162949\n",
      "Train Epoch: 4 [33792/60000 ( 56%)]\tLoss: 67.031189\n",
      "Train Epoch: 4 [34304/60000 ( 57%)]\tLoss: 74.264694\n",
      "Train Epoch: 4 [34816/60000 ( 58%)]\tLoss: 73.044052\n",
      "Train Epoch: 4 [35328/60000 ( 59%)]\tLoss: 71.474716\n",
      "Train Epoch: 4 [35840/60000 ( 60%)]\tLoss: 73.686462\n",
      "Train Epoch: 4 [36352/60000 ( 60%)]\tLoss: 73.499825\n",
      "Train Epoch: 4 [36864/60000 ( 61%)]\tLoss: 73.803902\n",
      "Train Epoch: 4 [37376/60000 ( 62%)]\tLoss: 75.651306\n",
      "Train Epoch: 4 [37888/60000 ( 63%)]\tLoss: 80.184891\n",
      "Train Epoch: 4 [38400/60000 ( 64%)]\tLoss: 72.845291\n",
      "Train Epoch: 4 [38912/60000 ( 65%)]\tLoss: 75.794807\n",
      "Train Epoch: 4 [39424/60000 ( 66%)]\tLoss: 71.721497\n",
      "Train Epoch: 4 [39936/60000 ( 66%)]\tLoss: 74.481750\n",
      "Train Epoch: 4 [40448/60000 ( 67%)]\tLoss: 74.727837\n",
      "Train Epoch: 4 [40960/60000 ( 68%)]\tLoss: 74.645706\n",
      "Train Epoch: 4 [41472/60000 ( 69%)]\tLoss: 73.875458\n",
      "Train Epoch: 4 [41984/60000 ( 70%)]\tLoss: 71.740089\n",
      "Train Epoch: 4 [42496/60000 ( 71%)]\tLoss: 67.960648\n",
      "Train Epoch: 4 [43008/60000 ( 71%)]\tLoss: 74.134766\n",
      "Train Epoch: 4 [43520/60000 ( 72%)]\tLoss: 71.375938\n",
      "Train Epoch: 4 [44032/60000 ( 73%)]\tLoss: 70.764473\n",
      "Train Epoch: 4 [44544/60000 ( 74%)]\tLoss: 77.029716\n",
      "Train Epoch: 4 [45056/60000 ( 75%)]\tLoss: 70.291557\n",
      "Train Epoch: 4 [45568/60000 ( 76%)]\tLoss: 70.837173\n",
      "Train Epoch: 4 [46080/60000 ( 77%)]\tLoss: 72.902641\n",
      "Train Epoch: 4 [46592/60000 ( 77%)]\tLoss: 71.668015\n",
      "Train Epoch: 4 [47104/60000 ( 78%)]\tLoss: 67.879837\n",
      "Train Epoch: 4 [47616/60000 ( 79%)]\tLoss: 67.732864\n",
      "Train Epoch: 4 [48128/60000 ( 80%)]\tLoss: 71.284927\n",
      "Train Epoch: 4 [48640/60000 ( 81%)]\tLoss: 72.161530\n",
      "Train Epoch: 4 [49152/60000 ( 82%)]\tLoss: 70.250122\n",
      "Train Epoch: 4 [49664/60000 ( 83%)]\tLoss: 72.478760\n",
      "Train Epoch: 4 [50176/60000 ( 83%)]\tLoss: 71.958633\n",
      "Train Epoch: 4 [50688/60000 ( 84%)]\tLoss: 71.334091\n",
      "Train Epoch: 4 [51200/60000 ( 85%)]\tLoss: 68.291656\n",
      "Train Epoch: 4 [51712/60000 ( 86%)]\tLoss: 73.475151\n",
      "Train Epoch: 4 [52224/60000 ( 87%)]\tLoss: 74.619759\n",
      "Train Epoch: 4 [52736/60000 ( 88%)]\tLoss: 68.102303\n",
      "Train Epoch: 4 [53248/60000 ( 89%)]\tLoss: 74.201286\n",
      "Train Epoch: 4 [53760/60000 ( 89%)]\tLoss: 73.438721\n",
      "Train Epoch: 4 [54272/60000 ( 90%)]\tLoss: 74.599991\n",
      "Train Epoch: 4 [54784/60000 ( 91%)]\tLoss: 77.179337\n",
      "Train Epoch: 4 [55296/60000 ( 92%)]\tLoss: 73.557610\n",
      "Train Epoch: 4 [55808/60000 ( 93%)]\tLoss: 72.546227\n",
      "Train Epoch: 4 [56320/60000 ( 94%)]\tLoss: 69.803070\n",
      "Train Epoch: 4 [56832/60000 ( 94%)]\tLoss: 71.664894\n",
      "Train Epoch: 4 [57344/60000 ( 95%)]\tLoss: 73.075401\n",
      "Train Epoch: 4 [57856/60000 ( 96%)]\tLoss: 69.927940\n",
      "Train Epoch: 4 [58368/60000 ( 97%)]\tLoss: 70.032394\n",
      "Train Epoch: 4 [58880/60000 ( 98%)]\tLoss: 69.054100\n",
      "Train Epoch: 4 [59392/60000 ( 99%)]\tLoss: 67.474289\n",
      "Train Epoch: 4 [22464/60000 (100%)]\tLoss: 72.622899\n",
      "====> Epoch: 4 Average loss: 73.2430\n",
      "====> Test set loss: 72.3600\n",
      "Train Epoch: 5 [    0/60000 (  0%)]\tLoss: 69.752937\n",
      "Train Epoch: 5 [  512/60000 (  1%)]\tLoss: 66.663528\n",
      "Train Epoch: 5 [ 1024/60000 (  2%)]\tLoss: 74.930229\n",
      "Train Epoch: 5 [ 1536/60000 (  3%)]\tLoss: 72.352066\n",
      "Train Epoch: 5 [ 2048/60000 (  3%)]\tLoss: 80.234741\n",
      "Train Epoch: 5 [ 2560/60000 (  4%)]\tLoss: 71.859444\n",
      "Train Epoch: 5 [ 3072/60000 (  5%)]\tLoss: 70.444481\n",
      "Train Epoch: 5 [ 3584/60000 (  6%)]\tLoss: 75.415825\n",
      "Train Epoch: 5 [ 4096/60000 (  7%)]\tLoss: 73.476349\n",
      "Train Epoch: 5 [ 4608/60000 (  8%)]\tLoss: 72.006149\n",
      "Train Epoch: 5 [ 5120/60000 (  9%)]\tLoss: 74.672745\n",
      "Train Epoch: 5 [ 5632/60000 (  9%)]\tLoss: 68.023949\n",
      "Train Epoch: 5 [ 6144/60000 ( 10%)]\tLoss: 73.056503\n",
      "Train Epoch: 5 [ 6656/60000 ( 11%)]\tLoss: 73.984398\n",
      "Train Epoch: 5 [ 7168/60000 ( 12%)]\tLoss: 71.922470\n",
      "Train Epoch: 5 [ 7680/60000 ( 13%)]\tLoss: 69.015198\n",
      "Train Epoch: 5 [ 8192/60000 ( 14%)]\tLoss: 69.938515\n",
      "Train Epoch: 5 [ 8704/60000 ( 14%)]\tLoss: 72.416992\n",
      "Train Epoch: 5 [ 9216/60000 ( 15%)]\tLoss: 75.044434\n",
      "Train Epoch: 5 [ 9728/60000 ( 16%)]\tLoss: 71.040962\n",
      "Train Epoch: 5 [10240/60000 ( 17%)]\tLoss: 68.800110\n",
      "Train Epoch: 5 [10752/60000 ( 18%)]\tLoss: 68.594170\n",
      "Train Epoch: 5 [11264/60000 ( 19%)]\tLoss: 68.799568\n",
      "Train Epoch: 5 [11776/60000 ( 20%)]\tLoss: 65.461113\n",
      "Train Epoch: 5 [12288/60000 ( 20%)]\tLoss: 73.360397\n",
      "Train Epoch: 5 [12800/60000 ( 21%)]\tLoss: 69.243385\n",
      "Train Epoch: 5 [13312/60000 ( 22%)]\tLoss: 68.200409\n",
      "Train Epoch: 5 [13824/60000 ( 23%)]\tLoss: 68.962692\n",
      "Train Epoch: 5 [14336/60000 ( 24%)]\tLoss: 66.837456\n",
      "Train Epoch: 5 [14848/60000 ( 25%)]\tLoss: 69.876312\n",
      "Train Epoch: 5 [15360/60000 ( 26%)]\tLoss: 69.888672\n",
      "Train Epoch: 5 [15872/60000 ( 26%)]\tLoss: 68.765030\n",
      "Train Epoch: 5 [16384/60000 ( 27%)]\tLoss: 72.471863\n",
      "Train Epoch: 5 [16896/60000 ( 28%)]\tLoss: 73.391670\n",
      "Train Epoch: 5 [17408/60000 ( 29%)]\tLoss: 71.298508\n",
      "Train Epoch: 5 [17920/60000 ( 30%)]\tLoss: 73.162468\n",
      "Train Epoch: 5 [18432/60000 ( 31%)]\tLoss: 73.044609\n",
      "Train Epoch: 5 [18944/60000 ( 31%)]\tLoss: 71.536682\n",
      "Train Epoch: 5 [19456/60000 ( 32%)]\tLoss: 71.094772\n",
      "Train Epoch: 5 [19968/60000 ( 33%)]\tLoss: 68.635147\n",
      "Train Epoch: 5 [20480/60000 ( 34%)]\tLoss: 68.011841\n",
      "Train Epoch: 5 [20992/60000 ( 35%)]\tLoss: 70.403221\n",
      "Train Epoch: 5 [21504/60000 ( 36%)]\tLoss: 68.095444\n",
      "Train Epoch: 5 [22016/60000 ( 37%)]\tLoss: 68.598312\n",
      "Train Epoch: 5 [22528/60000 ( 37%)]\tLoss: 72.170776\n",
      "Train Epoch: 5 [23040/60000 ( 38%)]\tLoss: 70.049477\n",
      "Train Epoch: 5 [23552/60000 ( 39%)]\tLoss: 72.002678\n",
      "Train Epoch: 5 [24064/60000 ( 40%)]\tLoss: 68.778252\n",
      "Train Epoch: 5 [24576/60000 ( 41%)]\tLoss: 70.706833\n",
      "Train Epoch: 5 [25088/60000 ( 42%)]\tLoss: 71.753799\n",
      "Train Epoch: 5 [25600/60000 ( 43%)]\tLoss: 70.876381\n",
      "Train Epoch: 5 [26112/60000 ( 43%)]\tLoss: 72.462616\n",
      "Train Epoch: 5 [26624/60000 ( 44%)]\tLoss: 70.943871\n",
      "Train Epoch: 5 [27136/60000 ( 45%)]\tLoss: 71.104744\n",
      "Train Epoch: 5 [27648/60000 ( 46%)]\tLoss: 69.342766\n",
      "Train Epoch: 5 [28160/60000 ( 47%)]\tLoss: 68.972626\n",
      "Train Epoch: 5 [28672/60000 ( 48%)]\tLoss: 69.603180\n",
      "Train Epoch: 5 [29184/60000 ( 49%)]\tLoss: 69.623352\n",
      "Train Epoch: 5 [29696/60000 ( 49%)]\tLoss: 66.417801\n",
      "Train Epoch: 5 [30208/60000 ( 50%)]\tLoss: 70.652473\n",
      "Train Epoch: 5 [30720/60000 ( 51%)]\tLoss: 70.524635\n",
      "Train Epoch: 5 [31232/60000 ( 52%)]\tLoss: 67.434723\n",
      "Train Epoch: 5 [31744/60000 ( 53%)]\tLoss: 72.022385\n",
      "Train Epoch: 5 [32256/60000 ( 54%)]\tLoss: 66.350502\n",
      "Train Epoch: 5 [32768/60000 ( 54%)]\tLoss: 65.508507\n",
      "Train Epoch: 5 [33280/60000 ( 55%)]\tLoss: 69.929474\n",
      "Train Epoch: 5 [33792/60000 ( 56%)]\tLoss: 70.316002\n",
      "Train Epoch: 5 [34304/60000 ( 57%)]\tLoss: 69.232635\n",
      "Train Epoch: 5 [34816/60000 ( 58%)]\tLoss: 72.187271\n",
      "Train Epoch: 5 [35328/60000 ( 59%)]\tLoss: 70.254616\n",
      "Train Epoch: 5 [35840/60000 ( 60%)]\tLoss: 70.007706\n",
      "Train Epoch: 5 [36352/60000 ( 60%)]\tLoss: 70.184593\n",
      "Train Epoch: 5 [36864/60000 ( 61%)]\tLoss: 66.308289\n",
      "Train Epoch: 5 [37376/60000 ( 62%)]\tLoss: 67.158394\n",
      "Train Epoch: 5 [37888/60000 ( 63%)]\tLoss: 70.039917\n",
      "Train Epoch: 5 [38400/60000 ( 64%)]\tLoss: 70.061188\n",
      "Train Epoch: 5 [38912/60000 ( 65%)]\tLoss: 70.114433\n",
      "Train Epoch: 5 [39424/60000 ( 66%)]\tLoss: 68.240219\n",
      "Train Epoch: 5 [39936/60000 ( 66%)]\tLoss: 67.631523\n",
      "Train Epoch: 5 [40448/60000 ( 67%)]\tLoss: 67.442444\n",
      "Train Epoch: 5 [40960/60000 ( 68%)]\tLoss: 68.428116\n",
      "Train Epoch: 5 [41472/60000 ( 69%)]\tLoss: 68.061569\n",
      "Train Epoch: 5 [41984/60000 ( 70%)]\tLoss: 73.303459\n",
      "Train Epoch: 5 [42496/60000 ( 71%)]\tLoss: 67.451035\n",
      "Train Epoch: 5 [43008/60000 ( 71%)]\tLoss: 67.360062\n",
      "Train Epoch: 5 [43520/60000 ( 72%)]\tLoss: 70.521034\n",
      "Train Epoch: 5 [44032/60000 ( 73%)]\tLoss: 66.713257\n",
      "Train Epoch: 5 [44544/60000 ( 74%)]\tLoss: 67.144958\n",
      "Train Epoch: 5 [45056/60000 ( 75%)]\tLoss: 67.773827\n",
      "Train Epoch: 5 [45568/60000 ( 76%)]\tLoss: 69.434631\n",
      "Train Epoch: 5 [46080/60000 ( 77%)]\tLoss: 66.304260\n",
      "Train Epoch: 5 [46592/60000 ( 77%)]\tLoss: 69.315872\n",
      "Train Epoch: 5 [47104/60000 ( 78%)]\tLoss: 69.726654\n",
      "Train Epoch: 5 [47616/60000 ( 79%)]\tLoss: 65.878540\n",
      "Train Epoch: 5 [48128/60000 ( 80%)]\tLoss: 64.537483\n",
      "Train Epoch: 5 [48640/60000 ( 81%)]\tLoss: 68.407013\n",
      "Train Epoch: 5 [49152/60000 ( 82%)]\tLoss: 64.463043\n",
      "Train Epoch: 5 [49664/60000 ( 83%)]\tLoss: 65.171951\n",
      "Train Epoch: 5 [50176/60000 ( 83%)]\tLoss: 67.332329\n",
      "Train Epoch: 5 [50688/60000 ( 84%)]\tLoss: 66.964569\n",
      "Train Epoch: 5 [51200/60000 ( 85%)]\tLoss: 66.050240\n",
      "Train Epoch: 5 [51712/60000 ( 86%)]\tLoss: 72.604958\n",
      "Train Epoch: 5 [52224/60000 ( 87%)]\tLoss: 69.854263\n",
      "Train Epoch: 5 [52736/60000 ( 88%)]\tLoss: 65.417404\n",
      "Train Epoch: 5 [53248/60000 ( 89%)]\tLoss: 66.614235\n",
      "Train Epoch: 5 [53760/60000 ( 89%)]\tLoss: 68.774551\n",
      "Train Epoch: 5 [54272/60000 ( 90%)]\tLoss: 65.473709\n",
      "Train Epoch: 5 [54784/60000 ( 91%)]\tLoss: 64.155998\n",
      "Train Epoch: 5 [55296/60000 ( 92%)]\tLoss: 68.378677\n",
      "Train Epoch: 5 [55808/60000 ( 93%)]\tLoss: 66.963318\n",
      "Train Epoch: 5 [56320/60000 ( 94%)]\tLoss: 69.064240\n",
      "Train Epoch: 5 [56832/60000 ( 94%)]\tLoss: 67.328148\n",
      "Train Epoch: 5 [57344/60000 ( 95%)]\tLoss: 65.272949\n",
      "Train Epoch: 5 [57856/60000 ( 96%)]\tLoss: 63.921486\n",
      "Train Epoch: 5 [58368/60000 ( 97%)]\tLoss: 70.026550\n",
      "Train Epoch: 5 [58880/60000 ( 98%)]\tLoss: 69.192879\n",
      "Train Epoch: 5 [59392/60000 ( 99%)]\tLoss: 69.331429\n",
      "Train Epoch: 5 [22464/60000 (100%)]\tLoss: 73.676620\n",
      "====> Epoch: 5 Average loss: 69.4776\n",
      "====> Test set loss: 70.9013\n",
      "Train Epoch: 6 [    0/60000 (  0%)]\tLoss: 65.514725\n",
      "Train Epoch: 6 [  512/60000 (  1%)]\tLoss: 71.866974\n",
      "Train Epoch: 6 [ 1024/60000 (  2%)]\tLoss: 65.404327\n",
      "Train Epoch: 6 [ 1536/60000 (  3%)]\tLoss: 67.456474\n",
      "Train Epoch: 6 [ 2048/60000 (  3%)]\tLoss: 65.993248\n",
      "Train Epoch: 6 [ 2560/60000 (  4%)]\tLoss: 67.785538\n",
      "Train Epoch: 6 [ 3072/60000 (  5%)]\tLoss: 68.076263\n",
      "Train Epoch: 6 [ 3584/60000 (  6%)]\tLoss: 62.810799\n",
      "Train Epoch: 6 [ 4096/60000 (  7%)]\tLoss: 65.476395\n",
      "Train Epoch: 6 [ 4608/60000 (  8%)]\tLoss: 67.190056\n",
      "Train Epoch: 6 [ 5120/60000 (  9%)]\tLoss: 65.699989\n",
      "Train Epoch: 6 [ 5632/60000 (  9%)]\tLoss: 65.807632\n",
      "Train Epoch: 6 [ 6144/60000 ( 10%)]\tLoss: 67.561714\n",
      "Train Epoch: 6 [ 6656/60000 ( 11%)]\tLoss: 62.979912\n",
      "Train Epoch: 6 [ 7168/60000 ( 12%)]\tLoss: 66.329453\n",
      "Train Epoch: 6 [ 7680/60000 ( 13%)]\tLoss: 67.077164\n",
      "Train Epoch: 6 [ 8192/60000 ( 14%)]\tLoss: 65.045837\n",
      "Train Epoch: 6 [ 8704/60000 ( 14%)]\tLoss: 66.135017\n",
      "Train Epoch: 6 [ 9216/60000 ( 15%)]\tLoss: 68.707962\n",
      "Train Epoch: 6 [ 9728/60000 ( 16%)]\tLoss: 68.366165\n",
      "Train Epoch: 6 [10240/60000 ( 17%)]\tLoss: 69.892265\n",
      "Train Epoch: 6 [10752/60000 ( 18%)]\tLoss: 67.047867\n",
      "Train Epoch: 6 [11264/60000 ( 19%)]\tLoss: 66.809158\n",
      "Train Epoch: 6 [11776/60000 ( 20%)]\tLoss: 67.360451\n",
      "Train Epoch: 6 [12288/60000 ( 20%)]\tLoss: 66.650208\n",
      "Train Epoch: 6 [12800/60000 ( 21%)]\tLoss: 66.792114\n",
      "Train Epoch: 6 [13312/60000 ( 22%)]\tLoss: 64.721046\n",
      "Train Epoch: 6 [13824/60000 ( 23%)]\tLoss: 67.755165\n",
      "Train Epoch: 6 [14336/60000 ( 24%)]\tLoss: 68.579933\n",
      "Train Epoch: 6 [14848/60000 ( 25%)]\tLoss: 67.007484\n",
      "Train Epoch: 6 [15360/60000 ( 26%)]\tLoss: 65.252487\n",
      "Train Epoch: 6 [15872/60000 ( 26%)]\tLoss: 73.087540\n",
      "Train Epoch: 6 [16384/60000 ( 27%)]\tLoss: 71.576508\n",
      "Train Epoch: 6 [16896/60000 ( 28%)]\tLoss: 68.630440\n",
      "Train Epoch: 6 [17408/60000 ( 29%)]\tLoss: 70.326347\n",
      "Train Epoch: 6 [17920/60000 ( 30%)]\tLoss: 69.277603\n",
      "Train Epoch: 6 [18432/60000 ( 31%)]\tLoss: 68.793442\n",
      "Train Epoch: 6 [18944/60000 ( 31%)]\tLoss: 67.040108\n",
      "Train Epoch: 6 [19456/60000 ( 32%)]\tLoss: 64.337463\n",
      "Train Epoch: 6 [19968/60000 ( 33%)]\tLoss: 64.292793\n",
      "Train Epoch: 6 [20480/60000 ( 34%)]\tLoss: 67.809357\n",
      "Train Epoch: 6 [20992/60000 ( 35%)]\tLoss: 68.294846\n",
      "Train Epoch: 6 [21504/60000 ( 36%)]\tLoss: 66.462418\n",
      "Train Epoch: 6 [22016/60000 ( 37%)]\tLoss: 66.581306\n",
      "Train Epoch: 6 [22528/60000 ( 37%)]\tLoss: 64.429123\n",
      "Train Epoch: 6 [23040/60000 ( 38%)]\tLoss: 65.125946\n",
      "Train Epoch: 6 [23552/60000 ( 39%)]\tLoss: 65.734650\n",
      "Train Epoch: 6 [24064/60000 ( 40%)]\tLoss: 68.706253\n",
      "Train Epoch: 6 [24576/60000 ( 41%)]\tLoss: 67.470444\n",
      "Train Epoch: 6 [25088/60000 ( 42%)]\tLoss: 70.586258\n",
      "Train Epoch: 6 [25600/60000 ( 43%)]\tLoss: 66.946548\n",
      "Train Epoch: 6 [26112/60000 ( 43%)]\tLoss: 63.040520\n",
      "Train Epoch: 6 [26624/60000 ( 44%)]\tLoss: 63.573742\n",
      "Train Epoch: 6 [27136/60000 ( 45%)]\tLoss: 64.587143\n",
      "Train Epoch: 6 [27648/60000 ( 46%)]\tLoss: 69.615967\n",
      "Train Epoch: 6 [28160/60000 ( 47%)]\tLoss: 67.769699\n",
      "Train Epoch: 6 [28672/60000 ( 48%)]\tLoss: 62.418091\n",
      "Train Epoch: 6 [29184/60000 ( 49%)]\tLoss: 67.238762\n",
      "Train Epoch: 6 [29696/60000 ( 49%)]\tLoss: 65.559372\n",
      "Train Epoch: 6 [30208/60000 ( 50%)]\tLoss: 66.862511\n",
      "Train Epoch: 6 [30720/60000 ( 51%)]\tLoss: 68.088799\n",
      "Train Epoch: 6 [31232/60000 ( 52%)]\tLoss: 62.201122\n",
      "Train Epoch: 6 [31744/60000 ( 53%)]\tLoss: 66.958923\n",
      "Train Epoch: 6 [32256/60000 ( 54%)]\tLoss: 68.821648\n",
      "Train Epoch: 6 [32768/60000 ( 54%)]\tLoss: 67.558586\n",
      "Train Epoch: 6 [33280/60000 ( 55%)]\tLoss: 69.931885\n",
      "Train Epoch: 6 [33792/60000 ( 56%)]\tLoss: 66.863960\n",
      "Train Epoch: 6 [34304/60000 ( 57%)]\tLoss: 67.641373\n",
      "Train Epoch: 6 [34816/60000 ( 58%)]\tLoss: 63.309116\n",
      "Train Epoch: 6 [35328/60000 ( 59%)]\tLoss: 65.467484\n",
      "Train Epoch: 6 [35840/60000 ( 60%)]\tLoss: 68.871124\n",
      "Train Epoch: 6 [36352/60000 ( 60%)]\tLoss: 64.627098\n",
      "Train Epoch: 6 [36864/60000 ( 61%)]\tLoss: 69.427505\n",
      "Train Epoch: 6 [37376/60000 ( 62%)]\tLoss: 68.879654\n",
      "Train Epoch: 6 [37888/60000 ( 63%)]\tLoss: 66.343224\n",
      "Train Epoch: 6 [38400/60000 ( 64%)]\tLoss: 71.378960\n",
      "Train Epoch: 6 [38912/60000 ( 65%)]\tLoss: 67.620514\n",
      "Train Epoch: 6 [39424/60000 ( 66%)]\tLoss: 64.931046\n",
      "Train Epoch: 6 [39936/60000 ( 66%)]\tLoss: 66.390450\n",
      "Train Epoch: 6 [40448/60000 ( 67%)]\tLoss: 70.157967\n",
      "Train Epoch: 6 [40960/60000 ( 68%)]\tLoss: 68.787308\n",
      "Train Epoch: 6 [41472/60000 ( 69%)]\tLoss: 66.850792\n",
      "Train Epoch: 6 [41984/60000 ( 70%)]\tLoss: 70.286598\n",
      "Train Epoch: 6 [42496/60000 ( 71%)]\tLoss: 68.298111\n",
      "Train Epoch: 6 [43008/60000 ( 71%)]\tLoss: 67.163574\n",
      "Train Epoch: 6 [43520/60000 ( 72%)]\tLoss: 67.093201\n",
      "Train Epoch: 6 [44032/60000 ( 73%)]\tLoss: 68.614586\n",
      "Train Epoch: 6 [44544/60000 ( 74%)]\tLoss: 65.291145\n",
      "Train Epoch: 6 [45056/60000 ( 75%)]\tLoss: 63.796452\n",
      "Train Epoch: 6 [45568/60000 ( 76%)]\tLoss: 65.549988\n",
      "Train Epoch: 6 [46080/60000 ( 77%)]\tLoss: 68.109451\n",
      "Train Epoch: 6 [46592/60000 ( 77%)]\tLoss: 67.405167\n",
      "Train Epoch: 6 [47104/60000 ( 78%)]\tLoss: 64.405594\n",
      "Train Epoch: 6 [47616/60000 ( 79%)]\tLoss: 67.363564\n",
      "Train Epoch: 6 [48128/60000 ( 80%)]\tLoss: 68.644272\n",
      "Train Epoch: 6 [48640/60000 ( 81%)]\tLoss: 66.926743\n",
      "Train Epoch: 6 [49152/60000 ( 82%)]\tLoss: 67.819443\n",
      "Train Epoch: 6 [49664/60000 ( 83%)]\tLoss: 63.122078\n",
      "Train Epoch: 6 [50176/60000 ( 83%)]\tLoss: 65.577881\n",
      "Train Epoch: 6 [50688/60000 ( 84%)]\tLoss: 67.136215\n",
      "Train Epoch: 6 [51200/60000 ( 85%)]\tLoss: 66.290787\n",
      "Train Epoch: 6 [51712/60000 ( 86%)]\tLoss: 66.018616\n",
      "Train Epoch: 6 [52224/60000 ( 87%)]\tLoss: 67.427322\n",
      "Train Epoch: 6 [52736/60000 ( 88%)]\tLoss: 64.325516\n",
      "Train Epoch: 6 [53248/60000 ( 89%)]\tLoss: 69.899117\n",
      "Train Epoch: 6 [53760/60000 ( 89%)]\tLoss: 67.001083\n",
      "Train Epoch: 6 [54272/60000 ( 90%)]\tLoss: 64.738419\n",
      "Train Epoch: 6 [54784/60000 ( 91%)]\tLoss: 65.238457\n",
      "Train Epoch: 6 [55296/60000 ( 92%)]\tLoss: 64.885658\n",
      "Train Epoch: 6 [55808/60000 ( 93%)]\tLoss: 68.903320\n",
      "Train Epoch: 6 [56320/60000 ( 94%)]\tLoss: 65.533363\n",
      "Train Epoch: 6 [56832/60000 ( 94%)]\tLoss: 67.896767\n",
      "Train Epoch: 6 [57344/60000 ( 95%)]\tLoss: 67.156586\n",
      "Train Epoch: 6 [57856/60000 ( 96%)]\tLoss: 68.304901\n",
      "Train Epoch: 6 [58368/60000 ( 97%)]\tLoss: 66.013245\n",
      "Train Epoch: 6 [58880/60000 ( 98%)]\tLoss: 66.494141\n",
      "Train Epoch: 6 [59392/60000 ( 99%)]\tLoss: 68.981628\n",
      "Train Epoch: 6 [22464/60000 (100%)]\tLoss: 63.879415\n",
      "====> Epoch: 6 Average loss: 66.9442\n",
      "====> Test set loss: 66.8848\n",
      "Train Epoch: 7 [    0/60000 (  0%)]\tLoss: 67.270950\n",
      "Train Epoch: 7 [  512/60000 (  1%)]\tLoss: 64.869934\n",
      "Train Epoch: 7 [ 1024/60000 (  2%)]\tLoss: 64.626709\n",
      "Train Epoch: 7 [ 1536/60000 (  3%)]\tLoss: 71.638489\n",
      "Train Epoch: 7 [ 2048/60000 (  3%)]\tLoss: 65.251770\n",
      "Train Epoch: 7 [ 2560/60000 (  4%)]\tLoss: 67.630363\n",
      "Train Epoch: 7 [ 3072/60000 (  5%)]\tLoss: 64.121262\n",
      "Train Epoch: 7 [ 3584/60000 (  6%)]\tLoss: 66.216553\n",
      "Train Epoch: 7 [ 4096/60000 (  7%)]\tLoss: 61.352150\n",
      "Train Epoch: 7 [ 4608/60000 (  8%)]\tLoss: 65.290756\n",
      "Train Epoch: 7 [ 5120/60000 (  9%)]\tLoss: 62.396088\n",
      "Train Epoch: 7 [ 5632/60000 (  9%)]\tLoss: 64.144501\n",
      "Train Epoch: 7 [ 6144/60000 ( 10%)]\tLoss: 65.783493\n",
      "Train Epoch: 7 [ 6656/60000 ( 11%)]\tLoss: 63.372078\n",
      "Train Epoch: 7 [ 7168/60000 ( 12%)]\tLoss: 66.942520\n",
      "Train Epoch: 7 [ 7680/60000 ( 13%)]\tLoss: 65.000565\n",
      "Train Epoch: 7 [ 8192/60000 ( 14%)]\tLoss: 68.810532\n",
      "Train Epoch: 7 [ 8704/60000 ( 14%)]\tLoss: 66.964218\n",
      "Train Epoch: 7 [ 9216/60000 ( 15%)]\tLoss: 64.290688\n",
      "Train Epoch: 7 [ 9728/60000 ( 16%)]\tLoss: 65.235291\n",
      "Train Epoch: 7 [10240/60000 ( 17%)]\tLoss: 63.003685\n",
      "Train Epoch: 7 [10752/60000 ( 18%)]\tLoss: 64.797844\n",
      "Train Epoch: 7 [11264/60000 ( 19%)]\tLoss: 65.614799\n",
      "Train Epoch: 7 [11776/60000 ( 20%)]\tLoss: 62.144947\n",
      "Train Epoch: 7 [12288/60000 ( 20%)]\tLoss: 64.923981\n",
      "Train Epoch: 7 [12800/60000 ( 21%)]\tLoss: 68.761093\n",
      "Train Epoch: 7 [13312/60000 ( 22%)]\tLoss: 62.005775\n",
      "Train Epoch: 7 [13824/60000 ( 23%)]\tLoss: 62.779022\n",
      "Train Epoch: 7 [14336/60000 ( 24%)]\tLoss: 66.873764\n",
      "Train Epoch: 7 [14848/60000 ( 25%)]\tLoss: 68.964752\n",
      "Train Epoch: 7 [15360/60000 ( 26%)]\tLoss: 68.380562\n",
      "Train Epoch: 7 [15872/60000 ( 26%)]\tLoss: 65.616592\n",
      "Train Epoch: 7 [16384/60000 ( 27%)]\tLoss: 63.833614\n",
      "Train Epoch: 7 [16896/60000 ( 28%)]\tLoss: 63.177658\n",
      "Train Epoch: 7 [17408/60000 ( 29%)]\tLoss: 66.383133\n",
      "Train Epoch: 7 [17920/60000 ( 30%)]\tLoss: 65.156189\n",
      "Train Epoch: 7 [18432/60000 ( 31%)]\tLoss: 66.309280\n",
      "Train Epoch: 7 [18944/60000 ( 31%)]\tLoss: 64.665329\n",
      "Train Epoch: 7 [19456/60000 ( 32%)]\tLoss: 63.905273\n",
      "Train Epoch: 7 [19968/60000 ( 33%)]\tLoss: 63.471443\n",
      "Train Epoch: 7 [20480/60000 ( 34%)]\tLoss: 66.371689\n",
      "Train Epoch: 7 [20992/60000 ( 35%)]\tLoss: 65.128784\n",
      "Train Epoch: 7 [21504/60000 ( 36%)]\tLoss: 65.053795\n",
      "Train Epoch: 7 [22016/60000 ( 37%)]\tLoss: 63.366295\n",
      "Train Epoch: 7 [22528/60000 ( 37%)]\tLoss: 65.858376\n",
      "Train Epoch: 7 [23040/60000 ( 38%)]\tLoss: 68.230354\n",
      "Train Epoch: 7 [23552/60000 ( 39%)]\tLoss: 64.600784\n",
      "Train Epoch: 7 [24064/60000 ( 40%)]\tLoss: 65.932541\n",
      "Train Epoch: 7 [24576/60000 ( 41%)]\tLoss: 61.946468\n",
      "Train Epoch: 7 [25088/60000 ( 42%)]\tLoss: 64.149078\n",
      "Train Epoch: 7 [25600/60000 ( 43%)]\tLoss: 67.255653\n",
      "Train Epoch: 7 [26112/60000 ( 43%)]\tLoss: 65.458008\n",
      "Train Epoch: 7 [26624/60000 ( 44%)]\tLoss: 65.121376\n",
      "Train Epoch: 7 [27136/60000 ( 45%)]\tLoss: 63.657341\n",
      "Train Epoch: 7 [27648/60000 ( 46%)]\tLoss: 66.690842\n",
      "Train Epoch: 7 [28160/60000 ( 47%)]\tLoss: 66.345573\n",
      "Train Epoch: 7 [28672/60000 ( 48%)]\tLoss: 65.976883\n",
      "Train Epoch: 7 [29184/60000 ( 49%)]\tLoss: 62.986694\n",
      "Train Epoch: 7 [29696/60000 ( 49%)]\tLoss: 65.057114\n",
      "Train Epoch: 7 [30208/60000 ( 50%)]\tLoss: 68.020950\n",
      "Train Epoch: 7 [30720/60000 ( 51%)]\tLoss: 66.268021\n",
      "Train Epoch: 7 [31232/60000 ( 52%)]\tLoss: 64.300522\n",
      "Train Epoch: 7 [31744/60000 ( 53%)]\tLoss: 59.628532\n",
      "Train Epoch: 7 [32256/60000 ( 54%)]\tLoss: 64.352585\n",
      "Train Epoch: 7 [32768/60000 ( 54%)]\tLoss: 61.758522\n",
      "Train Epoch: 7 [33280/60000 ( 55%)]\tLoss: 67.729889\n",
      "Train Epoch: 7 [33792/60000 ( 56%)]\tLoss: 66.310310\n",
      "Train Epoch: 7 [34304/60000 ( 57%)]\tLoss: 65.914398\n",
      "Train Epoch: 7 [34816/60000 ( 58%)]\tLoss: 65.143494\n",
      "Train Epoch: 7 [35328/60000 ( 59%)]\tLoss: 61.972462\n",
      "Train Epoch: 7 [35840/60000 ( 60%)]\tLoss: 64.479462\n",
      "Train Epoch: 7 [36352/60000 ( 60%)]\tLoss: 63.492470\n",
      "Train Epoch: 7 [36864/60000 ( 61%)]\tLoss: 60.989445\n",
      "Train Epoch: 7 [37376/60000 ( 62%)]\tLoss: 70.326263\n",
      "Train Epoch: 7 [37888/60000 ( 63%)]\tLoss: 66.955612\n",
      "Train Epoch: 7 [38400/60000 ( 64%)]\tLoss: 65.612564\n",
      "Train Epoch: 7 [38912/60000 ( 65%)]\tLoss: 63.582397\n",
      "Train Epoch: 7 [39424/60000 ( 66%)]\tLoss: 62.597130\n",
      "Train Epoch: 7 [39936/60000 ( 66%)]\tLoss: 60.710278\n",
      "Train Epoch: 7 [40448/60000 ( 67%)]\tLoss: 67.832062\n",
      "Train Epoch: 7 [40960/60000 ( 68%)]\tLoss: 65.043915\n",
      "Train Epoch: 7 [41472/60000 ( 69%)]\tLoss: 64.226028\n",
      "Train Epoch: 7 [41984/60000 ( 70%)]\tLoss: 64.684685\n",
      "Train Epoch: 7 [42496/60000 ( 71%)]\tLoss: 62.654907\n",
      "Train Epoch: 7 [43008/60000 ( 71%)]\tLoss: 67.056076\n",
      "Train Epoch: 7 [43520/60000 ( 72%)]\tLoss: 64.072113\n",
      "Train Epoch: 7 [44032/60000 ( 73%)]\tLoss: 66.042686\n",
      "Train Epoch: 7 [44544/60000 ( 74%)]\tLoss: 66.422737\n",
      "Train Epoch: 7 [45056/60000 ( 75%)]\tLoss: 64.967010\n",
      "Train Epoch: 7 [45568/60000 ( 76%)]\tLoss: 63.575516\n",
      "Train Epoch: 7 [46080/60000 ( 77%)]\tLoss: 64.629936\n",
      "Train Epoch: 7 [46592/60000 ( 77%)]\tLoss: 60.872887\n",
      "Train Epoch: 7 [47104/60000 ( 78%)]\tLoss: 60.323513\n",
      "Train Epoch: 7 [47616/60000 ( 79%)]\tLoss: 64.569443\n",
      "Train Epoch: 7 [48128/60000 ( 80%)]\tLoss: 63.201756\n",
      "Train Epoch: 7 [48640/60000 ( 81%)]\tLoss: 63.018074\n",
      "Train Epoch: 7 [49152/60000 ( 82%)]\tLoss: 68.355949\n",
      "Train Epoch: 7 [49664/60000 ( 83%)]\tLoss: 65.300751\n",
      "Train Epoch: 7 [50176/60000 ( 83%)]\tLoss: 61.351482\n",
      "Train Epoch: 7 [50688/60000 ( 84%)]\tLoss: 68.100403\n",
      "Train Epoch: 7 [51200/60000 ( 85%)]\tLoss: 62.968292\n",
      "Train Epoch: 7 [51712/60000 ( 86%)]\tLoss: 66.072243\n",
      "Train Epoch: 7 [52224/60000 ( 87%)]\tLoss: 64.761391\n",
      "Train Epoch: 7 [52736/60000 ( 88%)]\tLoss: 63.151817\n",
      "Train Epoch: 7 [53248/60000 ( 89%)]\tLoss: 65.519073\n",
      "Train Epoch: 7 [53760/60000 ( 89%)]\tLoss: 61.454750\n",
      "Train Epoch: 7 [54272/60000 ( 90%)]\tLoss: 65.104736\n",
      "Train Epoch: 7 [54784/60000 ( 91%)]\tLoss: 66.800858\n",
      "Train Epoch: 7 [55296/60000 ( 92%)]\tLoss: 63.467274\n",
      "Train Epoch: 7 [55808/60000 ( 93%)]\tLoss: 67.327217\n",
      "Train Epoch: 7 [56320/60000 ( 94%)]\tLoss: 60.658165\n",
      "Train Epoch: 7 [56832/60000 ( 94%)]\tLoss: 69.038780\n",
      "Train Epoch: 7 [57344/60000 ( 95%)]\tLoss: 64.759087\n",
      "Train Epoch: 7 [57856/60000 ( 96%)]\tLoss: 65.485970\n",
      "Train Epoch: 7 [58368/60000 ( 97%)]\tLoss: 65.661446\n",
      "Train Epoch: 7 [58880/60000 ( 98%)]\tLoss: 62.532848\n",
      "Train Epoch: 7 [59392/60000 ( 99%)]\tLoss: 63.102959\n",
      "Train Epoch: 7 [22464/60000 (100%)]\tLoss: 60.634684\n",
      "====> Epoch: 7 Average loss: 65.0113\n",
      "====> Test set loss: 65.4194\n",
      "Train Epoch: 8 [    0/60000 (  0%)]\tLoss: 65.881561\n",
      "Train Epoch: 8 [  512/60000 (  1%)]\tLoss: 64.017128\n",
      "Train Epoch: 8 [ 1024/60000 (  2%)]\tLoss: 62.390057\n",
      "Train Epoch: 8 [ 1536/60000 (  3%)]\tLoss: 65.598122\n",
      "Train Epoch: 8 [ 2048/60000 (  3%)]\tLoss: 63.777992\n",
      "Train Epoch: 8 [ 2560/60000 (  4%)]\tLoss: 62.777321\n",
      "Train Epoch: 8 [ 3072/60000 (  5%)]\tLoss: 67.077087\n",
      "Train Epoch: 8 [ 3584/60000 (  6%)]\tLoss: 65.622765\n",
      "Train Epoch: 8 [ 4096/60000 (  7%)]\tLoss: 63.453907\n",
      "Train Epoch: 8 [ 4608/60000 (  8%)]\tLoss: 68.187668\n",
      "Train Epoch: 8 [ 5120/60000 (  9%)]\tLoss: 68.247879\n",
      "Train Epoch: 8 [ 5632/60000 (  9%)]\tLoss: 61.906338\n",
      "Train Epoch: 8 [ 6144/60000 ( 10%)]\tLoss: 65.654449\n",
      "Train Epoch: 8 [ 6656/60000 ( 11%)]\tLoss: 62.341675\n",
      "Train Epoch: 8 [ 7168/60000 ( 12%)]\tLoss: 63.240509\n",
      "Train Epoch: 8 [ 7680/60000 ( 13%)]\tLoss: 65.489876\n",
      "Train Epoch: 8 [ 8192/60000 ( 14%)]\tLoss: 64.148903\n",
      "Train Epoch: 8 [ 8704/60000 ( 14%)]\tLoss: 66.956955\n",
      "Train Epoch: 8 [ 9216/60000 ( 15%)]\tLoss: 63.461891\n",
      "Train Epoch: 8 [ 9728/60000 ( 16%)]\tLoss: 62.745461\n",
      "Train Epoch: 8 [10240/60000 ( 17%)]\tLoss: 68.119446\n",
      "Train Epoch: 8 [10752/60000 ( 18%)]\tLoss: 63.838253\n",
      "Train Epoch: 8 [11264/60000 ( 19%)]\tLoss: 59.524025\n",
      "Train Epoch: 8 [11776/60000 ( 20%)]\tLoss: 64.330200\n",
      "Train Epoch: 8 [12288/60000 ( 20%)]\tLoss: 64.062851\n",
      "Train Epoch: 8 [12800/60000 ( 21%)]\tLoss: 64.230606\n",
      "Train Epoch: 8 [13312/60000 ( 22%)]\tLoss: 61.007584\n",
      "Train Epoch: 8 [13824/60000 ( 23%)]\tLoss: 63.912766\n",
      "Train Epoch: 8 [14336/60000 ( 24%)]\tLoss: 64.884560\n",
      "Train Epoch: 8 [14848/60000 ( 25%)]\tLoss: 62.112244\n",
      "Train Epoch: 8 [15360/60000 ( 26%)]\tLoss: 65.738686\n",
      "Train Epoch: 8 [15872/60000 ( 26%)]\tLoss: 64.381142\n",
      "Train Epoch: 8 [16384/60000 ( 27%)]\tLoss: 62.639442\n",
      "Train Epoch: 8 [16896/60000 ( 28%)]\tLoss: 65.207031\n",
      "Train Epoch: 8 [17408/60000 ( 29%)]\tLoss: 65.542694\n",
      "Train Epoch: 8 [17920/60000 ( 30%)]\tLoss: 58.980587\n",
      "Train Epoch: 8 [18432/60000 ( 31%)]\tLoss: 67.126450\n",
      "Train Epoch: 8 [18944/60000 ( 31%)]\tLoss: 65.441467\n",
      "Train Epoch: 8 [19456/60000 ( 32%)]\tLoss: 61.956490\n",
      "Train Epoch: 8 [19968/60000 ( 33%)]\tLoss: 62.934624\n",
      "Train Epoch: 8 [20480/60000 ( 34%)]\tLoss: 65.528580\n",
      "Train Epoch: 8 [20992/60000 ( 35%)]\tLoss: 63.150040\n",
      "Train Epoch: 8 [21504/60000 ( 36%)]\tLoss: 64.802406\n",
      "Train Epoch: 8 [22016/60000 ( 37%)]\tLoss: 66.479889\n",
      "Train Epoch: 8 [22528/60000 ( 37%)]\tLoss: 60.304863\n",
      "Train Epoch: 8 [23040/60000 ( 38%)]\tLoss: 65.629677\n",
      "Train Epoch: 8 [23552/60000 ( 39%)]\tLoss: 63.915852\n",
      "Train Epoch: 8 [24064/60000 ( 40%)]\tLoss: 66.188301\n",
      "Train Epoch: 8 [24576/60000 ( 41%)]\tLoss: 63.522331\n",
      "Train Epoch: 8 [25088/60000 ( 42%)]\tLoss: 64.487946\n",
      "Train Epoch: 8 [25600/60000 ( 43%)]\tLoss: 64.878181\n",
      "Train Epoch: 8 [26112/60000 ( 43%)]\tLoss: 64.023064\n",
      "Train Epoch: 8 [26624/60000 ( 44%)]\tLoss: 64.163727\n",
      "Train Epoch: 8 [27136/60000 ( 45%)]\tLoss: 63.286697\n",
      "Train Epoch: 8 [27648/60000 ( 46%)]\tLoss: 62.042709\n",
      "Train Epoch: 8 [28160/60000 ( 47%)]\tLoss: 62.093414\n",
      "Train Epoch: 8 [28672/60000 ( 48%)]\tLoss: 63.212158\n",
      "Train Epoch: 8 [29184/60000 ( 49%)]\tLoss: 64.641296\n",
      "Train Epoch: 8 [29696/60000 ( 49%)]\tLoss: 64.293602\n",
      "Train Epoch: 8 [30208/60000 ( 50%)]\tLoss: 64.755035\n",
      "Train Epoch: 8 [30720/60000 ( 51%)]\tLoss: 66.976128\n",
      "Train Epoch: 8 [31232/60000 ( 52%)]\tLoss: 66.073814\n",
      "Train Epoch: 8 [31744/60000 ( 53%)]\tLoss: 60.701416\n",
      "Train Epoch: 8 [32256/60000 ( 54%)]\tLoss: 63.441704\n",
      "Train Epoch: 8 [32768/60000 ( 54%)]\tLoss: 61.617104\n",
      "Train Epoch: 8 [33280/60000 ( 55%)]\tLoss: 61.939117\n",
      "Train Epoch: 8 [33792/60000 ( 56%)]\tLoss: 61.989815\n",
      "Train Epoch: 8 [34304/60000 ( 57%)]\tLoss: 62.154709\n",
      "Train Epoch: 8 [34816/60000 ( 58%)]\tLoss: 63.507896\n",
      "Train Epoch: 8 [35328/60000 ( 59%)]\tLoss: 65.643707\n",
      "Train Epoch: 8 [35840/60000 ( 60%)]\tLoss: 63.098000\n",
      "Train Epoch: 8 [36352/60000 ( 60%)]\tLoss: 59.969643\n",
      "Train Epoch: 8 [36864/60000 ( 61%)]\tLoss: 64.290901\n",
      "Train Epoch: 8 [37376/60000 ( 62%)]\tLoss: 67.358871\n",
      "Train Epoch: 8 [37888/60000 ( 63%)]\tLoss: 64.576202\n",
      "Train Epoch: 8 [38400/60000 ( 64%)]\tLoss: 64.739090\n",
      "Train Epoch: 8 [38912/60000 ( 65%)]\tLoss: 66.764091\n",
      "Train Epoch: 8 [39424/60000 ( 66%)]\tLoss: 62.061798\n",
      "Train Epoch: 8 [39936/60000 ( 66%)]\tLoss: 65.025467\n",
      "Train Epoch: 8 [40448/60000 ( 67%)]\tLoss: 63.869514\n",
      "Train Epoch: 8 [40960/60000 ( 68%)]\tLoss: 64.425781\n",
      "Train Epoch: 8 [41472/60000 ( 69%)]\tLoss: 63.843399\n",
      "Train Epoch: 8 [41984/60000 ( 70%)]\tLoss: 60.992256\n",
      "Train Epoch: 8 [42496/60000 ( 71%)]\tLoss: 62.835068\n",
      "Train Epoch: 8 [43008/60000 ( 71%)]\tLoss: 64.969238\n",
      "Train Epoch: 8 [43520/60000 ( 72%)]\tLoss: 64.211464\n",
      "Train Epoch: 8 [44032/60000 ( 73%)]\tLoss: 63.658676\n",
      "Train Epoch: 8 [44544/60000 ( 74%)]\tLoss: 64.022476\n",
      "Train Epoch: 8 [45056/60000 ( 75%)]\tLoss: 61.388832\n",
      "Train Epoch: 8 [45568/60000 ( 76%)]\tLoss: 66.554337\n",
      "Train Epoch: 8 [46080/60000 ( 77%)]\tLoss: 60.628979\n",
      "Train Epoch: 8 [46592/60000 ( 77%)]\tLoss: 60.538330\n",
      "Train Epoch: 8 [47104/60000 ( 78%)]\tLoss: 65.642197\n",
      "Train Epoch: 8 [47616/60000 ( 79%)]\tLoss: 64.090469\n",
      "Train Epoch: 8 [48128/60000 ( 80%)]\tLoss: 64.000702\n",
      "Train Epoch: 8 [48640/60000 ( 81%)]\tLoss: 63.657513\n",
      "Train Epoch: 8 [49152/60000 ( 82%)]\tLoss: 64.453621\n",
      "Train Epoch: 8 [49664/60000 ( 83%)]\tLoss: 63.093426\n",
      "Train Epoch: 8 [50176/60000 ( 83%)]\tLoss: 64.146194\n",
      "Train Epoch: 8 [50688/60000 ( 84%)]\tLoss: 63.766472\n",
      "Train Epoch: 8 [51200/60000 ( 85%)]\tLoss: 65.155731\n",
      "Train Epoch: 8 [51712/60000 ( 86%)]\tLoss: 64.216469\n",
      "Train Epoch: 8 [52224/60000 ( 87%)]\tLoss: 65.930176\n",
      "Train Epoch: 8 [52736/60000 ( 88%)]\tLoss: 64.356186\n",
      "Train Epoch: 8 [53248/60000 ( 89%)]\tLoss: 64.490814\n",
      "Train Epoch: 8 [53760/60000 ( 89%)]\tLoss: 63.915939\n",
      "Train Epoch: 8 [54272/60000 ( 90%)]\tLoss: 60.621395\n",
      "Train Epoch: 8 [54784/60000 ( 91%)]\tLoss: 63.368221\n",
      "Train Epoch: 8 [55296/60000 ( 92%)]\tLoss: 62.175980\n",
      "Train Epoch: 8 [55808/60000 ( 93%)]\tLoss: 62.891304\n",
      "Train Epoch: 8 [56320/60000 ( 94%)]\tLoss: 64.971024\n",
      "Train Epoch: 8 [56832/60000 ( 94%)]\tLoss: 61.568909\n",
      "Train Epoch: 8 [57344/60000 ( 95%)]\tLoss: 66.474663\n",
      "Train Epoch: 8 [57856/60000 ( 96%)]\tLoss: 63.715942\n",
      "Train Epoch: 8 [58368/60000 ( 97%)]\tLoss: 64.440178\n",
      "Train Epoch: 8 [58880/60000 ( 98%)]\tLoss: 61.142822\n",
      "Train Epoch: 8 [59392/60000 ( 99%)]\tLoss: 59.326267\n",
      "Train Epoch: 8 [22464/60000 (100%)]\tLoss: 67.077657\n",
      "====> Epoch: 8 Average loss: 63.7995\n",
      "====> Test set loss: 65.0832\n",
      "Train Epoch: 9 [    0/60000 (  0%)]\tLoss: 63.738380\n",
      "Train Epoch: 9 [  512/60000 (  1%)]\tLoss: 61.057888\n",
      "Train Epoch: 9 [ 1024/60000 (  2%)]\tLoss: 62.689167\n",
      "Train Epoch: 9 [ 1536/60000 (  3%)]\tLoss: 62.752186\n",
      "Train Epoch: 9 [ 2048/60000 (  3%)]\tLoss: 58.874493\n",
      "Train Epoch: 9 [ 2560/60000 (  4%)]\tLoss: 64.599815\n",
      "Train Epoch: 9 [ 3072/60000 (  5%)]\tLoss: 65.725006\n",
      "Train Epoch: 9 [ 3584/60000 (  6%)]\tLoss: 62.362926\n",
      "Train Epoch: 9 [ 4096/60000 (  7%)]\tLoss: 64.929703\n",
      "Train Epoch: 9 [ 4608/60000 (  8%)]\tLoss: 63.302242\n",
      "Train Epoch: 9 [ 5120/60000 (  9%)]\tLoss: 63.474579\n",
      "Train Epoch: 9 [ 5632/60000 (  9%)]\tLoss: 65.486145\n",
      "Train Epoch: 9 [ 6144/60000 ( 10%)]\tLoss: 63.405491\n",
      "Train Epoch: 9 [ 6656/60000 ( 11%)]\tLoss: 66.539131\n",
      "Train Epoch: 9 [ 7168/60000 ( 12%)]\tLoss: 62.956867\n",
      "Train Epoch: 9 [ 7680/60000 ( 13%)]\tLoss: 60.152084\n",
      "Train Epoch: 9 [ 8192/60000 ( 14%)]\tLoss: 60.979790\n",
      "Train Epoch: 9 [ 8704/60000 ( 14%)]\tLoss: 62.865746\n",
      "Train Epoch: 9 [ 9216/60000 ( 15%)]\tLoss: 65.266869\n",
      "Train Epoch: 9 [ 9728/60000 ( 16%)]\tLoss: 61.861336\n",
      "Train Epoch: 9 [10240/60000 ( 17%)]\tLoss: 63.857025\n",
      "Train Epoch: 9 [10752/60000 ( 18%)]\tLoss: 61.617115\n",
      "Train Epoch: 9 [11264/60000 ( 19%)]\tLoss: 61.387352\n",
      "Train Epoch: 9 [11776/60000 ( 20%)]\tLoss: 62.641289\n",
      "Train Epoch: 9 [12288/60000 ( 20%)]\tLoss: 61.637802\n",
      "Train Epoch: 9 [12800/60000 ( 21%)]\tLoss: 65.957542\n",
      "Train Epoch: 9 [13312/60000 ( 22%)]\tLoss: 64.777290\n",
      "Train Epoch: 9 [13824/60000 ( 23%)]\tLoss: 60.019291\n",
      "Train Epoch: 9 [14336/60000 ( 24%)]\tLoss: 61.785641\n",
      "Train Epoch: 9 [14848/60000 ( 25%)]\tLoss: 60.245224\n",
      "Train Epoch: 9 [15360/60000 ( 26%)]\tLoss: 65.213234\n",
      "Train Epoch: 9 [15872/60000 ( 26%)]\tLoss: 65.330345\n",
      "Train Epoch: 9 [16384/60000 ( 27%)]\tLoss: 63.718590\n",
      "Train Epoch: 9 [16896/60000 ( 28%)]\tLoss: 60.529793\n",
      "Train Epoch: 9 [17408/60000 ( 29%)]\tLoss: 63.010189\n",
      "Train Epoch: 9 [17920/60000 ( 30%)]\tLoss: 64.092232\n",
      "Train Epoch: 9 [18432/60000 ( 31%)]\tLoss: 62.468964\n",
      "Train Epoch: 9 [18944/60000 ( 31%)]\tLoss: 61.270317\n",
      "Train Epoch: 9 [19456/60000 ( 32%)]\tLoss: 63.181633\n",
      "Train Epoch: 9 [19968/60000 ( 33%)]\tLoss: 60.628281\n",
      "Train Epoch: 9 [20480/60000 ( 34%)]\tLoss: 62.047096\n",
      "Train Epoch: 9 [20992/60000 ( 35%)]\tLoss: 61.607903\n",
      "Train Epoch: 9 [21504/60000 ( 36%)]\tLoss: 62.014137\n",
      "Train Epoch: 9 [22016/60000 ( 37%)]\tLoss: 60.291397\n",
      "Train Epoch: 9 [22528/60000 ( 37%)]\tLoss: 59.952095\n",
      "Train Epoch: 9 [23040/60000 ( 38%)]\tLoss: 68.165260\n",
      "Train Epoch: 9 [23552/60000 ( 39%)]\tLoss: 64.647781\n",
      "Train Epoch: 9 [24064/60000 ( 40%)]\tLoss: 60.068604\n",
      "Train Epoch: 9 [24576/60000 ( 41%)]\tLoss: 70.229919\n",
      "Train Epoch: 9 [25088/60000 ( 42%)]\tLoss: 64.917702\n",
      "Train Epoch: 9 [25600/60000 ( 43%)]\tLoss: 59.876526\n",
      "Train Epoch: 9 [26112/60000 ( 43%)]\tLoss: 65.913727\n",
      "Train Epoch: 9 [26624/60000 ( 44%)]\tLoss: 65.639595\n",
      "Train Epoch: 9 [27136/60000 ( 45%)]\tLoss: 63.195103\n",
      "Train Epoch: 9 [27648/60000 ( 46%)]\tLoss: 62.322212\n",
      "Train Epoch: 9 [28160/60000 ( 47%)]\tLoss: 65.935806\n",
      "Train Epoch: 9 [28672/60000 ( 48%)]\tLoss: 62.306526\n",
      "Train Epoch: 9 [29184/60000 ( 49%)]\tLoss: 64.259781\n",
      "Train Epoch: 9 [29696/60000 ( 49%)]\tLoss: 64.718590\n",
      "Train Epoch: 9 [30208/60000 ( 50%)]\tLoss: 62.486923\n",
      "Train Epoch: 9 [30720/60000 ( 51%)]\tLoss: 64.364365\n",
      "Train Epoch: 9 [31232/60000 ( 52%)]\tLoss: 65.355438\n",
      "Train Epoch: 9 [31744/60000 ( 53%)]\tLoss: 62.211128\n",
      "Train Epoch: 9 [32256/60000 ( 54%)]\tLoss: 65.660141\n",
      "Train Epoch: 9 [32768/60000 ( 54%)]\tLoss: 62.607033\n",
      "Train Epoch: 9 [33280/60000 ( 55%)]\tLoss: 64.718552\n",
      "Train Epoch: 9 [33792/60000 ( 56%)]\tLoss: 64.203987\n",
      "Train Epoch: 9 [34304/60000 ( 57%)]\tLoss: 57.775150\n",
      "Train Epoch: 9 [34816/60000 ( 58%)]\tLoss: 63.798691\n",
      "Train Epoch: 9 [35328/60000 ( 59%)]\tLoss: 63.659180\n",
      "Train Epoch: 9 [35840/60000 ( 60%)]\tLoss: 64.352539\n",
      "Train Epoch: 9 [36352/60000 ( 60%)]\tLoss: 62.291237\n",
      "Train Epoch: 9 [36864/60000 ( 61%)]\tLoss: 62.031242\n",
      "Train Epoch: 9 [37376/60000 ( 62%)]\tLoss: 58.963150\n",
      "Train Epoch: 9 [37888/60000 ( 63%)]\tLoss: 61.516518\n",
      "Train Epoch: 9 [38400/60000 ( 64%)]\tLoss: 62.656403\n",
      "Train Epoch: 9 [38912/60000 ( 65%)]\tLoss: 63.282333\n",
      "Train Epoch: 9 [39424/60000 ( 66%)]\tLoss: 58.951790\n",
      "Train Epoch: 9 [39936/60000 ( 66%)]\tLoss: 61.252876\n",
      "Train Epoch: 9 [40448/60000 ( 67%)]\tLoss: 63.897224\n",
      "Train Epoch: 9 [40960/60000 ( 68%)]\tLoss: 61.538322\n",
      "Train Epoch: 9 [41472/60000 ( 69%)]\tLoss: 64.740540\n",
      "Train Epoch: 9 [41984/60000 ( 70%)]\tLoss: 62.613853\n",
      "Train Epoch: 9 [42496/60000 ( 71%)]\tLoss: 65.631683\n",
      "Train Epoch: 9 [43008/60000 ( 71%)]\tLoss: 62.068451\n",
      "Train Epoch: 9 [43520/60000 ( 72%)]\tLoss: 62.223667\n",
      "Train Epoch: 9 [44032/60000 ( 73%)]\tLoss: 61.566818\n",
      "Train Epoch: 9 [44544/60000 ( 74%)]\tLoss: 58.468994\n",
      "Train Epoch: 9 [45056/60000 ( 75%)]\tLoss: 60.843037\n",
      "Train Epoch: 9 [45568/60000 ( 76%)]\tLoss: 64.163361\n",
      "Train Epoch: 9 [46080/60000 ( 77%)]\tLoss: 63.017735\n",
      "Train Epoch: 9 [46592/60000 ( 77%)]\tLoss: 60.853546\n",
      "Train Epoch: 9 [47104/60000 ( 78%)]\tLoss: 62.073112\n",
      "Train Epoch: 9 [47616/60000 ( 79%)]\tLoss: 63.709206\n",
      "Train Epoch: 9 [48128/60000 ( 80%)]\tLoss: 63.794701\n",
      "Train Epoch: 9 [48640/60000 ( 81%)]\tLoss: 62.480583\n",
      "Train Epoch: 9 [49152/60000 ( 82%)]\tLoss: 66.331711\n",
      "Train Epoch: 9 [49664/60000 ( 83%)]\tLoss: 60.767399\n",
      "Train Epoch: 9 [50176/60000 ( 83%)]\tLoss: 58.558300\n",
      "Train Epoch: 9 [50688/60000 ( 84%)]\tLoss: 62.700790\n",
      "Train Epoch: 9 [51200/60000 ( 85%)]\tLoss: 61.439415\n",
      "Train Epoch: 9 [51712/60000 ( 86%)]\tLoss: 61.131596\n",
      "Train Epoch: 9 [52224/60000 ( 87%)]\tLoss: 61.662811\n",
      "Train Epoch: 9 [52736/60000 ( 88%)]\tLoss: 63.528603\n",
      "Train Epoch: 9 [53248/60000 ( 89%)]\tLoss: 62.540718\n",
      "Train Epoch: 9 [53760/60000 ( 89%)]\tLoss: 64.179749\n",
      "Train Epoch: 9 [54272/60000 ( 90%)]\tLoss: 61.638107\n",
      "Train Epoch: 9 [54784/60000 ( 91%)]\tLoss: 63.061806\n",
      "Train Epoch: 9 [55296/60000 ( 92%)]\tLoss: 66.575943\n",
      "Train Epoch: 9 [55808/60000 ( 93%)]\tLoss: 65.313026\n",
      "Train Epoch: 9 [56320/60000 ( 94%)]\tLoss: 63.159908\n",
      "Train Epoch: 9 [56832/60000 ( 94%)]\tLoss: 61.401787\n",
      "Train Epoch: 9 [57344/60000 ( 95%)]\tLoss: 61.288803\n",
      "Train Epoch: 9 [57856/60000 ( 96%)]\tLoss: 60.810894\n",
      "Train Epoch: 9 [58368/60000 ( 97%)]\tLoss: 64.250496\n",
      "Train Epoch: 9 [58880/60000 ( 98%)]\tLoss: 63.464321\n",
      "Train Epoch: 9 [59392/60000 ( 99%)]\tLoss: 66.461884\n",
      "Train Epoch: 9 [22464/60000 (100%)]\tLoss: 64.803274\n",
      "====> Epoch: 9 Average loss: 62.8325\n",
      "====> Test set loss: 68.4451\n",
      "Train Epoch: 10 [    0/60000 (  0%)]\tLoss: 63.135567\n",
      "Train Epoch: 10 [  512/60000 (  1%)]\tLoss: 63.428764\n",
      "Train Epoch: 10 [ 1024/60000 (  2%)]\tLoss: 62.687195\n",
      "Train Epoch: 10 [ 1536/60000 (  3%)]\tLoss: 58.605370\n",
      "Train Epoch: 10 [ 2048/60000 (  3%)]\tLoss: 67.030983\n",
      "Train Epoch: 10 [ 2560/60000 (  4%)]\tLoss: 63.996227\n",
      "Train Epoch: 10 [ 3072/60000 (  5%)]\tLoss: 59.563320\n",
      "Train Epoch: 10 [ 3584/60000 (  6%)]\tLoss: 59.114128\n",
      "Train Epoch: 10 [ 4096/60000 (  7%)]\tLoss: 64.048935\n",
      "Train Epoch: 10 [ 4608/60000 (  8%)]\tLoss: 58.663235\n",
      "Train Epoch: 10 [ 5120/60000 (  9%)]\tLoss: 62.329803\n",
      "Train Epoch: 10 [ 5632/60000 (  9%)]\tLoss: 60.300461\n",
      "Train Epoch: 10 [ 6144/60000 ( 10%)]\tLoss: 60.312637\n",
      "Train Epoch: 10 [ 6656/60000 ( 11%)]\tLoss: 60.516594\n",
      "Train Epoch: 10 [ 7168/60000 ( 12%)]\tLoss: 61.327618\n",
      "Train Epoch: 10 [ 7680/60000 ( 13%)]\tLoss: 61.162903\n",
      "Train Epoch: 10 [ 8192/60000 ( 14%)]\tLoss: 64.081604\n",
      "Train Epoch: 10 [ 8704/60000 ( 14%)]\tLoss: 62.378128\n",
      "Train Epoch: 10 [ 9216/60000 ( 15%)]\tLoss: 63.442001\n",
      "Train Epoch: 10 [ 9728/60000 ( 16%)]\tLoss: 63.312309\n",
      "Train Epoch: 10 [10240/60000 ( 17%)]\tLoss: 60.541607\n",
      "Train Epoch: 10 [10752/60000 ( 18%)]\tLoss: 61.950844\n",
      "Train Epoch: 10 [11264/60000 ( 19%)]\tLoss: 63.501801\n",
      "Train Epoch: 10 [11776/60000 ( 20%)]\tLoss: 59.789101\n",
      "Train Epoch: 10 [12288/60000 ( 20%)]\tLoss: 63.296040\n",
      "Train Epoch: 10 [12800/60000 ( 21%)]\tLoss: 61.619572\n",
      "Train Epoch: 10 [13312/60000 ( 22%)]\tLoss: 60.283134\n",
      "Train Epoch: 10 [13824/60000 ( 23%)]\tLoss: 59.253902\n",
      "Train Epoch: 10 [14336/60000 ( 24%)]\tLoss: 60.075142\n",
      "Train Epoch: 10 [14848/60000 ( 25%)]\tLoss: 61.058586\n",
      "Train Epoch: 10 [15360/60000 ( 26%)]\tLoss: 61.336784\n",
      "Train Epoch: 10 [15872/60000 ( 26%)]\tLoss: 62.877300\n",
      "Train Epoch: 10 [16384/60000 ( 27%)]\tLoss: 64.003998\n",
      "Train Epoch: 10 [16896/60000 ( 28%)]\tLoss: 59.602692\n",
      "Train Epoch: 10 [17408/60000 ( 29%)]\tLoss: 64.037018\n",
      "Train Epoch: 10 [17920/60000 ( 30%)]\tLoss: 59.390282\n",
      "Train Epoch: 10 [18432/60000 ( 31%)]\tLoss: 64.640144\n",
      "Train Epoch: 10 [18944/60000 ( 31%)]\tLoss: 63.453499\n",
      "Train Epoch: 10 [19456/60000 ( 32%)]\tLoss: 60.337200\n",
      "Train Epoch: 10 [19968/60000 ( 33%)]\tLoss: 59.647270\n",
      "Train Epoch: 10 [20480/60000 ( 34%)]\tLoss: 65.519310\n",
      "Train Epoch: 10 [20992/60000 ( 35%)]\tLoss: 62.976810\n",
      "Train Epoch: 10 [21504/60000 ( 36%)]\tLoss: 60.685730\n",
      "Train Epoch: 10 [22016/60000 ( 37%)]\tLoss: 59.833935\n",
      "Train Epoch: 10 [22528/60000 ( 37%)]\tLoss: 64.570671\n",
      "Train Epoch: 10 [23040/60000 ( 38%)]\tLoss: 62.719383\n",
      "Train Epoch: 10 [23552/60000 ( 39%)]\tLoss: 63.748528\n",
      "Train Epoch: 10 [24064/60000 ( 40%)]\tLoss: 65.730614\n",
      "Train Epoch: 10 [24576/60000 ( 41%)]\tLoss: 62.323738\n",
      "Train Epoch: 10 [25088/60000 ( 42%)]\tLoss: 61.716499\n",
      "Train Epoch: 10 [25600/60000 ( 43%)]\tLoss: 63.963341\n",
      "Train Epoch: 10 [26112/60000 ( 43%)]\tLoss: 58.601818\n",
      "Train Epoch: 10 [26624/60000 ( 44%)]\tLoss: 63.727814\n",
      "Train Epoch: 10 [27136/60000 ( 45%)]\tLoss: 61.898232\n",
      "Train Epoch: 10 [27648/60000 ( 46%)]\tLoss: 60.760139\n",
      "Train Epoch: 10 [28160/60000 ( 47%)]\tLoss: 61.427723\n",
      "Train Epoch: 10 [28672/60000 ( 48%)]\tLoss: 63.293198\n",
      "Train Epoch: 10 [29184/60000 ( 49%)]\tLoss: 60.734421\n",
      "Train Epoch: 10 [29696/60000 ( 49%)]\tLoss: 64.445007\n",
      "Train Epoch: 10 [30208/60000 ( 50%)]\tLoss: 63.206444\n",
      "Train Epoch: 10 [30720/60000 ( 51%)]\tLoss: 60.395088\n",
      "Train Epoch: 10 [31232/60000 ( 52%)]\tLoss: 61.809101\n",
      "Train Epoch: 10 [31744/60000 ( 53%)]\tLoss: 62.644657\n",
      "Train Epoch: 10 [32256/60000 ( 54%)]\tLoss: 62.282494\n",
      "Train Epoch: 10 [32768/60000 ( 54%)]\tLoss: 60.476662\n",
      "Train Epoch: 10 [33280/60000 ( 55%)]\tLoss: 63.848251\n",
      "Train Epoch: 10 [33792/60000 ( 56%)]\tLoss: 63.436848\n",
      "Train Epoch: 10 [34304/60000 ( 57%)]\tLoss: 61.114021\n",
      "Train Epoch: 10 [34816/60000 ( 58%)]\tLoss: 61.648987\n",
      "Train Epoch: 10 [35328/60000 ( 59%)]\tLoss: 59.331661\n",
      "Train Epoch: 10 [35840/60000 ( 60%)]\tLoss: 64.604889\n",
      "Train Epoch: 10 [36352/60000 ( 60%)]\tLoss: 62.502228\n",
      "Train Epoch: 10 [36864/60000 ( 61%)]\tLoss: 60.132385\n",
      "Train Epoch: 10 [37376/60000 ( 62%)]\tLoss: 62.257645\n",
      "Train Epoch: 10 [37888/60000 ( 63%)]\tLoss: 62.548431\n",
      "Train Epoch: 10 [38400/60000 ( 64%)]\tLoss: 63.664688\n",
      "Train Epoch: 10 [38912/60000 ( 65%)]\tLoss: 62.646580\n",
      "Train Epoch: 10 [39424/60000 ( 66%)]\tLoss: 61.059853\n",
      "Train Epoch: 10 [39936/60000 ( 66%)]\tLoss: 61.311146\n",
      "Train Epoch: 10 [40448/60000 ( 67%)]\tLoss: 61.690224\n",
      "Train Epoch: 10 [40960/60000 ( 68%)]\tLoss: 63.068504\n",
      "Train Epoch: 10 [41472/60000 ( 69%)]\tLoss: 61.169502\n",
      "Train Epoch: 10 [41984/60000 ( 70%)]\tLoss: 61.331818\n",
      "Train Epoch: 10 [42496/60000 ( 71%)]\tLoss: 58.100605\n",
      "Train Epoch: 10 [43008/60000 ( 71%)]\tLoss: 65.078644\n",
      "Train Epoch: 10 [43520/60000 ( 72%)]\tLoss: 59.577980\n",
      "Train Epoch: 10 [44032/60000 ( 73%)]\tLoss: 61.323631\n",
      "Train Epoch: 10 [44544/60000 ( 74%)]\tLoss: 62.371197\n",
      "Train Epoch: 10 [45056/60000 ( 75%)]\tLoss: 61.544655\n",
      "Train Epoch: 10 [45568/60000 ( 76%)]\tLoss: 60.937157\n",
      "Train Epoch: 10 [46080/60000 ( 77%)]\tLoss: 58.264694\n",
      "Train Epoch: 10 [46592/60000 ( 77%)]\tLoss: 60.532021\n",
      "Train Epoch: 10 [47104/60000 ( 78%)]\tLoss: 61.347839\n",
      "Train Epoch: 10 [47616/60000 ( 79%)]\tLoss: 61.410130\n",
      "Train Epoch: 10 [48128/60000 ( 80%)]\tLoss: 58.474709\n",
      "Train Epoch: 10 [48640/60000 ( 81%)]\tLoss: 59.908020\n",
      "Train Epoch: 10 [49152/60000 ( 82%)]\tLoss: 62.468231\n",
      "Train Epoch: 10 [49664/60000 ( 83%)]\tLoss: 62.845795\n",
      "Train Epoch: 10 [50176/60000 ( 83%)]\tLoss: 62.140545\n",
      "Train Epoch: 10 [50688/60000 ( 84%)]\tLoss: 59.895973\n",
      "Train Epoch: 10 [51200/60000 ( 85%)]\tLoss: 62.485977\n",
      "Train Epoch: 10 [51712/60000 ( 86%)]\tLoss: 62.279282\n",
      "Train Epoch: 10 [52224/60000 ( 87%)]\tLoss: 62.363213\n",
      "Train Epoch: 10 [52736/60000 ( 88%)]\tLoss: 64.738045\n",
      "Train Epoch: 10 [53248/60000 ( 89%)]\tLoss: 62.589798\n",
      "Train Epoch: 10 [53760/60000 ( 89%)]\tLoss: 57.343895\n",
      "Train Epoch: 10 [54272/60000 ( 90%)]\tLoss: 61.598259\n",
      "Train Epoch: 10 [54784/60000 ( 91%)]\tLoss: 59.747833\n",
      "Train Epoch: 10 [55296/60000 ( 92%)]\tLoss: 60.066933\n",
      "Train Epoch: 10 [55808/60000 ( 93%)]\tLoss: 64.496689\n",
      "Train Epoch: 10 [56320/60000 ( 94%)]\tLoss: 62.673435\n",
      "Train Epoch: 10 [56832/60000 ( 94%)]\tLoss: 59.607048\n",
      "Train Epoch: 10 [57344/60000 ( 95%)]\tLoss: 60.817558\n",
      "Train Epoch: 10 [57856/60000 ( 96%)]\tLoss: 61.457924\n",
      "Train Epoch: 10 [58368/60000 ( 97%)]\tLoss: 62.965553\n",
      "Train Epoch: 10 [58880/60000 ( 98%)]\tLoss: 62.269264\n",
      "Train Epoch: 10 [59392/60000 ( 99%)]\tLoss: 60.529984\n",
      "Train Epoch: 10 [22464/60000 (100%)]\tLoss: 61.019404\n",
      "====> Epoch: 10 Average loss: 61.9720\n",
      "====> Test set loss: 62.6416\n"
     ]
    }
   ],
   "source": [
    "parser = create_argument_parser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Create configuration\n",
    "config = Config(\n",
    "    batch_size=args.batch_size,\n",
    "    epochs=args.epochs,\n",
    "    learning_rate=args.learning_rate,\n",
    "    no_cuda=args.no_cuda,\n",
    "    seed=args.seed,\n",
    "    log_interval=args.log_interval,\n",
    "    category=args.category,\n",
    "    alpha=args.alpha,\n",
    "    data_dir=args.data_dir,\n",
    "    output_dir=args.output_dir,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Setup device and random seeds\n",
    "    device = setup_device_and_seed(config)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader, test_loader = create_data_loaders(config)\n",
    "\n",
    "    # Create model\n",
    "    model = DirVAE(config, device).to(device)\n",
    "    print(\n",
    "        f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\"\n",
    "    )\n",
    "\n",
    "    # Create trainer and start training\n",
    "    trainer = DirVAETrainer(model, config, device, train_loader, test_loader)\n",
    "    trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\", file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffc34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
