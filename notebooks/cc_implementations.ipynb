{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c974a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Uniform\n",
    "import numpy as np\n",
    "EPS = 1e-8\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            last = h\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        # output positive concentration parameters \\hat{alpha} for each latent dim\n",
    "        self.alpha_layer = nn.Linear(last, latent_dim)\n",
    "        self.mu_layer = nn.Linear(last, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(last, latent_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        # softplus to ensure positive alpha_hat; add small bias to avoid zero\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        alpha_hat = F.softplus(self.alpha_layer(h)) + 1e-6\n",
    "        alpha_hat = alpha_hat.clamp(min=1e-3, max=50.0)\n",
    "        return alpha_hat, mu, logvar\n",
    "\n",
    "class BernoulliDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = latent_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, z):\n",
    "        logits = self.net(z)\n",
    "        # return logits (use BCEWithLogitsLoss)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CC_VAE(nn.Module):\n",
    "    def __init__(self, input_dim, enc_hidden_dims, dec_hidden_dims, latent_dim, prior_lambda=None):\n",
    "        \"\"\"\n",
    "        input_dim: flattened input size (e.g. 28*28)\n",
    "        enc_hidden_dims: list of encoder hidden sizes\n",
    "        dec_hidden_dims: list of decoder hidden sizes\n",
    "        latent_dim: K\n",
    "        prior_lambda: vector or scalar for CC prior lambda (if scalar, replicate)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = MLPEncoder(input_dim, enc_hidden_dims, latent_dim)\n",
    "        self.decoder = BernoulliDecoder(latent_dim, dec_hidden_dims, input_dim)\n",
    "        if prior_lambda is None:\n",
    "            # default weak symmetric prior; user can override\n",
    "            prior_lambda = torch.ones(latent_dim) * 0.98\n",
    "        elif torch.is_tensor(prior_lambda):\n",
    "            if prior_lambda.numel() == 1:\n",
    "                prior_lambda = prior_lambda.repeat(latent_dim)\n",
    "        else:\n",
    "            # numeric scalar\n",
    "            prior_lambda = torch.tensor(float(prior_lambda)).repeat(latent_dim)\n",
    "        self.register_buffer('prior_lambda', prior_lambda.float())\n",
    " \n",
    "    def sample_cc_from_lambda(self, lambda_hat):\n",
    "        \"\"\"\n",
    "        Ordered rejection sampler for Continuous Categorical CC(Î»)\n",
    "        Returns z (batch, K), v (batch, K), u (for reproducibility)\n",
    "        \"\"\"\n",
    "        def sample_continuous_bernoulli(theta): \n",
    "            u=torch.rand(1).item()\n",
    "            if torch.isclose(theta.detach().clone(),torch.tensor(1.0)):\n",
    "                return u\n",
    "            else:\n",
    "                return torch.log(u*(theta-1)+1)/torch.log(theta.detach().clone())\n",
    "        \n",
    "        if lambda_hat.dim()==1: \n",
    "            lambda_hat=lambda_hat.unsqueeze(0)\n",
    "            batch_size,K=lambda_hat.shape\n",
    "            out=[]\n",
    "            for b in range(batch_size):\n",
    "                lmb=lambda_hat[b]\n",
    "                order=torch.argsort(-lmb,dim=0)\n",
    "                l_sorted=lmb[order]\n",
    "                for _ in range(100000): \n",
    "                    x=torch.zeros(K)\n",
    "                    c=0.0\n",
    "                    i=1\n",
    "                    while c<1 and i<K: \n",
    "                        theta=l_sorted[i]/(l_sorted[i]+l_sorted[0])\n",
    "                        x[i]=sample_continuous_bernoulli(theta)\n",
    "                        c+=x[i].item()\n",
    "                        i+=1\n",
    "                        if c<=1: \n",
    "                            x[0]=1-torch.sum(x[1:])\n",
    "                            x=x[torch.argsort(order)]\n",
    "                            out.append(x)\n",
    "                            break\n",
    "                        if len(out)==b+1: break\n",
    "                    else: raise RuntimeError(\"Sampler failed to converge\")\n",
    "                return torch.stack(out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: flattened input (batch, input_dim) with values in [0,1] for Bernoulli decoding\n",
    "        returns: reconstruction logits, z, lambda_hat\n",
    "        \"\"\"\n",
    "        lambda_hat = self.encoder(x)  # (batch, K)\n",
    "        z = self.sample_cc_from_lambda(lambda_hat)\n",
    "        logits = self.decoder(z)  # (batch, input_dim)\n",
    "        return logits, z, lambda_hat\n",
    "    \n",
    "    \n",
    "def multi_gamma_kl(alpha_hat, prior_alpha, reduction='batchmean'):\n",
    "    \"\"\"\n",
    "    KL between MultiGamma(alpha_hat, beta=1) and MultiGamma(prior_alpha, beta=1)\n",
    "    Per paper (Equation 3):\n",
    "      KL(Q||P) = sum_k [ log Gamma(alpha_k) - log Gamma(alpha_hat_k) + (alpha_hat_k - alpha_k) * psi(alpha_hat_k) ]\n",
    "    alpha_hat: (batch, K)\n",
    "    prior_alpha: (K,) or (batch, K)\n",
    "    reduction: 'batchmean', 'sum', 'none'\n",
    "    Returns scalar KL (averaged over batch if batchmean)\n",
    "    \"\"\"\n",
    "    # broadcast prior_alpha to batch if necessary\n",
    "    if prior_alpha.dim() == 1:\n",
    "        prior = prior_alpha.unsqueeze(0).expand_as(alpha_hat)\n",
    "    else:\n",
    "        prior = prior_alpha\n",
    "    term1 = torch.lgamma(prior) - torch.lgamma(alpha_hat)\n",
    "    term2 = (alpha_hat - prior) * torch.digamma(alpha_hat)\n",
    "    kl_comp = term1 + term2\n",
    "    kl = kl_comp.sum(dim=1)  # per example sum over K\n",
    "    if reduction == 'batchmean':\n",
    "        return kl.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return kl.sum()\n",
    "    else:\n",
    "        return kl  # per example\n",
    "    \n",
    "    \n",
    "def dirvae_elbo_loss(model, x, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Compute negative ELBO (loss to minimize) for Bernoulli decoder.\n",
    "    x: (batch, input_dim) values in {0,1} or [0,1]\n",
    "    returns loss (scalar), recon_loss (scalar), kl (scalar)\n",
    "    \"\"\"\n",
    "    logits, z, alpha_hat, v = model(x)\n",
    "    # Reconstruction: bernoulli likelihood -> BCEWithLogits\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, x, reduction='none')\n",
    "    recon_per_sample = bce.sum(dim=1)  # per example reconstruction negative log-likelihood\n",
    "    if reduction == 'mean':\n",
    "        recon_loss = recon_per_sample.mean()\n",
    "    else:\n",
    "        recon_loss = recon_per_sample.sum()\n",
    "    # KL between MultiGamma post (alpha_hat) and prior MultiGamma (prior_alpha)\n",
    "    kl = multi_gamma_kl(alpha_hat, model.prior_alpha, reduction='batchmean')\n",
    "    # ELBO = E_q[log p(x|z)] - KL -> loss = -ELBO = recon_loss + KL\n",
    "    loss = recon_loss + kl\n",
    "    return loss, recon_loss, kl\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example training loop skeleton\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick usage example for MNIST-like data (flattened, binary)\n",
    "    import torchvision\n",
    "    import torchvision.transforms as T\n",
    "    from torch.utils.data import DataLoader\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Simple binarization transform\n",
    "    #transform = T.Compose([T.ToTensor(), lambda t: (t > 0.5).float(), lambda t: t.view(-1)])\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "    loader = DataLoader(trainset, batch_size=100, shuffle=True, num_workers=0)\n",
    "    input_dim = 28 * 28\n",
    "    latent_dim = 50\n",
    "\n",
    "\n",
    "\n",
    "    model_DIR = DirVAE(input_dim=input_dim,\n",
    "                   enc_hidden_dims=[500,500],\n",
    "                   dec_hidden_dims=[500],\n",
    "                   latent_dim=latent_dim,\n",
    "                   prior_alpha=0.98).to(device)\n",
    "    optimizer_DIR = torch.optim.Adam(model_DIR.parameters(), lr=1e-3)\n",
    "    for epoch in range(1, 50):\n",
    "        model_DIR.train()\n",
    "        tot_loss = 0.0\n",
    "        tot_recon = 0.0\n",
    "        tot_kl = 0.0\n",
    "        samlet=0\n",
    "\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            optimizer_DIR.zero_grad()\n",
    "            loss, recon, kl = dirvae_elbo_loss(model_DIR, xb, reduction='mean')\n",
    "            loss.backward()\n",
    "            optimizer_DIR.step()\n",
    "            tot_loss += loss.item() * xb.size(0)\n",
    "            tot_recon += recon.item() * xb.size(0)\n",
    "            tot_kl += kl.item() * xb.size(0)\n",
    "\n",
    "        n = len(loader.dataset)\n",
    "        print(f\"Epoch {epoch:02d} DIR Loss {tot_loss/n:.4f} Recon {tot_recon/n:.4f} KL {tot_kl/n:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
