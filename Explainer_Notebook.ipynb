{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce47b7d",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Variational Autoencoders With Alternative Bottlenecks](#toc1_)    \n",
    "- [Project Overview](#toc2_)    \n",
    "- [Hydra Configuration System](#toc3_)    \n",
    "      - [Main training configuration:](#toc3_1_1_1_)    \n",
    "      - [Hyperparameter sweep setup:](#toc3_1_1_2_)    \n",
    "- [Data Pipeline](#toc4_)    \n",
    "- [Model Implementations](#toc5_)    \n",
    "  - [Shared blocks](#toc5_1_)    \n",
    "  - [VAE classes](#toc5_2_)    \n",
    "  - [Loss functions](#toc5_3_)    \n",
    "- [Training Pipeline](#toc6_)    \n",
    "- [Evaluation Script](#toc7_)    \n",
    "  - [Multi-model Comparison](#toc7_1_)    \n",
    "  - [Visualization Utilities](#toc7_2_)    \n",
    "- [Final Findings](#toc8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cd7e5",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Variational Autoencoders With Alternative Bottlenecks](#toc0_)\n",
    "Authors:\n",
    "| Name | Student ID |\n",
    "|------|------------|\n",
    "| Olivia Sommer Droob | S214696 |\n",
    "| Karoline Klan Hansen | S214638 |\n",
    "| Martha Falkesgaard Nybroe | S214692 |\n",
    "| Signe Djernis Olsen | S206759 |\n",
    "| Bella Strandfort | S214205 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c929c4a",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Project Overview](#toc0_)\n",
    "\n",
    "This repository contains the implementation for our Deep Learning project at DTU (course 02456), where we investigate how different latent bottlenecks affect the behaviour of a Variational Autoencoder (VAE).\n",
    "Specifically, we compare:\n",
    "\n",
    "- Gaussian VAE (standard)\n",
    "- Dirichlet VAE (simplex-constrained latent space)\n",
    "- Continuous Categorical (CC) VAE (a newer exponential-family simplex distribution)\n",
    "\n",
    "\n",
    "![image](project_images/VAE_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645576a3",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Hydra Configuration System](#toc0_)\n",
    "Hydra is used to keep all experiment settings clean, centralized, and easy to override from the command line.  \n",
    "In this project, two configuration files control how training runs: [`base_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/base_config.yaml) and [`wandb_sweep_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/wandb_sweep_config.yaml).\n",
    "\n",
    "#### <a id='toc3_1_1_1_'></a>[Main training configuration:](#toc0_)\n",
    "[`configs/base_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/base_config.yaml)\n",
    "\n",
    "This is the default configuration used when running:\n",
    "\n",
    "```bash\n",
    "python -m src.deep_proj.train\n",
    "```\n",
    "\n",
    "It defines:\n",
    "\n",
    "- Weights and Biases (WandB) settings\n",
    "    - Whether logging is enabled, project name, entity, grouping of runs.\n",
    "\n",
    "- Dataset setup\n",
    "    - Choose mnist or medmnist, data path, batch size, validation split, and optional class filtering\n",
    "\n",
    "- Model configuration\n",
    "    - Select bottleneck type (gaussian, dirichlet, cc), latent dimension, and model-specific parameters.\n",
    "\n",
    "- Training hyperparameters\n",
    "    - Learning rate, number of epochs, GPU/CPU selection, early stopping.\n",
    "\n",
    "- Output structure\n",
    "\n",
    "#### <a id='toc3_1_1_2_'></a>[Hyperparameter sweep setup:](#toc0_)\n",
    "[`configs/wandb_sweep_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/wandb_sweep_config.yaml)\n",
    "\n",
    "This file defines a WandB grid search sweep, allowing us to automatically try multiple combinations of:\n",
    "- datasets\n",
    "- model types (gaussian, diriclet and cc)\n",
    "- learning rates\n",
    "- latent dimensions\n",
    "\n",
    "To launch the sweep run following from the terminal:\n",
    "```bash\n",
    "wandb sweep configs/wandb_sweep_config.yaml\n",
    "wandb agent <sweep_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579662b",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Data Pipeline](#toc0_)\n",
    "\n",
    "In the datascript found in [`src/deep_proj/data.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/data.py), we make sure to handle both **MNIST** and **MedMNIST** datasets in a unified way.  \n",
    "\n",
    "The script provides:\n",
    "- **Dataset builders**:\n",
    "  - `_build_mnist` - Loads MNIST with standard normalization ()  and optionally filters the dataset to only include user-specified classes loaded from the config file (`mnist_classes=[0,1,...]`).\n",
    "  - `_build_medmnist`  \n",
    "    Loads MedMNIST subsets using metadata from the `medmnist` package and applies appropriate normalization.\n",
    "\n",
    "- **Dataloader construction** via `get_dataloaders`:\n",
    "  - Selects which dataset loader to use (`mnist` or `medmnist`)\n",
    "  - Splits the training set deterministically into **train/val** subsets based on `val_split`\n",
    "  - Wraps everything into PyTorch `DataLoader` objects for training, validation, and testing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e9258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MNIST ===\n",
      "[MNIST FILTER] Selected classes: {0, 1, 4}\n",
      "[MNIST FILTER] Train samples: 18507\n",
      "[MNIST FILTER] Test samples:  3097\n",
      "\n",
      "=== MedMNIST (subset: organcmnist ) ===\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Train samples: 12975\n",
      "Test samples: 8216\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from src.deep_proj.data import _build_mnist, _build_medmnist\n",
    "\n",
    "# Load base config (we will override the dataset field manually)\n",
    "cfg = OmegaConf.load(\"configs/base_config.yaml\")\n",
    "\n",
    "# -------------------------\n",
    "# MNIST\n",
    "# -------------------------\n",
    "cfg.dataset = \"mnist\"\n",
    "cfg.mnist_classes = [0,1,4]   # only include the 3 subclasses \n",
    "\n",
    "print(\"=== MNIST ===\")\n",
    "mnist_train, mnist_test = _build_mnist(cfg)\n",
    "\n",
    "print()\n",
    "\n",
    "# -------------------------\n",
    "# MedMNIST\n",
    "# -------------------------\n",
    "print(\"=== MedMNIST (subset:\", cfg.medmnist_subset, \") ===\")\n",
    "cfg.dataset = \"medmnist\"\n",
    "cfg.medmnist_subset = \"organcmnist\"   # change if needed\n",
    "cfg.medmnist_classes = [0,1,2]  # only include the 3 organ classes\n",
    "\n",
    "med_train, med_test = _build_medmnist(cfg)\n",
    "\n",
    "\n",
    "print(\"Train samples:\", len(med_train))\n",
    "print(\"Test samples:\", len(med_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f8ff9",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Model Implementations](#toc0_)\n",
    "The [`src/deep_proj/model.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/model.py) collects all neural network models and ELBO loss functions used in the project. The key idea is that all three VAEs share the same encoder/decoder architecture but differ in the latent bottleneck distribution.\n",
    "\n",
    "## <a id='toc5_1_'></a>[Shared blocks](#toc0_)\n",
    "\n",
    "- `MLPEncoder`  \n",
    "  A fully connected network that maps a flattened image to latent parameters.  \n",
    "  It behaves differently depending on `bottle`:\n",
    "  - `\"gaus\"` – outputs mean `μ` and log-variance `logvar` for a Gaussian latent.\n",
    "  - `\"dir\"` – outputs positive concentration parameters `α̂` for a Dirichlet latent (clamped to a safe range).\n",
    "  - `\"cc\"` – outputs positive parameters `λ̂` for the Continuous Categorical latent.\n",
    "\n",
    "- `BernoulliDecoder`  \n",
    "  A mirrored MLP that maps a latent vector back to image logits.  \n",
    "  The last layer outputs raw logits which are used with `BCEWithLogitsLoss` to model Bernoulli pixels.\n",
    "\n",
    "## <a id='toc5_2_'></a>[VAE classes](#toc0_)\n",
    "\n",
    "- `GaussianVAE`  \n",
    "  Standard VAE:\n",
    "  1. Encoder produces `μ` and `logvar`.\n",
    "  2. `reparameterize` draws `z = μ + σ ⊙ ε` with `ε ~ N(0, I)`.\n",
    "  3. Decoder maps `z` to reconstruction logits.\n",
    "\n",
    "- `DirVAE`  \n",
    "  Uses a Dirichlet latent:\n",
    "  1. Encoder outputs `α̂` (Dirichlet parameters).\n",
    "  2. Samples are generated via an approximate inverse Gamma CDF, then normalized to the simplex.\n",
    "  3. The prior is another Dirichlet with concentration `prior_alpha`.\n",
    "  4. `dirvae_elbo_loss` combines a Bernoulli reconstruction term with a KL term based on the MultiGamma representation of the Dirichlet.\n",
    "\n",
    "- `CCVAE`  \n",
    "  Uses the Continuous Categorical distribution on the simplex:\n",
    "  1. Encoder outputs `λ̂`, which is normalized to a mean vector `λ` on the simplex.\n",
    "  2. `sample_cc_ordered_reparam` implements a differentiable sampler that returns `z` on the simplex.\n",
    "  3. A uniform prior over the simplex is encoded as `prior_lambda` which is just flat (`torch.ones(latent_dim)`).\n",
    "  4. `ccvae_elbo_loss` uses a Bernoulli reconstruction term and a CC-specific KL term (`cc_kl`) based on natural parameters `η` and the log-normalizing constant `log C(η)`.\n",
    "\n",
    "## <a id='toc5_3_'></a>[Loss functions](#toc0_)\n",
    "\n",
    "Each model has a corresponding ELBO helper:\n",
    "\n",
    "- `gaussian_vae_elbo_loss(model, x)`  \n",
    "  Standard VAE ELBO with analytic Gaussian KL.\n",
    "\n",
    "- `dirvae_elbo_loss(model, x)`  \n",
    "  ELBO for the Dirichlet VAE using the MultiGamma KL.\n",
    "\n",
    "- `ccvae_elbo_loss(model, x)`  \n",
    "  ELBO for the CC-VAE using the CC KL based on natural parameters and the normalizing constant.\n",
    "\n",
    "Together, these components let us isolate the effect of changing only the latent distribution while keeping the encoder/decoder architecture and reconstruction loss fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e67265f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "              Gaussian VAE (Standard Normal Bottleneck)               \n",
      "======================================================================\n",
      "μ (mean) for first sample:\n",
      "  [0.006777741014957428, -0.018862128257751465, 0.0059876032173633575, 0.008925219997763634, 0.05221676081418991, -0.009353311732411385, -0.04312464967370033, 0.002020535059273243, 0.07870528101921082, -0.02894168347120285]\n",
      "\n",
      "σ² (variance) for first sample:\n",
      "  [1.050595998764038, 0.9927116632461548, 0.977631151676178, 0.9883387684822083, 1.0319002866744995, 0.9219326376914978, 1.057778239250183, 1.007879614830017, 1.019608974456787, 1.0233640670776367]\n",
      "\n",
      "KL( q(z|x) || N(0, I) ):   0.0185\n",
      "\n",
      "======================================================================\n",
      "                 Dirichlet VAE (Simplex Constrained)                  \n",
      "======================================================================\n",
      "α̂ (concentration) for first sample:\n",
      "  [0.6840500831604004, 0.7139037251472473, 0.7407388091087341, 0.7028700709342957, 0.6711534857749939, 0.6853014826774597, 0.7448384165763855, 0.6747437715530396, 0.6862039566040039, 0.7255652546882629]\n",
      "\n",
      "Sample z on simplex:\n",
      "  sum(z) = 1.0000,   min(z) = 0.0001\n",
      "\n",
      "KL( Dirichlet posterior || prior ):   0.9251\n",
      "\n",
      "======================================================================\n",
      "              Continuous Categorical VAE (CC Bottleneck)              \n",
      "======================================================================\n",
      "λ (mean vector) for first sample:\n",
      "  [0.0914432480931282, 0.1032538115978241, 0.10222785174846649, 0.10302983224391937, 0.09933216869831085, 0.09695427864789963, 0.10025377571582794, 0.0992736667394638, 0.09928561002016068, 0.10494574904441833]\n",
      "  sum(λ) = 1.0000\n",
      "\n",
      "Sample z on simplex:\n",
      "  sum(z) = 1.0000,   min(z) = 0.0000\n",
      "\n",
      "KL( CC posterior || prior ):          59.0756\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.deep_proj.model import (\n",
    "    GaussianVAE, DirVAE, CCVAE,\n",
    "    gaussian_vae_elbo_loss, dirvae_elbo_loss, ccvae_elbo_loss\n",
    ")\n",
    "\n",
    "# Setup\n",
    "input_dim = 28 * 28\n",
    "enc_hidden = [500, 500]\n",
    "dec_hidden = [500]\n",
    "latent_dim = 10\n",
    "\n",
    "x = torch.rand(8, input_dim)\n",
    "\n",
    "gaus = GaussianVAE(input_dim, enc_hidden, dec_hidden, latent_dim)\n",
    "dirv = DirVAE(input_dim, enc_hidden, dec_hidden, latent_dim)\n",
    "ccv  = CCVAE(input_dim, enc_hidden, dec_hidden, latent_dim)\n",
    "\n",
    "\n",
    "def pretty(title):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{title}\".center(70))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Gaussian VAE\n",
    "# ------------------------------------------------------------------------\n",
    "pretty(\"Gaussian VAE (Standard Normal Bottleneck)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_g, mu, logvar, z_g = gaus(x)\n",
    "    _, _, kl_g = gaussian_vae_elbo_loss(gaus, x)\n",
    "\n",
    "print(\"μ (mean) for first sample:\")\n",
    "print(\" \", mu[0].tolist())\n",
    "print(\"\\nσ² (variance) for first sample:\")\n",
    "print(\" \", logvar[0].exp().tolist())\n",
    "\n",
    "print(f\"\\nKL( q(z|x) || N(0, I) ):   {kl_g.item():.4f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Dirichlet VAE\n",
    "# ------------------------------------------------------------------------\n",
    "pretty(\"Dirichlet VAE (Simplex Constrained)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_d, z_d, alpha_hat, v = dirv(x)\n",
    "    _, _, kl_d = dirvae_elbo_loss(dirv, x)\n",
    "\n",
    "a0 = alpha_hat[0]\n",
    "z0 = z_d[0]\n",
    "\n",
    "print(\"α̂ (concentration) for first sample:\")\n",
    "print(\" \", a0.tolist())\n",
    "\n",
    "print(\"\\nSample z on simplex:\")\n",
    "print(f\"  sum(z) = {z0.sum().item():.4f},   min(z) = {z0.min().item():.4f}\")\n",
    "\n",
    "print(f\"\\nKL( Dirichlet posterior || prior ):   {kl_d.item():.4f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# CC VAE\n",
    "# ------------------------------------------------------------------------\n",
    "pretty(\"Continuous Categorical VAE (CC Bottleneck)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_c, z_c, lam = ccv(x)\n",
    "    _, _, kl_c = ccvae_elbo_loss(ccv, x)\n",
    "\n",
    "lam0 = lam[0]\n",
    "z0c = z_c[0]\n",
    "\n",
    "print(\"λ (mean vector) for first sample:\")\n",
    "print(\" \", lam0.tolist())\n",
    "print(f\"  sum(λ) = {lam0.sum().item():.4f}\")\n",
    "\n",
    "print(\"\\nSample z on simplex:\")\n",
    "print(f\"  sum(z) = {z0c.sum().item():.4f},   min(z) = {z0c.min().item():.4f}\")\n",
    "\n",
    "print(f\"\\nKL( CC posterior || prior ):          {kl_c.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b2110",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Training Pipeline](#toc0_)\n",
    "\n",
    "The training script is found in [`src/deep_proj/train.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/train.py).\n",
    "\n",
    "It does the following:\n",
    "1. Loads config and picks device  \n",
    "   Hydra loads `base_config.yaml`, and `get_device(cfg)` chooses GPU if available (or CPU otherwise).\n",
    "\n",
    "2. Builds dataloaders and model  \n",
    "   Using `get_dataloaders(cfg)` and the `model_name` in the config, the script creates one of:\n",
    "   - `GaussianVAE`\n",
    "   - `DirVAE`\n",
    "   - `CCVAE`  \n",
    "   together with the matching ELBO loss function.\n",
    "\n",
    "3. Sets up optimizer, early stopping, and logging \n",
    "   - Adam optimizer  \n",
    "   - `EarlyStopping` based on validation loss  \n",
    "   - Optional Weights & Biases logging and checkpoint directory under `models/`.\n",
    "\n",
    "The core of the script is the epoch loop:\n",
    "```python\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    tot_loss = tot_recon = tot_kl = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for xb, _ in train_loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # undo normalization - pixels back in [0,1]\n",
    "        if cfg.dataset.lower() == \"mnist\":\n",
    "            xb = xb * 0.3081 + 0.1307\n",
    "        elif cfg.dataset.lower() == \"medmnist\":\n",
    "            xb = xb * 0.5 + 0.5\n",
    "\n",
    "        xb = xb.view(xb.size(0), -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, recon, kl = loss_fn(model, xb, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        n += bs\n",
    "        tot_loss += loss.item() * bs\n",
    "        tot_recon += recon.item() * bs\n",
    "        tot_kl += kl.item() * bs\n",
    "```\n",
    "\n",
    "Key points:\n",
    "- Inputs are unnormalized back to [0,1] before the ELBO, so the Bernoulli likelihood is consistent.\n",
    "- The chosen ELBO loss returns total loss, reconstruction term, and KL term.\n",
    "- Gradients are clipped (max_norm=5) for stability.\n",
    "\n",
    "After each epoch, the script:\n",
    "- Runs a validation pass with evaluate_split(...).\n",
    "- Prints train/val losses and updates history.\n",
    "- Saves a “best” checkpoint if the validation loss improves.\n",
    "- Optionally logs metrics and visualizations to WandB.\n",
    "- Checks early stopping; if there is no improvement for 10 epochs, training stops.\n",
    "\n",
    "For full details see the script in [`src/deep_proj/train.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/train.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871198e",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Evaluation Script](#toc0_)\n",
    "We evaluate the trained models using the [`src/deep_proj/evaluate.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/evaluate.py). It takes a single checkpoint file and rebuilds everything needed to evaluate it.\n",
    "\n",
    "It also saves:\n",
    "- A simplex projection of the latent space (plot_latent_simplex)\n",
    "- A t-SNE latent plot (plot_latent)\n",
    "- A reconstruction grid (plot_recons)\n",
    "\n",
    "All plots are stored under:\n",
    "`reports/figures/<run_id>/evaluation/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c5569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KAROLINE/opt/miniconda3/envs/deep/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MNIST FILTER] Selected classes: {0, 1}\n",
      "[MNIST FILTER] Train samples: 12665\n",
      "[MNIST FILTER] Test samples:  2115\n",
      "Validation size: 1266\n",
      "Test size:       2115\n",
      "\n",
      "=== Final model evaluation ===\n",
      "Validation | Loss 108.5586 | Recon 96.2061 | KL 12.3525\n",
      "Test       | Loss 106.6115 | Recon 94.1676 | KL 12.4439\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from src.deep_proj.evaluate import build_model_from_config\n",
    "from src.deep_proj.data import get_dataloaders\n",
    "from src.deep_proj.train import evaluate_split\n",
    "\n",
    "# Path to one of your saved checkpoints (adjust this)\n",
    "ckpt_path = \"models/mnist_gaussian_z10_lr0.0004_best.pt\" #EDIT WHEN WE HAVE THE FINAL MODELS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load checkpoint and restore config\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "cfg = DictConfig(OmegaConf.create(ckpt[\"config\"]))\n",
    "\n",
    "# Rebuild model and loss function\n",
    "model, loss_fn = build_model_from_config(cfg, device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Rebuild dataloaders\n",
    "loaders = get_dataloaders(cfg)\n",
    "val_loader = loaders[\"val\"]\n",
    "test_loader = loaders[\"test\"]\n",
    "\n",
    "print(\"Validation size:\", len(val_loader.dataset))\n",
    "print(\"Test size:      \", len(test_loader.dataset))\n",
    "\n",
    "# Evaluate\n",
    "val_loss, val_recon, val_kl = evaluate_split(model, val_loader, loss_fn, cfg, device)\n",
    "test_loss, test_recon, test_kl = evaluate_split(model, test_loader, loss_fn, cfg, device)\n",
    "\n",
    "print(\"\\n=== Final model evaluation ===\")\n",
    "print(f\"Validation | Loss {val_loss:.4f} | Recon {val_recon:.4f} | KL {val_kl:.4f}\")\n",
    "print(f\"Test       | Loss {test_loss:.4f} | Recon {test_recon:.4f} | KL {test_kl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146f9a0",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[Multi-model Comparison](#toc0_)\n",
    "\n",
    "We also create a [`src/deep_proj/evaluate_multiple.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/evaluate_multiple.py) script that \n",
    "loads three trained checkpoints (Gaussian, Dirichlet, CC model), rebuilds each model from its saved config, and evaluates them on the same MNIST test set. For each model, it:\n",
    "\n",
    "- Extracts latent representations and runs **t-SNE** to create a side-by-side latent space comparison.\n",
    "- Uses a fixed batch of test images to plot a 4 × N reconstruction grid:\n",
    "  1. Original images  \n",
    "  2. Gaussian-VAE reconstructions  \n",
    "  3. Dirichlet-VAE reconstructions  \n",
    "  4. CC-VAE reconstructions\n",
    "\n",
    "The resulting comparison figures are saved under:\n",
    "\n",
    "`reports/figures/multi_eval/latent_comparison.png` \n",
    "and  \n",
    "`reports/figures/multi_eval/reconstruction_comparison.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042cfc9",
   "metadata": {},
   "source": [
    "## <a id='toc7_2_'></a>[Visualization Utilities](#toc0_)\n",
    "\n",
    "All plotting code for the project lives in two helper modules:\n",
    "\n",
    "- **[`simplex.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/simplex.py)**  \n",
    "  Contains the tools for plotting latent simplex projections, where simplex-valued latents (Dirichlet / CC) are projected onto a polygon in 2D and overlaid with:\n",
    "  - Colored class clusters\n",
    "  - Example MNIST images placed at the simplex corners  \n",
    "  The main function is:\n",
    "  - `plot_latent_simplex(...)`\n",
    "\n",
    "- **[`visualize.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/visualize.py)**  \n",
    "  Collects the general-purpose visualization functions used during training and evaluation, including:\n",
    "  - `plot_latent(...)` – t-SNE latent space plots  \n",
    "  - `plot_recons(...)` – original vs. reconstructed image grids  \n",
    "  - `plot_side_by_side(...)` – combine two images/plots into a single figure  \n",
    "  - `plot_training_progress(...)` – side-by-side reconstructions + latent t-SNE over epochs  \n",
    "  - `plot_training_loss(...)` – training and validation loss/ KL curves  \n",
    "\n",
    "These utilities are called from `train.py`, `evaluate.py`, and `evaluate_multiple.py` to produce the figures shown in the report and in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e8e1d",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Final Findings](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174ce9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
