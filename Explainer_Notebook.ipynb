{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933c84a7",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Variational Autoencoders With Alternative Bottlenecks](#toc1_)    \n",
    "- [Project Overview](#toc2_)    \n",
    "- [Hydra Configuration System](#toc3_)    \n",
    "      - [Main training configuration:](#toc3_1_1_1_)    \n",
    "      - [Hyperparameter sweep setup:](#toc3_1_1_2_)    \n",
    "- [Data Pipeline](#toc4_)    \n",
    "- [Model Implementations](#toc5_)    \n",
    "  - [Shared blocks](#toc5_1_)    \n",
    "  - [VAE classes](#toc5_2_)    \n",
    "- [Training Pipeline](#toc6_)    \n",
    "- [Evaluation Script](#toc7_)    \n",
    "  - [Multi-model Comparison](#toc7_1_)    \n",
    "  - [Visualization Utilities](#toc7_2_)    \n",
    "- [Final Findings](#toc8_)    \n",
    "    - [The final 3 best models](#toc8_1_1_)    \n",
    "      - [Reconstruction Comparison](#toc8_1_1_1_)    \n",
    "      - [Latent space representation](#toc8_1_1_2_)    \n",
    "  - [MedMNIST Experiment](#toc8_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cd7e5",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Variational Autoencoders With Alternative Bottlenecks](#toc0_)\n",
    "Authors:\n",
    "| Name | Student ID |\n",
    "|------|------------|\n",
    "| Olivia Sommer Droob | S214696 |\n",
    "| Karoline Klan Hansen | S214638 |\n",
    "| Martha Falkesgaard Nybroe | S214692 |\n",
    "| Signe Djernis Olsen | S206759 |\n",
    "| Bella Strandfort | S214205 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c929c4a",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Project Overview](#toc0_)\n",
    "\n",
    "This repository contains the implementation for our Deep Learning project at DTU (course 02456), where we investigate how different latent bottlenecks affect the behaviour of a Variational Autoencoder (VAE).\n",
    "Specifically, we compare:\n",
    "\n",
    "- Gaussian VAE (standard)\n",
    "- Dirichlet VAE (simplex-constrained latent space)\n",
    "- Continuous Categorical (CC) VAE (a newer exponential-family simplex distribution)\n",
    "\n",
    "\n",
    "![image](project_images/VAE_figure_FINAL.png)\n",
    "\n",
    "This Explainer Notebook is to provide documentation of the code setup and the experiments run. We refer to the report for more elaborate discussion and conslusion on the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645576a3",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Hydra Configuration System](#toc0_)\n",
    "Hydra is used to keep all experiment settings clean, centralized, and easy to override from the command line.  \n",
    "In this project, two configuration files control how training runs: [`base_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/base_config.yaml) and [`wandb_sweep_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/wandb_sweep_config.yaml).\n",
    "\n",
    "#### <a id='toc3_1_1_1_'></a>[Main training configuration:](#toc0_)\n",
    "[`configs/base_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/base_config.yaml)\n",
    "\n",
    "This is the default configuration used when running:\n",
    "\n",
    "```bash\n",
    "python -m src.deep_proj.train\n",
    "```\n",
    "\n",
    "It defines:\n",
    "\n",
    "- Weights and Biases (WandB) settings\n",
    "    - Whether logging is enabled, project name, entity, grouping of runs.\n",
    "\n",
    "- Dataset setup\n",
    "    - Choose mnist or medmnist, data path, batch size, validation split, and optional class filtering\n",
    "\n",
    "- Model configuration\n",
    "    - Select bottleneck type (gaussian, dirichlet, cc), latent dimension, and model-specific parameters.\n",
    "\n",
    "- Training hyperparameters\n",
    "    - Learning rate, number of epochs, GPU/CPU selection, early stopping.\n",
    "\n",
    "- Output structure\n",
    "\n",
    "#### <a id='toc3_1_1_2_'></a>[Hyperparameter sweep setup:](#toc0_)\n",
    "[`configs/wandb_sweep_config.yaml`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/configs/wandb_sweep_config.yaml)\n",
    "\n",
    "This file defines a WandB grid search sweep, allowing us to automatically try multiple combinations of:\n",
    "- datasets\n",
    "- model types (gaussian, diriclet and cc)\n",
    "- learning rates\n",
    "- latent dimensions\n",
    "\n",
    "To launch the sweep run following from the terminal:\n",
    "```bash\n",
    "wandb sweep configs/wandb_sweep_config.yaml\n",
    "wandb agent <sweep_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579662b",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Data Pipeline](#toc0_)\n",
    "\n",
    "In the datascript found in [`src/deep_proj/data.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/data.py), we make sure to handle both **MNIST** and **MedMNIST** datasets in a unified way.  \n",
    "\n",
    "The script provides:\n",
    "- **Dataset builders**:\n",
    "  - `_build_mnist` - Loads MNIST with standard normalization ()  and optionally filters the dataset to only include user-specified classes loaded from the config file (mnist_classes=[0,1,...]). \n",
    "  - `_build_medmnist`  \n",
    "    Loads MedMNIST subsets using metadata from the `medmnist` package and applies appropriate normalization.\n",
    "\n",
    "- **Dataloader construction** via `get_dataloaders`:\n",
    "  - Selects which dataset loader to use (`mnist` or `medmnist`)\n",
    "  - Splits the training set deterministically into **train/val** subsets based on `val_split`\n",
    "  - Wraps everything into PyTorch `DataLoader` objects for training, validation, and testing\n",
    "\n",
    "! Note on normalization: This was first introduced as a standard thing to add stability in the optimization, but is later removed in the training to keep the same simple setup as in the paper we tried to follow for the report. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e9258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MNIST ===\n",
      "Train samples: 18507\n",
      "Test samples: 3097\n",
      "\n",
      "=== MedMNIST (subset: organcmnist ) ===\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Train samples: 2792\n",
      "Test samples: 1713\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from src.deep_proj.data import _build_mnist, _build_medmnist\n",
    "\n",
    "# we will override the base_config manually\n",
    "cfg = OmegaConf.load(\"configs/base_config.yaml\")\n",
    "\n",
    "# -------------------------\n",
    "# MNIST\n",
    "# -------------------------\n",
    "cfg.dataset = \"mnist\"\n",
    "cfg.mnist_classes = [0,1,4]   # only include the 3 subclasses \n",
    "\n",
    "print(\"=== MNIST ===\")\n",
    "mnist_train, mnist_test = _build_mnist(cfg)\n",
    "print(\"Train samples:\", len(mnist_train))\n",
    "print(\"Test samples:\", len(mnist_test))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# -------------------------\n",
    "# MedMNIST\n",
    "# -------------------------\n",
    "print(\"=== MedMNIST (subset:\", cfg.medmnist_subset, \") ===\")\n",
    "cfg.dataset = \"medmnist\"\n",
    "cfg.medmnist_subset = \"organcmnist\"   \n",
    "cfg.medmnist_classes = [3,5,8]        # only include the 3 organ classes\n",
    "\n",
    "med_train, med_test = _build_medmnist(cfg)\n",
    "\n",
    "\n",
    "print(\"Train samples:\", len(med_train))\n",
    "print(\"Test samples:\", len(med_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f8ff9",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Model Implementations](#toc0_)\n",
    "The [`src/deep_proj/model.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/model.py) collects all neural network models and ELBO loss functions used in the project. The key idea is that all three VAEs share the same encoder/decoder architecture but differ in the latent bottleneck distribution.\n",
    "\n",
    "## <a id='toc5_1_'></a>[Shared blocks](#toc0_)\n",
    "\n",
    "- `MLPEncoder`  \n",
    "  A fully connected network that maps a flattened image to latent parameters.  \n",
    "  It behaves differently depending on `bottle`:\n",
    "  - `\"gaus\"` – outputs mean `μ` and log-variance `logvar` for a Gaussian latent.\n",
    "  - `\"dir\"` – outputs positive concentration parameters `α̂` for a Dirichlet latent (clamped to a safe range).\n",
    "  - `\"cc\"` – outputs positive parameters `λ̂` for the Continuous Categorical latent.\n",
    "\n",
    "- `BernoulliDecoder`  \n",
    "  A mirrored MLP that maps a latent vector back to image logits.  \n",
    "  The last layer outputs raw logits which are used with `BCEWithLogitsLoss` to model Bernoulli pixels.\n",
    "\n",
    "## <a id='toc5_2_'></a>[VAE classes](#toc0_)\n",
    "\n",
    "- `GaussianVAE`  \n",
    "  Standard VAE:\n",
    "  1. Encoder produces `μ` and `logvar`.\n",
    "  2. `reparameterize` draws `z = μ + σ ⊙ ε` with `ε ~ N(0, I)`.\n",
    "  3. Decoder maps `z` to reconstruction logits.\n",
    "\n",
    "- `DirVAE`  \n",
    "  Uses a Dirichlet latent:\n",
    "  1. Encoder outputs `α̂` (Dirichlet parameters).\n",
    "  2. Samples are generated via an approximate inverse Gamma CDF, then normalized to the simplex.\n",
    "  3. The prior is another Dirichlet with concentration `prior_alpha`.\n",
    "  4. `dirvae_elbo_loss` combines a Bernoulli reconstruction term with a KL term based on the MultiGamma representation of the Dirichlet.\n",
    "\n",
    "- `CCVAE`  \n",
    "  Uses the Continuous Categorical distribution on the simplex:\n",
    "  1. Encoder outputs `λ̂` (CC parameters) which has been normalized to 1.\n",
    "  2. `sample_cc_ordered_reparam` implements a differentiable sampler using the inverse of the continous Bernoulli distribution and returns samples `z` on the simplex.\n",
    "  3. A uniform prior over the simplex is encoded as `prior_lambda` which is just flat (`torch.ones(latent_dim)/K`).\n",
    "  4. `ccvae_elbo_loss` uses a reconstruction term and a MC estimate of CC-specific KL term (`cc_kl`) based on the log-normalizing constant `log C(η)` and the sample z.\n",
    "\n",
    "See  [`src/deep_proj/model.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/model.py) for all details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13123885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model        Latent dim  # trainable params   Prior                                                        ELBO loss fn             \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Gauss-VAE    3           1043796              N(0, I)                                                      gaussian_vae_elbo_loss   \n",
      "Dir-VAE      3           1043796               α=[0.98 0.98 0.98] (same as in paper)                       dirvae_elbo_loss         \n",
      "CC-VAE       3           1043796              λ=[0.33333334 0.33333334 0.33333334] (flat on simplex)       ccvae_elbo_loss          \n"
     ]
    }
   ],
   "source": [
    "# Showing model summaries\n",
    "import torch\n",
    "from src.deep_proj.model import (\n",
    "    GaussianVAE,\n",
    "    DirVAE,\n",
    "    CCVAE,\n",
    "    gaussian_vae_elbo_loss,\n",
    "    dirvae_elbo_loss,\n",
    "    ccvae_elbo_loss\n",
    ")\n",
    "\n",
    "input_dim   = 28 * 28\n",
    "enc_hidden  = [500, 500]\n",
    "dec_hidden  = [500]\n",
    "latent_dim  = 3  # or 5 / 8\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Instantiate one copy of each model\n",
    "gauss = GaussianVAE(input_dim, enc_hidden, dec_hidden, latent_dim)\n",
    "dirichlet = DirVAE(input_dim, enc_hidden, dec_hidden, latent_dim)\n",
    "cc = CCVAE(input_dim, enc_hidden, dec_hidden, latent_dim)\n",
    "\n",
    "# Build rows *directly from the model objects*\n",
    "rows = [\n",
    "    {\n",
    "        \"Model\": \"Gauss-VAE\",\n",
    "        \"Latent dim\": gauss.latent_dim,\n",
    "        \"# trainable params\": count_params(gauss),\n",
    "        \"Prior\": \"N(0, I)\",  # GaussianVAE uses a fixed standard normal prior\n",
    "        \"ELBO loss fn\": gaussian_vae_elbo_loss.__name__,\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Dir-VAE\",\n",
    "        \"Latent dim\": dirichlet.latent_dim,\n",
    "        \"# trainable params\": count_params(dirichlet),\n",
    "        # shows the actual prior vector stored in the model\n",
    "        \"Prior\": f\" α={dirichlet.prior_alpha.cpu().numpy()} (same as in paper)\",\n",
    "        \"ELBO loss fn\": dirvae_elbo_loss.__name__, #if i were to print the inputs to this function i would write: #dirvae_elbo_loss(recon_x, x, mu, logvar, dirichlet.prior_alpha)\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"CC-VAE\",\n",
    "        \"Latent dim\": cc.latent_dim,\n",
    "        \"# trainable params\": count_params(cc),\n",
    "        \"Prior\": f\"λ={cc.prior_lambda.cpu().numpy()} (flat on simplex)\",\n",
    "        \"ELBO loss fn\": ccvae_elbo_loss.__name__,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Pretty print table\n",
    "header = rows[0].keys()\n",
    "print(\"{:<12} {:<11} {:<20} {:<60} {:<25}\".format(*header))\n",
    "print(\"-\" * 125)\n",
    "for r in rows:\n",
    "    print(\"{Model:<12} {Latent dim:<11} {# trainable params:<20} {Prior:<60} {ELBO loss fn:<25}\".format(**r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b2110",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Training Pipeline](#toc0_)\n",
    "\n",
    "The training script is found in [`src/deep_proj/train.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/train.py).\n",
    "\n",
    "It does the following:\n",
    "1. Loads config and picks device  \n",
    "   Hydra loads `base_config.yaml`, and `get_device(cfg)` chooses GPU if available (or CPU otherwise).\n",
    "\n",
    "2. Builds dataloaders and model  \n",
    "   Using `get_dataloaders(cfg)` and the `model_name` in the config, the script creates one of:\n",
    "   - `GaussianVAE`\n",
    "   - `DirVAE`\n",
    "   - `CCVAE`  \n",
    "   together with the matching ELBO loss function.\n",
    "\n",
    "3. Sets up optimizer, early stopping, and logging \n",
    "   - Adam optimizer  \n",
    "   - `EarlyStopping` based on validation loss  \n",
    "   - Optional Weights & Biases logging and checkpoint directory under `models/`.\n",
    "\n",
    "The core of the script is the epoch loop:\n",
    "```python\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    tot_loss = tot_recon = tot_kl = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for xb, _ in train_loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # undo normalization - pixels back in [0,1] because paper we are trying to mimic does not normalize\n",
    "        if cfg.dataset.lower() == \"mnist\":\n",
    "            xb = xb * 0.3081 + 0.1307\n",
    "        elif cfg.dataset.lower() == \"medmnist\":\n",
    "            xb = xb * 0.5 + 0.5\n",
    "\n",
    "        xb = xb.view(xb.size(0), -1)                                # Flatten images to match the MLP architecture adopted from [7]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, recon, kl = loss_fn(model, xb, reduction=\"mean\")      # loss_fn is one of {gaussian_vae_elbo_loss, dirvae_elbo_loss, ccvae_elbo_loss}\n",
    "        loss.backward()                                             # Backpropagate stochastic gradient of the ELBO\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)     #apply gradient clipping with max norm = 5, to stabilize optimization\n",
    "        optimizer.step()                                            # Single Adam update step (same optimizer for all models)\n",
    "\n",
    "        # Accumulate sums so we can later compute averages over the full epoch\n",
    "        bs = xb.size(0)\n",
    "        n += bs\n",
    "        tot_loss += loss.item() * bs\n",
    "        tot_recon += recon.item() * bs\n",
    "        tot_kl += kl.item() * bs\n",
    "```\n",
    "\n",
    "Key points:\n",
    "- Inputs are unnormalized back to [0,1] before the ELBO, so the Bernoulli likelihood is consistent.\n",
    "- The chosen ELBO loss returns total loss, reconstruction term, and KL term.\n",
    "- Gradients are clipped (max_norm=5) for stability.\n",
    "\n",
    "After each epoch, the script:\n",
    "- Runs a validation pass with evaluate_split(...).\n",
    "- Prints train/val losses and updates history.\n",
    "- Saves a “best” checkpoint if the validation loss improves.\n",
    "- Optionally logs metrics and visualizations to WandB.\n",
    "- Checks early stopping; if there is no improvement for 10 epochs, training stops.\n",
    "\n",
    "For full details see the script in [`src/deep_proj/train.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/train.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871198e",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Evaluation Script](#toc0_)\n",
    "We evaluate the trained models using the [`src/deep_proj/evaluate.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/evaluate.py). It takes a single checkpoint file and rebuilds everything needed to evaluate it.\n",
    "\n",
    "It also saves:\n",
    "- A simplex projection of the latent space (plot_latent_simplex)\n",
    "- A t-SNE latent plot (plot_latent)\n",
    "- A reconstruction grid (plot_recons)\n",
    "\n",
    "All plots are stored under:\n",
    "`reports/figures/<run_id>/evaluation/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146f9a0",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[Multi-model Comparison](#toc0_)\n",
    "\n",
    "We also create a [`src/deep_proj/evaluate_multiple.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/evaluate_multiple.py) script that \n",
    "loads three trained checkpoints (Gaussian, Dirichlet, CC model), rebuilds each model from its saved config, and evaluates them on the same MNIST test set. For each model, it:\n",
    "\n",
    "- Extracts latent representations and runs **t-SNE** to create a side-by-side latent space comparison.\n",
    "- Uses a fixed batch of test images to plot a 4 × N reconstruction grid:\n",
    "  1. Original images  \n",
    "  2. Gaussian-VAE reconstructions  \n",
    "  3. Dirichlet-VAE reconstructions  \n",
    "  4. CC-VAE reconstructions\n",
    "\n",
    "The resulting comparison figures are saved under:\n",
    "\n",
    "`reports/figures/multi_eval/latent_comparison.png` \n",
    "and  \n",
    "`reports/figures/multi_eval/reconstruction_comparison.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042cfc9",
   "metadata": {},
   "source": [
    "## <a id='toc7_2_'></a>[Visualization Utilities](#toc0_)\n",
    "\n",
    "All plotting code for the project lives in two helper modules:\n",
    "\n",
    "- **[`simplex.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/simplex.py)**  \n",
    "  Contains the tools for plotting latent simplex projections, where simplex-valued latents (Dirichlet / CC) are projected onto a polygon in 2D and overlaid with:\n",
    "  - Colored class clusters\n",
    "  - Example MNIST images placed at the simplex corners  \n",
    "  The main function is:\n",
    "  - `plot_latent_simplex(...)`\n",
    "\n",
    "- **[`visualize.py`](https://github.com/KarolineKlan/deep_project_group_38/blob/main/src/deep_proj/visualize.py)**  \n",
    "  Collects the general-purpose visualization functions used during training and evaluation, including:\n",
    "  - `plot_latent(...)` – t-SNE latent space plots  \n",
    "  - `plot_recons(...)` – original vs. reconstructed image grids  \n",
    "  - `plot_side_by_side(...)` – combine two images/plots into a single figure (used for training logs mostly) \n",
    "  - `plot_training_loss(...)` – training and validation loss/ KL curves  \n",
    "\n",
    "These utilities are called from `train.py`, `evaluate.py`, and `evaluate_multiple.py` to produce the figures shown in the report and in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e8e1d",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Final Findings](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6271aa",
   "metadata": {},
   "source": [
    "This project explored how three different latent bottlenecks - Gaussian, Dirichlet, and CC - shape the behaviour of a Variational Autoencoder, both in terms of representation geometry and reconstruction quality. By keeping the encoder/decoder architecture and training pipeline identical across models, differences can be directly attributed to the choice of latent distribution.\n",
    "\n",
    "All training logs can be found in the [Weights and Biases report](https://api.wandb.ai/links/spicy-mlops/92cfrf8p).\n",
    "\n",
    "### <a id='toc8_1_1_'></a>[The final 3 best models](#toc0_)\n",
    "\n",
    "We found the 3 best models for gauss and dir to be the one with the highest latent space M=3 with learning rate 0.0007, and for CC the one with M=3 and learning rate 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94290f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAUSS_FINAL_MODEL = \"models/final_sweep/mnist_gaussian_z8_lr0.0007_best.pt\"\n",
    "DIR_FINAL_MODEL   = \"models/final_sweep/mnist_dirichlet_z8_lr0.0007_best.pt\"\n",
    "CC_FINAL_MODEL    = \"models/final_sweep/mnist_cc_z3_lr0.0003_best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6305f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gaussian-VAE (latent dim = 8) ===\n",
      "Validation | Loss 83.4912 | Recon 68.1760 | KL 15.3152\n",
      "Test       | Loss 84.2690 | Recon 69.1138 | KL 15.1553\n",
      "\n",
      "=== Dirichlet-VAE (latent dim = 8) ===\n",
      "Validation | Loss 104.3500 | Recon 90.9932 | KL 13.3568\n",
      "Test       | Loss 101.9431 | Recon 88.7045 | KL 13.2386\n",
      "\n",
      "=== CC-VAE (latent dim = 3) ===\n",
      "Validation | Loss 158.1684 | Recon 150.3114 | KL 7.8570\n",
      "Test       | Loss 158.9817 | Recon 151.1213 | KL 7.8604\n"
     ]
    }
   ],
   "source": [
    "from src.deep_proj.evaluate_multiple import build_model_from_config\n",
    "from src.deep_proj.data import get_dataloaders\n",
    "from src.deep_proj.train import evaluate_split\n",
    "from src.deep_proj.model import (\n",
    "    gaussian_vae_elbo_loss, dirvae_elbo_loss, ccvae_elbo_loss\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def run_eval(ckpt_path, name):\n",
    "    # Load checkpoint + config\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    cfg = DictConfig(OmegaConf.create(ckpt[\"config\"]))\n",
    "\n",
    "    # Build model\n",
    "    model = build_model_from_config(cfg, device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get loss function automatically\n",
    "    if cfg.model_name.lower() in (\"gaussian\", \"gaus\", \"gauss\"):\n",
    "        loss_fn = gaussian_vae_elbo_loss\n",
    "    elif cfg.model_name.lower() in (\"dirichlet\", \"dir\"):\n",
    "        loss_fn = dirvae_elbo_loss\n",
    "    else:\n",
    "        loss_fn = ccvae_elbo_loss\n",
    "\n",
    "    # Dataloaders\n",
    "    loaders = get_dataloaders(cfg)\n",
    "    val_loader = loaders[\"val\"]\n",
    "    test_loader = loaders[\"test\"]\n",
    "\n",
    "    # Eval\n",
    "    val_loss, val_recon, val_kl = evaluate_split(model, val_loader, loss_fn, cfg, device)\n",
    "    test_loss, test_recon, test_kl = evaluate_split(model, test_loader, loss_fn, cfg, device)\n",
    "\n",
    "    print(f\"\\n=== {name} (latent dim = {cfg.latent_dim}) ===\")\n",
    "    print(f\"Validation | Loss {val_loss:.4f} | Recon {val_recon:.4f} | KL {val_kl:.4f}\")\n",
    "    print(f\"Test       | Loss {test_loss:.4f} | Recon {test_recon:.4f} | KL {test_kl:.4f}\")\n",
    "\n",
    "run_eval(GAUSS_FINAL_MODEL, \"Gaussian-VAE\")\n",
    "run_eval(DIR_FINAL_MODEL,   \"Dirichlet-VAE\")\n",
    "run_eval(CC_FINAL_MODEL,    \"CC-VAE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f1c46",
   "metadata": {},
   "source": [
    "\n",
    "Based on our experiments, the Gaussian bottleneck consistently achieved the lowest reconstruction errors, while the CC-bottleneck produced the highest, with the Dirichlet in between. This trend is also visible in the reconstruction examples:\n",
    "\n",
    "#### <a id='toc8_1_1_1_'></a>[Reconstruction Comparison](#toc0_)\n",
    "\n",
    "<img src=\"project_images/reconstruction_comparison_final_mnist.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "#### <a id='toc8_1_1_2_'></a>[Latent space representation](#toc0_)\n",
    "\n",
    "**Latent space of the 3 models**\n",
    "For the latent spaces, the Gaussian-VAE and Dirichlet-VAE form clearly separated clusters, whereas the CC-VAE clusters remain a bit connected, indicating that the CC model might not fully discriminate between classes:\n",
    "\n",
    "<img src=\"project_images/latent_comparison_FINAL.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "**Dirichlet versus CC on the Simplex**\n",
    "When projecting the models onto the simplex, we observe\n",
    "- the CC-VAE uses the extremes of the simplex more strongly than the Dirichlet-VAE for both M=3 and M=5.\n",
    "- The Dirichlet-VAE tends to cluster toward the interior, while the CC-VAE spreads toward the corners\n",
    "\n",
    "<img src=\"project_images/dir_vs_cc_simplex_grid.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56082e34",
   "metadata": {},
   "source": [
    "## <a id='toc8_2_'></a>[MedMNIST Experiment](#toc0_)\n",
    "\n",
    "To test the robustness of the three bottlenecks beyond MNIST, we repeated the experiments on OrganCMNIST with three classes as well but due to time constraints no larger sweep was run. Only models with learning rate 0.0005 and M=5.\n",
    "\n",
    "Training logs can be found in the [MedMNIST WandB Report](https://api.wandb.ai/links/spicy-mlops/4fqt4npo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bffdf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MedMNIST (subset: organcmnist ) ===\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Train samples: 2792\n",
      "Test samples: 1713\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MedMNIST (subset:\", cfg.medmnist_subset, \") ===\")\n",
    "cfg.dataset = \"medmnist\"\n",
    "cfg.medmnist_subset = \"organcmnist\"   \n",
    "cfg.medmnist_classes = [3,5,8]        # only include the 3 organ classes\n",
    "\n",
    "med_train, med_test = _build_medmnist(cfg)\n",
    "\n",
    "\n",
    "print(\"Train samples:\", len(med_train))\n",
    "print(\"Test samples:\", len(med_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda4fcb",
   "metadata": {},
   "source": [
    "Evaluation of the 3 models show:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97804cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "\n",
      "=== Gaussian-VAE (MedMNIST, M=5) (latent dim = 5) ===\n",
      "Validation | Loss 421.1960 | Recon 410.7639 | KL 10.4321\n",
      "Test       | Loss 443.6786 | Recon 434.0534 | KL 9.6253\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "\n",
      "=== Dirichlet-VAE (MedMNIST, M=5) (latent dim = 5) ===\n",
      "Validation | Loss 448.4669 | Recon 439.9898 | KL 8.4770\n",
      "Test       | Loss 459.2871 | Recon 451.5473 | KL 7.7398\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "Using downloaded and verified file: data/organcmnist.npz\n",
      "\n",
      "=== CC-VAE (MedMNIST, M=5) (latent dim = 5) ===\n",
      "Validation | Loss 490.4989 | Recon 460.0301 | KL 30.4688\n",
      "Test       | Loss 508.5288 | Recon 477.6432 | KL 30.8856\n"
     ]
    }
   ],
   "source": [
    "GAUSS_MED_z5 = \"models/medmnist_gaussian_z5_lr0.0005_best.pt\"\n",
    "DIR_MED_z5   = \"models/medmnist_dirichlet_z5_lr0.0005_best.pt\"\n",
    "CC_MED_z5    = \"models/medmnist_cc_z5_lr0.0005_best.pt\"\n",
    "\n",
    "run_eval(GAUSS_MED_z5, \"Gaussian-VAE (MedMNIST, M=5)\")\n",
    "run_eval(DIR_MED_z5,   \"Dirichlet-VAE (MedMNIST, M=5)\")\n",
    "run_eval(CC_MED_z5,    \"CC-VAE (MedMNIST, M=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083cb71",
   "metadata": {},
   "source": [
    "As described in the report, the images in OrganCMNIST is much more complex than MNIST: the images contain textures, anatomy, shading, and organ variation that a small MLP-based VAE cannot model well. Also the Train-set is much smaller than regular mnist when only selecting a subset of classes. As a result all models produce much worse reconstructions, which drives the reconstruction term (and therefore total ELBO) extremely high. Gaussian still performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33dd0eb",
   "metadata": {},
   "source": [
    "Accordingly, all models produced less impressive reconstructions, as they all appear very blurry:\n",
    "\n",
    "<img src=\"project_images/gauss_dir_cc_recon.png\" width=\"40%\">\n",
    "\n",
    "Gauss forms reasonably separated clusters, but the class boundaries are not as clean as they were on MNIST. CC has some pointy clusters almost looking like a star, something we also witnessed from training logs when training on the mnist for M=5 classes for CC.\n",
    "\n",
    "<img src=\"project_images/medmnist_latent_tsne.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "we see similar results when projecting the medmnist onto the simplex, that classes for CC spread towards the corners, but not as well as for the mnist data. We see some overlapping classes.\n",
    "\n",
    "<img src=\"project_images/medmnist_dir_vs_cc_simplex_grid.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90476c",
   "metadata": {},
   "source": [
    "We refer to the report for discussion and conslusion on the findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
